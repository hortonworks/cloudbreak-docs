{
    "docs": [
        {
            "location": "/", 
            "text": "Repository moved under Hortonwors account. Please use \nthis\n page.\n\n\n\n\n\nMarch 26, 2018 \nKnown issue related to Google Cloud\n\n\n\n\nDue to a recent change on Google Cloud Platform, all clusters created with Cloudbreak before 1.16.6 on Google Cloud will fail. This issue will be fixed in Cloudbreak 1.16.6. If you are already using Cloudbreak with Google Cloud, upgrade to Cloudbreak 1.16.6. If you were planning to install Cloudbreak 1.16.6, use Cloudbreak 1.16.6 instead. \n\n\n\n\n\nIntroduction\n\n\nCloudbreak simplifies the provisioning, management and monitoring of on-demand HDP clusters in virtual and cloud environments. Cloudbreak leverages the cloud infrastructure platform resources to create host instances, uses Docker technology to deploy the requisite containers cloud-agnostically, and uses Apache Ambari (via Ambari Blueprints) to install and manage the HDP cluster.\n\n\nUse the Cloudbreak Web UI or CLI to launch HDP clusters on public cloud infrastructure platforms such as Microsoft Azure, Amazon Web Services (AWS), and Google Cloud Platform (GCP) and the private cloud infrastructure platform OpenStack.\n\n\nCloudbreak has two main components: the \nCloudbreak Application\n and the \nCloudbreak Deployer\n.\n\n\nThe \nCloudbreak Application\n is made up from microservices (Cloudbreak, Uluwatu, Sultans, ...). The \nCloudbreak Deployer\n helps you to deploy the Cloudbreak application automatically with Docker support. Once the Cloudbreak Application is deployed, you can use it to provision HDP clusters in different cloud environments.\n\n\n\n\nFor an architectural overview of the Cloudbreak Deployer, the Cloudbreak Application, Apache Ambari, Docker and the rest of the Cloudbreak components, please follow this \nlink\n.\n\n\n\n\nInstallation\n\n\nThe high-level process to be able to use Cloudbreak to install an HDP cluster includes the following steps:\n\n\n\n\nInstall the Cloudbreak Deployer\n by either: \ninstalling the Cloudbreak Deployer\n on your own VM/host (Option #1); or by instantiating one of the \npre-built cloud images\nthat includes Cloudbreak Deployer\n pre-installed (Option #2).\n\n\nConfigure the Cloudbreak Deployer and install the Cloudbreak Application\n. Once you have installed Cloudbreak Deployer (called \"cbd\"), it will start up several Docker containers: Cloudbreak API, Cloudbreak Web UI (called \"Uluwatu\"), Identity Server and supporting databases. You have finished this step if you are able to login in your browser to Cloudbreak Web UI.\n\n\nProvision an HDP Cluster\n using the Cloudbreak Application.\n\n\n\n\n\n\n\nInstalling the Cloudbreak Deployer (Install Option #1)\n\n\n\n\nMinimum and Recommended System requirements\n:\n RHEL / CentOS / Oracle Linux 7 (64-bit), Docker 1.9.1, 4GB RAM, 10GB disk, 2 cores recommended\n\n\n\n\nYou can install the Cloudbreak Deployer on your own VM/host manually. Once installed, you will use the Deployer to setup\nthe Cloudbreak Application. We suggest you install the Cloudbreak Application as close to the\ndesired HDP clusters as possible. For example, if you plan to launch clusters on AWS, install the Cloudbreak Application in AWS.\n\n\nFollow the instructions for \ninstalling the Cloudbreak Deployer\n. Alternatively, you can consider using one of the \npre-built cloud images that includes Cloudbreak Deployer\n pre-installed.\n\n\n\n\nIMPORTANT:\n If you plan to use Cloudbreak on Azure, you \nmust\n use the \nAzure Setup\n instructions to configure the image.\n\n\n\n\n\n\n\nUsing the Pre-Built Cloud Images (Install Option #2)\n\n\nWe have pre-built cloud images with Cloudbreak Deployer pre-installed. In the below table you can find the \nprovider specific guides to configure and launch \ncbd (Cloudbreak Deployer)\n then clusters.\n\n\n\n\nMinimum and Recommended VM requirements:\n 4GB RAM, 10GB disk, 2 cores recommended\n\n\n\n\n\n\n\n\n\n\nCloud\n\n\nCloud Image\n\n\n\n\n\n\n\n\n\n\nAWS\n\n\nYou can follow the AWS instructions using this \nlink\n.\n\n\n\n\n\n\nAzure\n\n\nThere are no pre-built cloud images available for Azure. But we provide an option to use Azure Resource Manager Templates instead. See \nAzure Setup\n to get the Cloudbreak Deployer installed and configured.\n\n\n\n\n\n\nGCP\n\n\nYou can follow the GCP instructions using this \nlink\n\n\n\n\n\n\nOpenStack\n\n\nYou can follow the OpenStack instructions using this \nlink\n\n\n\n\n\n\n\n\nLearn More\n\n\nFor more information on Cloudbreak, Docker, Ambari and Ambari Blueprints, see:\n\n\n\n\n\n\n\n\nResource\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCloudbreak Project\n\n\nCloudbreak is a tool to help simplify the provisioning of HDP clusters in virtual and cloud environments.\n\n\n\n\n\n\nCloudbreak Forums\n\n\nGet connected with the community in the Cloudbreak Forums.\n\n\n\n\n\n\nApache Ambari Project\n\n\nApache Ambari is an operational platform for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari exposes a robust set of REST APIs and a rich Web interface for cluster management.\n\n\n\n\n\n\nAmbari Blueprints\n\n\nAmbari Blueprints are a declarative definition of a Hadoop cluster that Ambari can use to create Hadoop clusters.\n\n\n\n\n\n\nDocker\n\n\nDocker is an open platform for developers and system administrators to build, ship, and run distributed applications.", 
            "title": "Introduction"
        }, 
        {
            "location": "/#introduction", 
            "text": "Cloudbreak simplifies the provisioning, management and monitoring of on-demand HDP clusters in virtual and cloud environments. Cloudbreak leverages the cloud infrastructure platform resources to create host instances, uses Docker technology to deploy the requisite containers cloud-agnostically, and uses Apache Ambari (via Ambari Blueprints) to install and manage the HDP cluster.  Use the Cloudbreak Web UI or CLI to launch HDP clusters on public cloud infrastructure platforms such as Microsoft Azure, Amazon Web Services (AWS), and Google Cloud Platform (GCP) and the private cloud infrastructure platform OpenStack.  Cloudbreak has two main components: the  Cloudbreak Application  and the  Cloudbreak Deployer .  The  Cloudbreak Application  is made up from microservices (Cloudbreak, Uluwatu, Sultans, ...). The  Cloudbreak Deployer  helps you to deploy the Cloudbreak application automatically with Docker support. Once the Cloudbreak Application is deployed, you can use it to provision HDP clusters in different cloud environments.   For an architectural overview of the Cloudbreak Deployer, the Cloudbreak Application, Apache Ambari, Docker and the rest of the Cloudbreak components, please follow this  link .", 
            "title": "Introduction"
        }, 
        {
            "location": "/#installation", 
            "text": "The high-level process to be able to use Cloudbreak to install an HDP cluster includes the following steps:   Install the Cloudbreak Deployer  by either:  installing the Cloudbreak Deployer  on your own VM/host (Option #1); or by instantiating one of the  pre-built cloud images\nthat includes Cloudbreak Deployer  pre-installed (Option #2).  Configure the Cloudbreak Deployer and install the Cloudbreak Application . Once you have installed Cloudbreak Deployer (called \"cbd\"), it will start up several Docker containers: Cloudbreak API, Cloudbreak Web UI (called \"Uluwatu\"), Identity Server and supporting databases. You have finished this step if you are able to login in your browser to Cloudbreak Web UI.  Provision an HDP Cluster  using the Cloudbreak Application.", 
            "title": "Installation"
        }, 
        {
            "location": "/#installing-the-cloudbreak-deployer-install-option-1", 
            "text": "Minimum and Recommended System requirements :  RHEL / CentOS / Oracle Linux 7 (64-bit), Docker 1.9.1, 4GB RAM, 10GB disk, 2 cores recommended   You can install the Cloudbreak Deployer on your own VM/host manually. Once installed, you will use the Deployer to setup\nthe Cloudbreak Application. We suggest you install the Cloudbreak Application as close to the\ndesired HDP clusters as possible. For example, if you plan to launch clusters on AWS, install the Cloudbreak Application in AWS.  Follow the instructions for  installing the Cloudbreak Deployer . Alternatively, you can consider using one of the  pre-built cloud images that includes Cloudbreak Deployer  pre-installed.   IMPORTANT:  If you plan to use Cloudbreak on Azure, you  must  use the  Azure Setup  instructions to configure the image.", 
            "title": "Installing the Cloudbreak Deployer (Install Option #1)"
        }, 
        {
            "location": "/#using-the-pre-built-cloud-images-install-option-2", 
            "text": "We have pre-built cloud images with Cloudbreak Deployer pre-installed. In the below table you can find the \nprovider specific guides to configure and launch  cbd (Cloudbreak Deployer)  then clusters.   Minimum and Recommended VM requirements:  4GB RAM, 10GB disk, 2 cores recommended      Cloud  Cloud Image      AWS  You can follow the AWS instructions using this  link .    Azure  There are no pre-built cloud images available for Azure. But we provide an option to use Azure Resource Manager Templates instead. See  Azure Setup  to get the Cloudbreak Deployer installed and configured.    GCP  You can follow the GCP instructions using this  link    OpenStack  You can follow the OpenStack instructions using this  link", 
            "title": "Using the Pre-Built Cloud Images (Install Option #2)"
        }, 
        {
            "location": "/#learn-more", 
            "text": "For more information on Cloudbreak, Docker, Ambari and Ambari Blueprints, see:     Resource  Description      Cloudbreak Project  Cloudbreak is a tool to help simplify the provisioning of HDP clusters in virtual and cloud environments.    Cloudbreak Forums  Get connected with the community in the Cloudbreak Forums.    Apache Ambari Project  Apache Ambari is an operational platform for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari exposes a robust set of REST APIs and a rich Web interface for cluster management.    Ambari Blueprints  Ambari Blueprints are a declarative definition of a Hadoop cluster that Ambari can use to create Hadoop clusters.    Docker  Docker is an open platform for developers and system administrators to build, ship, and run distributed applications.", 
            "title": "Learn More"
        }, 
        {
            "location": "/architecture/", 
            "text": "Architecture\n\n\nCloudbreak Deployer Architecture\n\n\n\n\nuaa\n: OAuth Identity Server\n\n\ncloudbreak\n: the Cloudbreak app\n\n\nperiscope\n: the Periscope app\n\n\nuluwatu\n: Cloudbreak UI\n\n\nsultans\n: user management\n\n\n\n\nSystem Level Containers\n\n\n\n\nconsul\n: Service Registry\n\n\nregistrator\n: automatically registers/unregisters containers with consul\n\n\n\n\nCloudbreak Application Architecture\n\n\nCloudbreak is built on the foundation of cloud providers APIs, Apache Ambari, Docker containers, Swarm and Consul.\n\n\nApache Ambari\n\n\nThe Apache Ambari project is aimed at making Hadoop management simpler by developing software for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari provides an intuitive, easy-to-use Hadoop management web UI backed by its RESTful APIs.\n\n\n\n\nSystem Administrators\n\n\nAmbari enables to integrate Hadoop provisioning, management and monitoring capabilities into applications with the Ambari REST APIs.\n\n\n\n\nProvision a Hadoop Cluster\n:\n\n\nAmbari provides a step-by-step wizard for installing Hadoop services across any number of hosts.\n\n\nAmbari handles configuration of Hadoop services for the cluster.\n\n\n\n\n\n\nManage a Hadoop Cluster\n:\n\n\nAmbari provides central management for starting, stopping, and reconfiguring Hadoop services across the entire.\n   cluster.\n\n\n\n\n\n\nMonitor a Hadoop Cluster\n:\n\n\nAmbari provides a dashboard for monitoring health and status of the Hadoop cluster.\n\n\nAmbari allows to choose between predefined alerts or add your custom ones.\n\n\n\n\n\n\n\n\nAmbari Blueprint\n\n\nAmbari blueprints are a declarative definition of a cluster. With a blueprint, you can specify stack, component\n layout and configurations to materialise a Hadoop cluster instance (via a REST API) without having to use the Ambari\n  Cluster Install Wizard.\n\n\n\n\nDocker\n\n\nDocker is an open platform for developers and sysadmins to build, ship and run distributed applications. Consisting \nof Docker Engine, a portable, lightweight runtime and packaging tool and hub. Docker Hub is a cloud service for sharing \napplications and automating the workflow.\n\n\nDocker enables apps to be quickly assembled from components and eliminates the\n friction between development, QA and production environments. As a result, IT can ship faster and run the same app, \n unchanged, on laptops, data center VMs and any cloud.\n\n\nThe main features of Docker\n\n\n\n\nLightweight\n\n\nPortable\n\n\nBuild once\n\n\nRun anywhere\n\n\n\n\nVM - without the overhead of a VM\n\n\n\n\nEach virtualised application includes not only the application and the necessary binaries and libraries, but \n   also an entire guest operating system\n\n\nThe Docker Engine container comprises just the application and its dependencies. It runs as an isolated process \n   in userspace on the host operating system, sharing the kernel with other containers.\n\n\n\n\n\n\n\n\n\nContainers are isolated\n\n\n\n\nIt can be automated and scripted\n\n\n\n\nSwarm\n\n\nDocker Swarm is native clustering for Docker. It turns a pool of Docker hosts into a single, virtual host. Swarm serves the standard Docker API.\n\n\n\n\nDistributed container orchestration\n: Allows to remotely orchestrate Docker containers on different hosts.\n    \n\n\nDiscovery services\n: Supports different discovery backends to provide service discovery, as such: token (hosted) \n  and file based, etcd, Consul, Zookeeper.\n\n\nAdvanced scheduling\n: Swarm will schedule containers on hosts based on different filters and strategies.\n\n\n\n\nConsul\n\n\nConsul it is a tool for discovering and configuring services in your infrastructure.\n\n\nKey features\n\n\n\n\nService Discovery\n: Clients of Consul can provide a service, such as api or mysql and other clients can use \n  Consul to discover providers of a given service. Using either DNS or HTTP. Applications can easily find the services \n  they depend upon.\n\n\nHealth Checking\n: Consul clients can provide any number of health checks, either associated with a given service (\"is the webserver returning 200 OK\"), or with the local node (\"is memory utilization below 90%\"). This information can be used by an operator to monitor cluster health, and it is used by the service discovery components to route traffic away from unhealthy hosts.\n\n\nKey/Value Store\n: Applications can make use of Consul's hierarchical key/value store for any number of \n  purposes, including dynamic configuration, feature flagging, coordination, leader election and more. The simple HTTP API makes it easy to use.\n\n\n\n\nMulti Datacenter\n: Consul supports multiple datacenters out of the box. This means users of Consul do not have to worry about building additional layers of abstraction to grow to multiple regions.", 
            "title": "Architecture"
        }, 
        {
            "location": "/architecture/#architecture", 
            "text": "", 
            "title": "Architecture"
        }, 
        {
            "location": "/architecture/#cloudbreak-deployer-architecture", 
            "text": "uaa : OAuth Identity Server  cloudbreak : the Cloudbreak app  periscope : the Periscope app  uluwatu : Cloudbreak UI  sultans : user management", 
            "title": "Cloudbreak Deployer Architecture"
        }, 
        {
            "location": "/architecture/#system-level-containers", 
            "text": "consul : Service Registry  registrator : automatically registers/unregisters containers with consul", 
            "title": "System Level Containers"
        }, 
        {
            "location": "/architecture/#cloudbreak-application-architecture", 
            "text": "Cloudbreak is built on the foundation of cloud providers APIs, Apache Ambari, Docker containers, Swarm and Consul.", 
            "title": "Cloudbreak Application Architecture"
        }, 
        {
            "location": "/architecture/#apache-ambari", 
            "text": "The Apache Ambari project is aimed at making Hadoop management simpler by developing software for provisioning, managing, and monitoring Apache Hadoop clusters. Ambari provides an intuitive, easy-to-use Hadoop management web UI backed by its RESTful APIs.", 
            "title": "Apache Ambari"
        }, 
        {
            "location": "/architecture/#system-administrators", 
            "text": "Ambari enables to integrate Hadoop provisioning, management and monitoring capabilities into applications with the Ambari REST APIs.   Provision a Hadoop Cluster :  Ambari provides a step-by-step wizard for installing Hadoop services across any number of hosts.  Ambari handles configuration of Hadoop services for the cluster.    Manage a Hadoop Cluster :  Ambari provides central management for starting, stopping, and reconfiguring Hadoop services across the entire.\n   cluster.    Monitor a Hadoop Cluster :  Ambari provides a dashboard for monitoring health and status of the Hadoop cluster.  Ambari allows to choose between predefined alerts or add your custom ones.", 
            "title": "System Administrators"
        }, 
        {
            "location": "/architecture/#ambari-blueprint", 
            "text": "Ambari blueprints are a declarative definition of a cluster. With a blueprint, you can specify stack, component\n layout and configurations to materialise a Hadoop cluster instance (via a REST API) without having to use the Ambari\n  Cluster Install Wizard.", 
            "title": "Ambari Blueprint"
        }, 
        {
            "location": "/architecture/#docker", 
            "text": "Docker is an open platform for developers and sysadmins to build, ship and run distributed applications. Consisting \nof Docker Engine, a portable, lightweight runtime and packaging tool and hub. Docker Hub is a cloud service for sharing \napplications and automating the workflow.  Docker enables apps to be quickly assembled from components and eliminates the\n friction between development, QA and production environments. As a result, IT can ship faster and run the same app, \n unchanged, on laptops, data center VMs and any cloud.", 
            "title": "Docker"
        }, 
        {
            "location": "/architecture/#the-main-features-of-docker", 
            "text": "Lightweight  Portable  Build once  Run anywhere   VM - without the overhead of a VM   Each virtualised application includes not only the application and the necessary binaries and libraries, but \n   also an entire guest operating system  The Docker Engine container comprises just the application and its dependencies. It runs as an isolated process \n   in userspace on the host operating system, sharing the kernel with other containers.     Containers are isolated   It can be automated and scripted", 
            "title": "The main features of Docker"
        }, 
        {
            "location": "/architecture/#swarm", 
            "text": "Docker Swarm is native clustering for Docker. It turns a pool of Docker hosts into a single, virtual host. Swarm serves the standard Docker API.   Distributed container orchestration : Allows to remotely orchestrate Docker containers on different hosts.\n      Discovery services : Supports different discovery backends to provide service discovery, as such: token (hosted) \n  and file based, etcd, Consul, Zookeeper.  Advanced scheduling : Swarm will schedule containers on hosts based on different filters and strategies.", 
            "title": "Swarm"
        }, 
        {
            "location": "/architecture/#consul", 
            "text": "Consul it is a tool for discovering and configuring services in your infrastructure.", 
            "title": "Consul"
        }, 
        {
            "location": "/architecture/#key-features", 
            "text": "Service Discovery : Clients of Consul can provide a service, such as api or mysql and other clients can use \n  Consul to discover providers of a given service. Using either DNS or HTTP. Applications can easily find the services \n  they depend upon.  Health Checking : Consul clients can provide any number of health checks, either associated with a given service (\"is the webserver returning 200 OK\"), or with the local node (\"is memory utilization below 90%\"). This information can be used by an operator to monitor cluster health, and it is used by the service discovery components to route traffic away from unhealthy hosts.  Key/Value Store : Applications can make use of Consul's hierarchical key/value store for any number of \n  purposes, including dynamic configuration, feature flagging, coordination, leader election and more. The simple HTTP API makes it easy to use.   Multi Datacenter : Consul supports multiple datacenters out of the box. This means users of Consul do not have to worry about building additional layers of abstraction to grow to multiple regions.", 
            "title": "Key features"
        }, 
        {
            "location": "/onprem/", 
            "text": "Install Cloudbreak Deployer\n\n\nTo install Cloudbreak Deployer on your selected environment you have to follow the steps below. The instruction \ndescribe a CentOS based installation.\n\n\nMinimum and Recommended System Requirements\n\n\nTo run the Cloudbreak Deployer and install the Cloudbreak Application, you must meet the following requirements:\n\n\n\n\nRHEL / CentOS / Oracle Linux 7 (64-bit)\n\n\nDocker 1.9.1\n\n\nMinimum and Recommended VM requirements:\n\n\n4GB RAM\n\n\n10GB disk\n\n\n2 cores\n\n\n\n\n\n\n\n\n\n\nYou can install Cloudbreak on \nMac OS X for evaluation purposes only\n. This operating system is not supported \nfor a production deployment of Cloudbreak.\n\n\n\n\nMake sure you opened the following ports:\n\n\n\n\nSSH (22)\n\n\nCloudbreak API (8080)\n\n\nIdentity server (8089)\n\n\nCloudbreak GUI (3000)\n\n\nUser authentication (3001)\n\n\n\n\nEvery command shall be executed as \nroot\n. In order to get root privileges execute:\n\n\nsudo -i\n\n\n\n\nEnsure that your system is up-to date and reboot if necessary (e.g. there was a kernel update)  :\n\n\nyum -y update\n\n\n\n\nYou need to install iptables-services, otherwise the 'iptables save' command will not be available:\n\n\nyum -y install iptables-services net-tools\n\n\n\n\nPlease configure permissive iptables on your machine:\n\n\niptables --flush INPUT \n \\\niptables --flush FORWARD \n \\\nservice iptables save\n\n\n\n\nConfigure a custom Docker repository for installing the correct version of Docker:\n\n\ncat \n /etc/yum.repos.d/docker.repo \nEOF\n\n[dockerrepo]\nname=Docker Repository\nbaseurl=https://yum.dockerproject.org/repo/main/centos/7\nenabled=1\ngpgcheck=1\ngpgkey=https://yum.dockerproject.org/gpg\nEOF\n\n\n\n\nThen you are able to install the Docker service:\n\n\nyum install -y docker-engine-1.9.1 docker-engine-selinux-1.9.1\nsystemctl start docker\nsystemctl enable docker\n\n\n\n\nInstall Cloudbreak Deployer\n\n\nInstall the Cloudbreak Deployer and unzip the platform specific single binary to your PATH. The one-liner way is:\n\n\nyum -y install unzip tar\ncurl -Ls s3.amazonaws.com/public-repo-1.hortonworks.com/HDP/cloudbreak/cloudbreak-deployer_1.2.1_$(uname)_x86_64.tgz | sudo tar -xz -C /bin cbd\ncbd --version\n\n\n\n\nOnce the Cloudbreak Deployer is installed, you can start to setup the Cloudbreak application.\n\n\nInitialize your Profile\n\n\nFirst initialize \ncbd\n by creating a \nProfile\n file:\n\n\nmkdir cloudbreak-deployment\ncd cloudbreak-deployment\ncbd init\n\n\n\n\nIt will create a \nProfile\n file in the current directory. Please edit the file - the only required\nconfiguration is the \nPUBLIC_IP\n. This IP will be used to access the Cloudbreak UI\n(called Uluwatu). In some cases the \ncbd\n tool tries to guess it, if can't than will give a hint.\n\n\nGenerate your Profile\n\n\nYou are done with the configuration of Cloudbreak Deployer. The last thing you have to do is to regenerate the configurations in order to take effect.\n\n\nrm *.yml\ncbd generate\n\n\n\n\nThis command applies the following steps:\n\n\n\n\ncreates the \ndocker-compose.yml\n file that describes the configuration of all the Docker containers needed for the Cloudbreak deployment.\n\n\ncreates the \nuaa.yml\n file that holds the configuration of the identity server used to authenticate users to Cloudbreak.\n\n\n\n\nStart Cloudbreak\n\n\nTo start the Cloudbreak application use the following command.\nThis will start all the Docker containers and initialize the application. It will take a few minutes until all the services start.\n\n\ncbd pull\ncbd start\n\n\n\n\n\n\nAt the very first time it will take for a while, because of need to download all the necessary docker images.\n\n\n\n\nAfter the \ncbd start\n command finishes you can check the logs of the Cloudbreak application with this command:\n\n\ncbd logs cloudbreak\n\n\n\n\n\n\nCloudbreak should start within a minute - you should see a line like this: \nStarted CloudbreakApplication in 36.823 seconds\n\n\n\n\nTroubleshooting\n\n\nIf you are faced with permission or connection issue, first you can try to disable \nSELinux\n:\n\n\n\n\nSetting the \nSELINUX=disabled\n in \n/etc/selinux/config\n\n\nReboot the machine\n\n\nEnsure the SELinux is not turned on after\n\n\n\n\nsetenforce 0 \n sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config\n\n\n\n\nNext steps\n\n\nNow you have all the pre-requisites for Cloudbreak. You can follow with the \ncloud provider specific\n configuration. Select one of the provider in the header \nthen follow the steps from the \nSetup\n section:\n\n\n\n\nAWS\n\n\nAzure\n\n\nGCP\n\n\nOpenStack\n\n\n\n\n\n\nNote!\n AWS and OpenStack Setup sections contain additional and provider specific \nProfile\n settings.", 
            "title": "Installation"
        }, 
        {
            "location": "/onprem/#install-cloudbreak-deployer", 
            "text": "To install Cloudbreak Deployer on your selected environment you have to follow the steps below. The instruction \ndescribe a CentOS based installation.", 
            "title": "Install Cloudbreak Deployer"
        }, 
        {
            "location": "/onprem/#minimum-and-recommended-system-requirements", 
            "text": "To run the Cloudbreak Deployer and install the Cloudbreak Application, you must meet the following requirements:   RHEL / CentOS / Oracle Linux 7 (64-bit)  Docker 1.9.1  Minimum and Recommended VM requirements:  4GB RAM  10GB disk  2 cores      You can install Cloudbreak on  Mac OS X for evaluation purposes only . This operating system is not supported \nfor a production deployment of Cloudbreak.   Make sure you opened the following ports:   SSH (22)  Cloudbreak API (8080)  Identity server (8089)  Cloudbreak GUI (3000)  User authentication (3001)   Every command shall be executed as  root . In order to get root privileges execute:  sudo -i  Ensure that your system is up-to date and reboot if necessary (e.g. there was a kernel update)  :  yum -y update  You need to install iptables-services, otherwise the 'iptables save' command will not be available:  yum -y install iptables-services net-tools  Please configure permissive iptables on your machine:  iptables --flush INPUT   \\\niptables --flush FORWARD   \\\nservice iptables save  Configure a custom Docker repository for installing the correct version of Docker:  cat   /etc/yum.repos.d/docker.repo  EOF \n[dockerrepo]\nname=Docker Repository\nbaseurl=https://yum.dockerproject.org/repo/main/centos/7\nenabled=1\ngpgcheck=1\ngpgkey=https://yum.dockerproject.org/gpg\nEOF  Then you are able to install the Docker service:  yum install -y docker-engine-1.9.1 docker-engine-selinux-1.9.1\nsystemctl start docker\nsystemctl enable docker", 
            "title": "Minimum and Recommended System Requirements"
        }, 
        {
            "location": "/onprem/#install-cloudbreak-deployer_1", 
            "text": "Install the Cloudbreak Deployer and unzip the platform specific single binary to your PATH. The one-liner way is:  yum -y install unzip tar\ncurl -Ls s3.amazonaws.com/public-repo-1.hortonworks.com/HDP/cloudbreak/cloudbreak-deployer_1.2.1_$(uname)_x86_64.tgz | sudo tar -xz -C /bin cbd\ncbd --version  Once the Cloudbreak Deployer is installed, you can start to setup the Cloudbreak application.", 
            "title": "Install Cloudbreak Deployer"
        }, 
        {
            "location": "/onprem/#initialize-your-profile", 
            "text": "First initialize  cbd  by creating a  Profile  file:  mkdir cloudbreak-deployment\ncd cloudbreak-deployment\ncbd init  It will create a  Profile  file in the current directory. Please edit the file - the only required\nconfiguration is the  PUBLIC_IP . This IP will be used to access the Cloudbreak UI\n(called Uluwatu). In some cases the  cbd  tool tries to guess it, if can't than will give a hint.", 
            "title": "Initialize your Profile"
        }, 
        {
            "location": "/onprem/#generate-your-profile", 
            "text": "You are done with the configuration of Cloudbreak Deployer. The last thing you have to do is to regenerate the configurations in order to take effect.  rm *.yml\ncbd generate  This command applies the following steps:   creates the  docker-compose.yml  file that describes the configuration of all the Docker containers needed for the Cloudbreak deployment.  creates the  uaa.yml  file that holds the configuration of the identity server used to authenticate users to Cloudbreak.", 
            "title": "Generate your Profile"
        }, 
        {
            "location": "/onprem/#start-cloudbreak", 
            "text": "To start the Cloudbreak application use the following command.\nThis will start all the Docker containers and initialize the application. It will take a few minutes until all the services start.  cbd pull\ncbd start   At the very first time it will take for a while, because of need to download all the necessary docker images.   After the  cbd start  command finishes you can check the logs of the Cloudbreak application with this command:  cbd logs cloudbreak   Cloudbreak should start within a minute - you should see a line like this:  Started CloudbreakApplication in 36.823 seconds", 
            "title": "Start Cloudbreak"
        }, 
        {
            "location": "/onprem/#troubleshooting", 
            "text": "If you are faced with permission or connection issue, first you can try to disable  SELinux :   Setting the  SELINUX=disabled  in  /etc/selinux/config  Reboot the machine  Ensure the SELinux is not turned on after   setenforce 0   sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/onprem/#next-steps", 
            "text": "Now you have all the pre-requisites for Cloudbreak. You can follow with the  cloud provider specific  configuration. Select one of the provider in the header \nthen follow the steps from the  Setup  section:   AWS  Azure  GCP  OpenStack    Note!  AWS and OpenStack Setup sections contain additional and provider specific  Profile  settings.", 
            "title": "Next steps"
        }, 
        {
            "location": "/releasenotes/", 
            "text": "Release Notes\n\n\nThe Release Notes summarize and describe changes released in Cloudbreak.\n\n\nFixes\n\n\nThis release includes the following fixes and improvements:\n\n\n\n\n\n\n\n\nFeature\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCloudbreak Cluster installation\n\n\nFixed hanging cluster installation\n.\n\n\n\n\n\n\nAzure Persistent Storage name fix\n\n\nStorage name generation was buggy after some API change.\n\n\n\n\n\n\nGCP attached disk type was buggy\n\n\nAttached disk was buggy after some API change.\n\n\n\n\n\n\n\n\nTechnical Preview\n\n\nThis release includes the following Technical Preview features and improvements:\n\n\n\n\n\n\n\n\nFeature\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nMesos\n\n\nTechnical Preview\n Support for Mesos cloud provider.\n\n\n\n\n\n\nKerberos\n\n\nTechnical Preview\n Support for enabling Kerberos on the HDP clusters deployed by Cloudbreak. See \nKerberos\n for more information.\n\n\n\n\n\n\nSSSD\n\n\nTechnical Preview\n Support for configuring System Security Services Daemon (SSSD) to help with cluster user management. See \nSSSD\n for more information.\n\n\n\n\n\n\nPlatforms\n\n\nTechnical Preview\n Support for defining Platforms to relate different configurations together. See \nPlatforms\n for more information.\n\n\n\n\n\n\n\n\nBehavioral Changes\n\n\nThis release introduces the following changes in behavior as compared to previous Cloudbreak versions:\n\n\n\n\n\n\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nCloudbreak Web UI\n\n\nNetwork creation form changes (easier creation of existing network).", 
            "title": "Release Notes"
        }, 
        {
            "location": "/releasenotes/#release-notes", 
            "text": "The Release Notes summarize and describe changes released in Cloudbreak.", 
            "title": "Release Notes"
        }, 
        {
            "location": "/releasenotes/#fixes", 
            "text": "This release includes the following fixes and improvements:     Feature  Description      Cloudbreak Cluster installation  Fixed hanging cluster installation .    Azure Persistent Storage name fix  Storage name generation was buggy after some API change.    GCP attached disk type was buggy  Attached disk was buggy after some API change.", 
            "title": "Fixes"
        }, 
        {
            "location": "/releasenotes/#technical-preview", 
            "text": "This release includes the following Technical Preview features and improvements:     Feature  Description      Mesos  Technical Preview  Support for Mesos cloud provider.    Kerberos  Technical Preview  Support for enabling Kerberos on the HDP clusters deployed by Cloudbreak. See  Kerberos  for more information.    SSSD  Technical Preview  Support for configuring System Security Services Daemon (SSSD) to help with cluster user management. See  SSSD  for more information.    Platforms  Technical Preview  Support for defining Platforms to relate different configurations together. See  Platforms  for more information.", 
            "title": "Technical Preview"
        }, 
        {
            "location": "/releasenotes/#behavioral-changes", 
            "text": "This release introduces the following changes in behavior as compared to previous Cloudbreak versions:     Title  Description      Cloudbreak Web UI  Network creation form changes (easier creation of existing network).", 
            "title": "Behavioral Changes"
        }, 
        {
            "location": "/changelog/", 
            "text": "Change Log\n\n\nThe Change Log summarizes the changes in Cloudbreak.\n\n\nUnreleased\n\n\nFixed\n\n\n\n\nconsul recursor now exculdes both docker ip and bridge ip to avoid recursive dns recursor chain\n\n\ndocs fixed about getting default credentials (cbd login)\n\n\nupdates cb-shell to 0.5.37 to fix ssl issues\n\n\n\n\nAdded\n\n\n\n\nCommand \ncbd azure configure-arm\n will create your arm application which can used by cloudbreak\n\n\nCommand \ncbd azure deploy-dash\n will deploy a dash application in your Azure account\n\n\nCommand \ncbd start\n will execute the migration by default. If SKIP_DB_MIGRATION_ON_START envvar set to true in Profile, the migration will be skipped\n\n\nUsing Dns SRV record in our services instead of ambassador\n\n\nUsing docker linking system in third party services instead of ambassador\n\n\nIntegration tests are added, where cbd binary is called, not only sourced functions\n\n\nDocker based CentOS integration test make target added\n\n\nUaa db migration\n\n\nSMTP default parameters added: \nCLOUDBREAK_SMTP_AUTH\n and \nCLOUDBREAK_SMTP_STARTTLS_ENABLE\n and \nCLOUDBREAK_SMTP_TYPE\n\n\nLocal development Uluwatu configuration by ULUWATU_VOLUME_HOST environment variable\n\n\nLocal development Sultans configuration by SULTANS_VOLUME_HOST environment variable\n\n\ninstall script for fixed version and install-latest for latest release added\n\n\nEach snapshot artifact is uploaded as http://public-repo-1.hortonworks.com/HDP/cloudbreak/cbd-snapshot-$(uname).tgz\n\n\nConfiguration ability to enable or disable ssh fingerprint verification of virtual machines on GCP and AWS\n\n\n\n\nRemoved\n\n\n\n\nFull removal of ambassador\n\n\n\n\nChanged\n\n\n\n\ncbd start\n doesn\u2019t start if compose yaml regeneration is needed\n\n\ncbd generate\n is less verbose, diff doesnt shown\n\n\ncbd doctor\n shows diff if generate would change\n\n\ncbd regenerate\n creates backup files if changes detected\n\n\nsequenceiq/uaadb:1.0.1 is used instead of postgres:9.4.1\n\n\n\n\n[v1.0.3] - 2015-09-03\n\n\nFixed\n\n\n\n\nAuthentication error with \ncloudbreak-shell\n and \ncloudbreak-shell-quiet\n is fixed\n\n\nCommand \ncbd update \nbranch\n checks for artifact\n\n\n\n\nAdded\n\n\n\n\nbinary version of gnu-sed 4.2.2 is now included, to solve lot of osx/busybox issues\n\n\nconsul recursor test are added\n\n\n\n\nRemoved\n\n\nChanged\n\n\n\n\n\n\nsequenceiq/cloudbreak image updated to 1.0.3\n\n\n\n\n\n\ndebug() function made multiline capable. Use \\n in messages\n\n\n\n\nrefactor bridge ip discovery to run helper docker command only once\n\n\nconsul recursor handling refactored to be more robust\n\n\n\n\n[v1.0.2] - 2015-08-25\n\n\nFixed\n\n\nAdded\n\n\n\n\nDOCKER_CONSUL_OPTIONS\n config option to provide arbitrary consul option\n\n\n\n\nRemoved\n\n\nChanged\n\n\n\n\nFixed docker version checker to be 1.8.1 compatible. (docker added --format option)\n\n\nsequenceiq/cloudbreak image updated to 1.0.2\n\n\nconsul image changed from sequenceiq/consul to gliderlabs/consul\n\n\nconsul image updated to 0.5.2 (from 0.5.0)\n\n\nconsul discovers host dns settings, and uses the configured nameserver as recursor\n\n\n\n\n[v1.0.0] - 2015-08-15\n\n\nFixed\n\n\nAdded\n\n\nRemoved\n\n\nChanged\n\n\n[v1.0.0] - 2015-07-23\n\n\nFixed\n\n\n\n\nGA Release\n\n\n\n\nAdded\n\n\nRemoved\n\n\nChanged\n\n\n[v0.5.8] - 2015-07-23\n\n\nFixed\n\n\n\n\nFix CircleCI release. CircleCI doesn\u2019t allow --rm on docker run\n\n\n\n\nAdded\n\n\nRemoved\n\n\nChanged\n\n\n[v0.5.7] - 2015-07-23\n\n\nFixed\n\n\n\n\nFix make release dependency\n\n\nFix CHANGELOG generation at \nmake release-next-ver\n avoid inserting extra -e\n\n\n\n\nAdded\n\n\nRemoved\n\n\nChanged\n\n\n[v0.5.6] - 2015-07-23\n\n\nFixed\n\n\nAdded\n\n\n\n\nRelease artifacts are published at public-repo-1.hortonworks.com/HDP/cloudbreak/\n\n\n\n\nRemoved\n\n\nChanged\n\n\n[v0.5.5] - 2015-07-10\n\n\nFixed\n\n\nAdded\n\n\n\n\nCommand \npull-parallel\n added for quicker/simultaneous image pull\n\n\nRelease process includes upload to public-repo s3 bucket\n\n\n\n\nRemoved\n\n\nChanged\n\n\n\n\nLicense changed from MIT to Apache v2\n\n\nrelease artifact includes additional files: license/readme/notes\n\n\n\n\n[v0.5.4] - 2015-07-03\n\n\nFixed\n\n\nAdded\n\n\n\n\nNew \ncbd-cleanup\n command for removing old images or exited containers\n\n\nBaywatch default parameters added: \nCB_BAYWATCH_ENABLED\n and\nCB_BAYWATCH_EXTERN_LOCATION\n\n\nLogs are saved via lospout\n\n\nTLS client certificate needed by Cloudbreak is generated with \ncbd generate\n\n\nCommand \naws delete-role\n added\n\n\nCommand \naws generate-role\n added\n\n\nCommand \naws show-role\n added\n\n\nCommand \ncloudbreak-shell\n added\n\n\nCommand \ncloudbreak-shell-quiet\n added\n\n\nCommand \nlocal-dev\n added\n\n\nCommand \ntoken\n added\n\n\n\n\nRemoved\n\n\nChanged\n\n\n\n\nAWS authentication env varibale is fixed to use the correct AWS_SECRET_ACCESS_KEY (instead the old AWS_SECRET_KEY)\n\n\nUsing sequenceiq/ambassadord:0.5.0 docker image instead of progrium/ambassadord:latest\n\n\n\n\n[v0.5.3] - 2015-06-03\n\n\nFixed\n\n\n\n\nOne-liner installer fixed, to work if previous cbd exists on path.\n\n\ncbd update\n upstream changes on go-bahser broke the selfupdate functionality\n\n\nIn some environment cloudbreak starts really slow. See: \ndetails\n, see: \ncommit\n\n\n\n\nAdded\n\n\n\n\nNew release proposal can be done by \nmake release-next-ver\n\n\n\n\nRemoved\n\n\nChanged\n\n\n[v0.5.2] - 2015-05-21\n\n\nFixed\n\n\nAdded\n\n\nRemoved\n\n\nChanged\n\n\n\n\nCommand \ndoctor\n hints to run boot2docker shellinit if env is unset\n\n\nCommand \ninit\n in case of OSX, DOCKER_XXX envs are initialized in local profile (Profile)\n\n\nDefault docker images are updated to:\n\n\nsequenceiq/cloudbreak:0.5.93\n\n\nsequenceiq/cbdb:0.5.92\n\n\nsequenceiq/uluwatu:0.5.28\n\n\nsequenceiq/sultans:0.5.3\n\n\nsequenceiq/periscope:0.5.5\n\n\n\n\n\n\n\n\n[v0.5.1] - 2015-05-18\n\n\nFixed\n\n\n\n\nIssue #55: Sed handles more robust the issue with: curl includes an extra CR char in header fields.\n\n\n\n\nAdded\n\n\nRemoved\n\n\n\n\ndeployer doesn\u2019t specify cloud specific image defaults. If left empty, they fall back\n  to defaults specified in \njava code\n\n\nCB_AZURE_IMAGE_URI\n\n\nCB_AWS_AMI_MAP\n\n\nCB_OPENSTACK_IMAGE\n\n\nCB_GCP_SOURCE_IMAGE_PATH\n\n\n\n\n\n\n\n\nChanged\n\n\n\n\nCommand \nlogs\n got usage example for specifying servies as filter\n\n\nDefault docker images are updated to:\n\n\nsequenceiq/cloudbreak:0.5.49\n\n\nsequenceiq/uluwatu:0.5.16\n\n\nsequenceiq/sutans:0.5.2\n\n\n\n\n\n\n\n\n[v0.5.0] - 2015-05-08\n\n\nFixed\n\n\n\n\nCommand \npull\n generates yaml files in case they are missing #31\n\n\n\n\nAdded\n\n\n\n\nCommand \nlogin\n Shows Uluwatu login url and credentials\n\n\nCommand \nregenerate\n deletes and generates docker-compose.yml and uaa.yml\n\n\nCommand \ndelete\n added: deletes yamls and dbs\n\n\nCommand \ncloudbreak-shell\n added, right now it internale use DEBUG=1 fn fn-call\n\n\nCommand \nversion\n does correct \nSemantic Versioning\n check to advise an upgrade\n\n\nCommand \ngenerate\n checks and shows if Profile change would result in yaml change.\n\n\nCommand \nstart\n: prints uluwatu url and credential hint\n\n\nCommand \ndoctor\n: fixes boot2docker date/time if not the same as on the host\n\n\nInternal command: \nbrowse\n added to be able to automatically open a browser to a specified url.\n\n\nMini Getting Started guide added into README\n\n\nmake dev-debug\n installs a special cbd on local OSX, which doesn\u2019t includes *.bash scrips, only refers them\n   by path. No need to \nmake dev\n to test small changes in bash scripts.\n\n\nLoad AWS key and AWS id from Profile\n\n\nCommand \ninit\n helps to guess the PUBLIC_IP in public clouds: google, amazon\n\n\n\n\nRemoved\n\n\nChanged\n\n\n\n\nCommand \ncbd env export\n adds export to the begining of each line\n\n\ncbd logs accepts optional [services] parameter\n\n\ndocker-compose uses \ncbreak_\n prefix for container naming instead of the directory name\n\n\nCommand \ngenerate\n prints out some more usefull info\n\n\nuaa.yml generation wont overwrite, just instruct to move existing file (like docker-compose.yml generation)\n\n\nCommand \ninit\n hint fixed on linux.\n\n\nCommand \ninit\n advise to run \ngenerate\n if it finds a Profile\n\n\nCommand \ninit\n set PRIVATE_IP the same as PUBLIC_IP for boot2docker\n\n\nCommand \nmigrate\n is introduced for db migration see \nMigrate the databases\n section of README\n\n\nCommand \nstartdb\n starts the cbdb and pcdb database containers only\n\n\nDatabases are not deleted after boot2docker restart\n\n\nImport ULU_HOST_ADDRESS and ULU_SULTANS_ADDRESS from Profile\n\n\n\n\n[v0.1.0] - 2015-04-16\n\n\nFixed\n\n\n\n\nSelfupdate updates the actual running binary intead of the fixed /us/local/bin/cbd\n\n\nSMTP default port is 25, to fix number conversion exception\n\n\n\n\nAdded\n\n\n\n\nCommand \ninit\n creates Profile\n\n\nInstall cbd to a directory which is available on $PATH\n\n\nDocker based test for the one-liner install from README.md: \nmake install-test\n\n\n\n\nRemoved\n\n\n\n\nupdate-snap\n command removed, replaced by parametrized \nupdate\n\n\n\n\nChanged\n\n\n\n\nCloudbreak/Persicope/Uluwatu/Sultans Dcoker images upgraded to 0.4.x\n\n\nUse the built in 'checksum' function instead of the external 'shasum' to generate secrets\n\n\nCommand \nupdate\n by default updates from latest Github release, parameter can point to branch on CircleCI\n\n\nDOCKER_XXX env varibles are inherited, so they not needed in Profile\n\n\ngenerate\n and compose specific commands are only available when \nProfile\n exists\n\n\ngenerate\n command genertes docker-compose.yml \nand\n uaa.yml\n\n\nPRIVATE_IP\n env var defaults to bridge IP (only PUBLC_IP is required in Profile)\n\n\nuse \nsulans-bin\n docker image istead of sultans\n\n\n\n\n[v0.0.9] - 2015-04-14\n\n\nFixed\n\n\n\n\nBash 4.3 is included in the binary, extracted into .deps/bin upon start\n\n\n\n\nAdded\n\n\nRemoved\n\n\nChanged\n\n\n[v0.0.8] - 2015-04-13\n\n\nFixed\n\n\n\n\nFixing deps module, golang fn: checksum added\n\n\nCircleCI mdule defines required jq\n\n\nFixing PATH issue for binary deps\n\n\n\n\nAdded\n\n\n\n\nuaadb start added\n\n\nidentity server start added\n\n\nmake dev\n added to mac based development\n\n\npull\n command added\n\n\nlogs\n command added\n\n\n\n\nRemoved\n\n\nChanged\n\n\n\n\nDocker containers are managed by \ndocker-compose\n\n\n\n\n[v0.0.7] - 2015-03-26\n\n\nFixed\n\n\nAdded\n\n\n\n\nmake tests\n runs unit tests\n\n\ndocker unit tests are added\n\n\nstart command added: WIP consul, registrator starts\n\n\nkill command addd: stops and removes cloudbreak specific containers\n\n\nSKIP_XXX skips the container start\n\n\n\n\nRemoved\n\n\nChanged\n\n\n\n\nenv command namespace is always exported, not only in DEBUG mode\n\n\nenv export: machine friendly config list\n\n\nenv show: human readable config list\n\n\ncircle runs unit tests\n\n\nsnapshot binaries include branch name in version string\n\n\n\n\n[v0.0.6] - 2015-03-25\n\n\nFixed\n\n\n\n\nremoved dos2unix dependency for the update command\n\n\n\n\nAdded\n\n\n\n\ndoctor command added\n\n\ndocker-check-version command added\n\n\ncci-latest accepts branch as parameter, needed for PR testing\n\n\nexport fn command in DEBUG mode\n\n\nexport env command in DEBUG mode\n\n\ndoctor: add instruction about setting DOCKER_XXX env vars in Profile\n\n\ninfo() function added to print green text to STDOUT\n\n\n\n\nRemoved\n\n\nChanged\n\n\n\n\nHOME env var is also inherited (boot2docker version failed)\n\n\nrelease process fully automatized\n\n\n\n\n[v0.0.5] - 2015-03-23\n\n\n\n\nupdate\n command works without dos2unix\n\n\n\n\n[v0.0.4] - 2015-03-23\n\n\nFixed\n\n\n\n\ndebug function fixed\n\n\nDEBUG, TRACE and CBD_DEFAULT_PROFILE env vars are inherited\n\n\n\n\nAdded\n\n\n\n\nProfile handling added with docs\n\n\nOne-liner install added\n\n\nDocs: install and update process described\n\n\nDocs: release process described with sample git commands\n\n\nPrint version number in debug mode\n\n\nupdate-snap\n downloads binary from latest os specific CircleCI binary artifact.\n\n\n\n\nRemoved\n\n\nChanged\n\n\n\n\nTool specific library renamed from cloudbreak.bash to deployer.bash\n\n\n\n\n[v0.0.6] - 2015-03-25\n\n\n[v0.0.3] - 2015-03-19\n\n\nFixed\n\n\n\n\nmake release\n creates binary with X.X.X version when on release branch otherwise X.X.X-gitrev\n\n\n\n\nAdded\n\n\n\n\nDocs: release process described\n\n\n\n\nRemoved\n\n\nChanged\n\n\n[v0.0.2] - 2015-03-19\n\n\nAdded\n\n\nAdded\n- selfupdate command\n- gray debug to stderr\n\n\n[v0.0.1] - 2015-03-18\n\n\nAdded\n\n\n\n\nhelp command added\n\n\nversion command added\n\n\nAdded --version\n\n\nCircleCI build\n\n\nLinux/Darwin binary releases on github", 
            "title": "Change Log"
        }, 
        {
            "location": "/changelog/#change-log", 
            "text": "The Change Log summarizes the changes in Cloudbreak.", 
            "title": "Change Log"
        }, 
        {
            "location": "/changelog/#unreleased", 
            "text": "", 
            "title": "Unreleased"
        }, 
        {
            "location": "/changelog/#fixed", 
            "text": "consul recursor now exculdes both docker ip and bridge ip to avoid recursive dns recursor chain  docs fixed about getting default credentials (cbd login)  updates cb-shell to 0.5.37 to fix ssl issues", 
            "title": "Fixed"
        }, 
        {
            "location": "/changelog/#added", 
            "text": "Command  cbd azure configure-arm  will create your arm application which can used by cloudbreak  Command  cbd azure deploy-dash  will deploy a dash application in your Azure account  Command  cbd start  will execute the migration by default. If SKIP_DB_MIGRATION_ON_START envvar set to true in Profile, the migration will be skipped  Using Dns SRV record in our services instead of ambassador  Using docker linking system in third party services instead of ambassador  Integration tests are added, where cbd binary is called, not only sourced functions  Docker based CentOS integration test make target added  Uaa db migration  SMTP default parameters added:  CLOUDBREAK_SMTP_AUTH  and  CLOUDBREAK_SMTP_STARTTLS_ENABLE  and  CLOUDBREAK_SMTP_TYPE  Local development Uluwatu configuration by ULUWATU_VOLUME_HOST environment variable  Local development Sultans configuration by SULTANS_VOLUME_HOST environment variable  install script for fixed version and install-latest for latest release added  Each snapshot artifact is uploaded as http://public-repo-1.hortonworks.com/HDP/cloudbreak/cbd-snapshot-$(uname).tgz  Configuration ability to enable or disable ssh fingerprint verification of virtual machines on GCP and AWS", 
            "title": "Added"
        }, 
        {
            "location": "/changelog/#removed", 
            "text": "Full removal of ambassador", 
            "title": "Removed"
        }, 
        {
            "location": "/changelog/#changed", 
            "text": "cbd start  doesn\u2019t start if compose yaml regeneration is needed  cbd generate  is less verbose, diff doesnt shown  cbd doctor  shows diff if generate would change  cbd regenerate  creates backup files if changes detected  sequenceiq/uaadb:1.0.1 is used instead of postgres:9.4.1", 
            "title": "Changed"
        }, 
        {
            "location": "/changelog/#v103-2015-09-03", 
            "text": "", 
            "title": "[v1.0.3] - 2015-09-03"
        }, 
        {
            "location": "/changelog/#fixed_1", 
            "text": "Authentication error with  cloudbreak-shell  and  cloudbreak-shell-quiet  is fixed  Command  cbd update  branch  checks for artifact", 
            "title": "Fixed"
        }, 
        {
            "location": "/changelog/#added_1", 
            "text": "binary version of gnu-sed 4.2.2 is now included, to solve lot of osx/busybox issues  consul recursor test are added", 
            "title": "Added"
        }, 
        {
            "location": "/changelog/#removed_1", 
            "text": "", 
            "title": "Removed"
        }, 
        {
            "location": "/changelog/#changed_1", 
            "text": "sequenceiq/cloudbreak image updated to 1.0.3    debug() function made multiline capable. Use \\n in messages   refactor bridge ip discovery to run helper docker command only once  consul recursor handling refactored to be more robust", 
            "title": "Changed"
        }, 
        {
            "location": "/changelog/#v102-2015-08-25", 
            "text": "", 
            "title": "[v1.0.2] - 2015-08-25"
        }, 
        {
            "location": "/changelog/#fixed_2", 
            "text": "", 
            "title": "Fixed"
        }, 
        {
            "location": "/changelog/#added_2", 
            "text": "DOCKER_CONSUL_OPTIONS  config option to provide arbitrary consul option", 
            "title": "Added"
        }, 
        {
            "location": "/changelog/#removed_2", 
            "text": "", 
            "title": "Removed"
        }, 
        {
            "location": "/changelog/#changed_2", 
            "text": "Fixed docker version checker to be 1.8.1 compatible. (docker added --format option)  sequenceiq/cloudbreak image updated to 1.0.2  consul image changed from sequenceiq/consul to gliderlabs/consul  consul image updated to 0.5.2 (from 0.5.0)  consul discovers host dns settings, and uses the configured nameserver as recursor", 
            "title": "Changed"
        }, 
        {
            "location": "/changelog/#v100-2015-08-15", 
            "text": "", 
            "title": "[v1.0.0] - 2015-08-15"
        }, 
        {
            "location": "/changelog/#fixed_3", 
            "text": "", 
            "title": "Fixed"
        }, 
        {
            "location": "/changelog/#added_3", 
            "text": "", 
            "title": "Added"
        }, 
        {
            "location": "/changelog/#removed_3", 
            "text": "", 
            "title": "Removed"
        }, 
        {
            "location": "/changelog/#changed_3", 
            "text": "", 
            "title": "Changed"
        }, 
        {
            "location": "/changelog/#v100-2015-07-23", 
            "text": "", 
            "title": "[v1.0.0] - 2015-07-23"
        }, 
        {
            "location": "/changelog/#fixed_4", 
            "text": "GA Release", 
            "title": "Fixed"
        }, 
        {
            "location": "/changelog/#added_4", 
            "text": "", 
            "title": "Added"
        }, 
        {
            "location": "/changelog/#removed_4", 
            "text": "", 
            "title": "Removed"
        }, 
        {
            "location": "/changelog/#changed_4", 
            "text": "", 
            "title": "Changed"
        }, 
        {
            "location": "/changelog/#v058-2015-07-23", 
            "text": "", 
            "title": "[v0.5.8] - 2015-07-23"
        }, 
        {
            "location": "/changelog/#fixed_5", 
            "text": "Fix CircleCI release. CircleCI doesn\u2019t allow --rm on docker run", 
            "title": "Fixed"
        }, 
        {
            "location": "/changelog/#added_5", 
            "text": "", 
            "title": "Added"
        }, 
        {
            "location": "/changelog/#removed_5", 
            "text": "", 
            "title": "Removed"
        }, 
        {
            "location": "/changelog/#changed_5", 
            "text": "", 
            "title": "Changed"
        }, 
        {
            "location": "/changelog/#v057-2015-07-23", 
            "text": "", 
            "title": "[v0.5.7] - 2015-07-23"
        }, 
        {
            "location": "/changelog/#fixed_6", 
            "text": "Fix make release dependency  Fix CHANGELOG generation at  make release-next-ver  avoid inserting extra -e", 
            "title": "Fixed"
        }, 
        {
            "location": "/changelog/#added_6", 
            "text": "", 
            "title": "Added"
        }, 
        {
            "location": "/changelog/#removed_6", 
            "text": "", 
            "title": "Removed"
        }, 
        {
            "location": "/changelog/#changed_6", 
            "text": "", 
            "title": "Changed"
        }, 
        {
            "location": "/changelog/#v056-2015-07-23", 
            "text": "", 
            "title": "[v0.5.6] - 2015-07-23"
        }, 
        {
            "location": "/changelog/#fixed_7", 
            "text": "", 
            "title": "Fixed"
        }, 
        {
            "location": "/changelog/#added_7", 
            "text": "Release artifacts are published at public-repo-1.hortonworks.com/HDP/cloudbreak/", 
            "title": "Added"
        }, 
        {
            "location": "/changelog/#removed_7", 
            "text": "", 
            "title": "Removed"
        }, 
        {
            "location": "/changelog/#changed_7", 
            "text": "", 
            "title": "Changed"
        }, 
        {
            "location": "/changelog/#v055-2015-07-10", 
            "text": "", 
            "title": "[v0.5.5] - 2015-07-10"
        }, 
        {
            "location": "/changelog/#fixed_8", 
            "text": "", 
            "title": "Fixed"
        }, 
        {
            "location": "/changelog/#added_8", 
            "text": "Command  pull-parallel  added for quicker/simultaneous image pull  Release process includes upload to public-repo s3 bucket", 
            "title": "Added"
        }, 
        {
            "location": "/changelog/#removed_8", 
            "text": "", 
            "title": "Removed"
        }, 
        {
            "location": "/changelog/#changed_8", 
            "text": "License changed from MIT to Apache v2  release artifact includes additional files: license/readme/notes", 
            "title": "Changed"
        }, 
        {
            "location": "/changelog/#v054-2015-07-03", 
            "text": "", 
            "title": "[v0.5.4] - 2015-07-03"
        }, 
        {
            "location": "/changelog/#fixed_9", 
            "text": "", 
            "title": "Fixed"
        }, 
        {
            "location": "/changelog/#added_9", 
            "text": "New  cbd-cleanup  command for removing old images or exited containers  Baywatch default parameters added:  CB_BAYWATCH_ENABLED  and CB_BAYWATCH_EXTERN_LOCATION  Logs are saved via lospout  TLS client certificate needed by Cloudbreak is generated with  cbd generate  Command  aws delete-role  added  Command  aws generate-role  added  Command  aws show-role  added  Command  cloudbreak-shell  added  Command  cloudbreak-shell-quiet  added  Command  local-dev  added  Command  token  added", 
            "title": "Added"
        }, 
        {
            "location": "/changelog/#removed_9", 
            "text": "", 
            "title": "Removed"
        }, 
        {
            "location": "/changelog/#changed_9", 
            "text": "AWS authentication env varibale is fixed to use the correct AWS_SECRET_ACCESS_KEY (instead the old AWS_SECRET_KEY)  Using sequenceiq/ambassadord:0.5.0 docker image instead of progrium/ambassadord:latest", 
            "title": "Changed"
        }, 
        {
            "location": "/changelog/#v053-2015-06-03", 
            "text": "", 
            "title": "[v0.5.3] - 2015-06-03"
        }, 
        {
            "location": "/changelog/#fixed_10", 
            "text": "One-liner installer fixed, to work if previous cbd exists on path.  cbd update  upstream changes on go-bahser broke the selfupdate functionality  In some environment cloudbreak starts really slow. See:  details , see:  commit", 
            "title": "Fixed"
        }, 
        {
            "location": "/changelog/#added_10", 
            "text": "New release proposal can be done by  make release-next-ver", 
            "title": "Added"
        }, 
        {
            "location": "/changelog/#removed_10", 
            "text": "", 
            "title": "Removed"
        }, 
        {
            "location": "/changelog/#changed_10", 
            "text": "", 
            "title": "Changed"
        }, 
        {
            "location": "/changelog/#v052-2015-05-21", 
            "text": "", 
            "title": "[v0.5.2] - 2015-05-21"
        }, 
        {
            "location": "/changelog/#fixed_11", 
            "text": "", 
            "title": "Fixed"
        }, 
        {
            "location": "/changelog/#added_11", 
            "text": "", 
            "title": "Added"
        }, 
        {
            "location": "/changelog/#removed_11", 
            "text": "", 
            "title": "Removed"
        }, 
        {
            "location": "/changelog/#changed_11", 
            "text": "Command  doctor  hints to run boot2docker shellinit if env is unset  Command  init  in case of OSX, DOCKER_XXX envs are initialized in local profile (Profile)  Default docker images are updated to:  sequenceiq/cloudbreak:0.5.93  sequenceiq/cbdb:0.5.92  sequenceiq/uluwatu:0.5.28  sequenceiq/sultans:0.5.3  sequenceiq/periscope:0.5.5", 
            "title": "Changed"
        }, 
        {
            "location": "/changelog/#v051-2015-05-18", 
            "text": "", 
            "title": "[v0.5.1] - 2015-05-18"
        }, 
        {
            "location": "/changelog/#fixed_12", 
            "text": "Issue #55: Sed handles more robust the issue with: curl includes an extra CR char in header fields.", 
            "title": "Fixed"
        }, 
        {
            "location": "/changelog/#added_12", 
            "text": "", 
            "title": "Added"
        }, 
        {
            "location": "/changelog/#removed_12", 
            "text": "deployer doesn\u2019t specify cloud specific image defaults. If left empty, they fall back\n  to defaults specified in  java code  CB_AZURE_IMAGE_URI  CB_AWS_AMI_MAP  CB_OPENSTACK_IMAGE  CB_GCP_SOURCE_IMAGE_PATH", 
            "title": "Removed"
        }, 
        {
            "location": "/changelog/#changed_12", 
            "text": "Command  logs  got usage example for specifying servies as filter  Default docker images are updated to:  sequenceiq/cloudbreak:0.5.49  sequenceiq/uluwatu:0.5.16  sequenceiq/sutans:0.5.2", 
            "title": "Changed"
        }, 
        {
            "location": "/changelog/#v050-2015-05-08", 
            "text": "", 
            "title": "[v0.5.0] - 2015-05-08"
        }, 
        {
            "location": "/changelog/#fixed_13", 
            "text": "Command  pull  generates yaml files in case they are missing #31", 
            "title": "Fixed"
        }, 
        {
            "location": "/changelog/#added_13", 
            "text": "Command  login  Shows Uluwatu login url and credentials  Command  regenerate  deletes and generates docker-compose.yml and uaa.yml  Command  delete  added: deletes yamls and dbs  Command  cloudbreak-shell  added, right now it internale use DEBUG=1 fn fn-call  Command  version  does correct  Semantic Versioning  check to advise an upgrade  Command  generate  checks and shows if Profile change would result in yaml change.  Command  start : prints uluwatu url and credential hint  Command  doctor : fixes boot2docker date/time if not the same as on the host  Internal command:  browse  added to be able to automatically open a browser to a specified url.  Mini Getting Started guide added into README  make dev-debug  installs a special cbd on local OSX, which doesn\u2019t includes *.bash scrips, only refers them\n   by path. No need to  make dev  to test small changes in bash scripts.  Load AWS key and AWS id from Profile  Command  init  helps to guess the PUBLIC_IP in public clouds: google, amazon", 
            "title": "Added"
        }, 
        {
            "location": "/changelog/#removed_13", 
            "text": "", 
            "title": "Removed"
        }, 
        {
            "location": "/changelog/#changed_13", 
            "text": "Command  cbd env export  adds export to the begining of each line  cbd logs accepts optional [services] parameter  docker-compose uses  cbreak_  prefix for container naming instead of the directory name  Command  generate  prints out some more usefull info  uaa.yml generation wont overwrite, just instruct to move existing file (like docker-compose.yml generation)  Command  init  hint fixed on linux.  Command  init  advise to run  generate  if it finds a Profile  Command  init  set PRIVATE_IP the same as PUBLIC_IP for boot2docker  Command  migrate  is introduced for db migration see  Migrate the databases  section of README  Command  startdb  starts the cbdb and pcdb database containers only  Databases are not deleted after boot2docker restart  Import ULU_HOST_ADDRESS and ULU_SULTANS_ADDRESS from Profile", 
            "title": "Changed"
        }, 
        {
            "location": "/changelog/#v010-2015-04-16", 
            "text": "", 
            "title": "[v0.1.0] - 2015-04-16"
        }, 
        {
            "location": "/changelog/#fixed_14", 
            "text": "Selfupdate updates the actual running binary intead of the fixed /us/local/bin/cbd  SMTP default port is 25, to fix number conversion exception", 
            "title": "Fixed"
        }, 
        {
            "location": "/changelog/#added_14", 
            "text": "Command  init  creates Profile  Install cbd to a directory which is available on $PATH  Docker based test for the one-liner install from README.md:  make install-test", 
            "title": "Added"
        }, 
        {
            "location": "/changelog/#removed_14", 
            "text": "update-snap  command removed, replaced by parametrized  update", 
            "title": "Removed"
        }, 
        {
            "location": "/changelog/#changed_14", 
            "text": "Cloudbreak/Persicope/Uluwatu/Sultans Dcoker images upgraded to 0.4.x  Use the built in 'checksum' function instead of the external 'shasum' to generate secrets  Command  update  by default updates from latest Github release, parameter can point to branch on CircleCI  DOCKER_XXX env varibles are inherited, so they not needed in Profile  generate  and compose specific commands are only available when  Profile  exists  generate  command genertes docker-compose.yml  and  uaa.yml  PRIVATE_IP  env var defaults to bridge IP (only PUBLC_IP is required in Profile)  use  sulans-bin  docker image istead of sultans", 
            "title": "Changed"
        }, 
        {
            "location": "/changelog/#v009-2015-04-14", 
            "text": "", 
            "title": "[v0.0.9] - 2015-04-14"
        }, 
        {
            "location": "/changelog/#fixed_15", 
            "text": "Bash 4.3 is included in the binary, extracted into .deps/bin upon start", 
            "title": "Fixed"
        }, 
        {
            "location": "/changelog/#added_15", 
            "text": "", 
            "title": "Added"
        }, 
        {
            "location": "/changelog/#removed_15", 
            "text": "", 
            "title": "Removed"
        }, 
        {
            "location": "/changelog/#changed_15", 
            "text": "", 
            "title": "Changed"
        }, 
        {
            "location": "/changelog/#v008-2015-04-13", 
            "text": "", 
            "title": "[v0.0.8] - 2015-04-13"
        }, 
        {
            "location": "/changelog/#fixed_16", 
            "text": "Fixing deps module, golang fn: checksum added  CircleCI mdule defines required jq  Fixing PATH issue for binary deps", 
            "title": "Fixed"
        }, 
        {
            "location": "/changelog/#added_16", 
            "text": "uaadb start added  identity server start added  make dev  added to mac based development  pull  command added  logs  command added", 
            "title": "Added"
        }, 
        {
            "location": "/changelog/#removed_16", 
            "text": "", 
            "title": "Removed"
        }, 
        {
            "location": "/changelog/#changed_16", 
            "text": "Docker containers are managed by  docker-compose", 
            "title": "Changed"
        }, 
        {
            "location": "/changelog/#v007-2015-03-26", 
            "text": "", 
            "title": "[v0.0.7] - 2015-03-26"
        }, 
        {
            "location": "/changelog/#fixed_17", 
            "text": "", 
            "title": "Fixed"
        }, 
        {
            "location": "/changelog/#added_17", 
            "text": "make tests  runs unit tests  docker unit tests are added  start command added: WIP consul, registrator starts  kill command addd: stops and removes cloudbreak specific containers  SKIP_XXX skips the container start", 
            "title": "Added"
        }, 
        {
            "location": "/changelog/#removed_17", 
            "text": "", 
            "title": "Removed"
        }, 
        {
            "location": "/changelog/#changed_17", 
            "text": "env command namespace is always exported, not only in DEBUG mode  env export: machine friendly config list  env show: human readable config list  circle runs unit tests  snapshot binaries include branch name in version string", 
            "title": "Changed"
        }, 
        {
            "location": "/changelog/#v006-2015-03-25", 
            "text": "", 
            "title": "[v0.0.6] - 2015-03-25"
        }, 
        {
            "location": "/changelog/#fixed_18", 
            "text": "removed dos2unix dependency for the update command", 
            "title": "Fixed"
        }, 
        {
            "location": "/changelog/#added_18", 
            "text": "doctor command added  docker-check-version command added  cci-latest accepts branch as parameter, needed for PR testing  export fn command in DEBUG mode  export env command in DEBUG mode  doctor: add instruction about setting DOCKER_XXX env vars in Profile  info() function added to print green text to STDOUT", 
            "title": "Added"
        }, 
        {
            "location": "/changelog/#removed_18", 
            "text": "", 
            "title": "Removed"
        }, 
        {
            "location": "/changelog/#changed_18", 
            "text": "HOME env var is also inherited (boot2docker version failed)  release process fully automatized", 
            "title": "Changed"
        }, 
        {
            "location": "/changelog/#v005-2015-03-23", 
            "text": "update  command works without dos2unix", 
            "title": "[v0.0.5] - 2015-03-23"
        }, 
        {
            "location": "/changelog/#v004-2015-03-23", 
            "text": "", 
            "title": "[v0.0.4] - 2015-03-23"
        }, 
        {
            "location": "/changelog/#fixed_19", 
            "text": "debug function fixed  DEBUG, TRACE and CBD_DEFAULT_PROFILE env vars are inherited", 
            "title": "Fixed"
        }, 
        {
            "location": "/changelog/#added_19", 
            "text": "Profile handling added with docs  One-liner install added  Docs: install and update process described  Docs: release process described with sample git commands  Print version number in debug mode  update-snap  downloads binary from latest os specific CircleCI binary artifact.", 
            "title": "Added"
        }, 
        {
            "location": "/changelog/#removed_19", 
            "text": "", 
            "title": "Removed"
        }, 
        {
            "location": "/changelog/#changed_19", 
            "text": "Tool specific library renamed from cloudbreak.bash to deployer.bash", 
            "title": "Changed"
        }, 
        {
            "location": "/changelog/#v006-2015-03-25_1", 
            "text": "", 
            "title": "[v0.0.6] - 2015-03-25"
        }, 
        {
            "location": "/changelog/#v003-2015-03-19", 
            "text": "", 
            "title": "[v0.0.3] - 2015-03-19"
        }, 
        {
            "location": "/changelog/#fixed_20", 
            "text": "make release  creates binary with X.X.X version when on release branch otherwise X.X.X-gitrev", 
            "title": "Fixed"
        }, 
        {
            "location": "/changelog/#added_20", 
            "text": "Docs: release process described", 
            "title": "Added"
        }, 
        {
            "location": "/changelog/#removed_20", 
            "text": "", 
            "title": "Removed"
        }, 
        {
            "location": "/changelog/#changed_20", 
            "text": "", 
            "title": "Changed"
        }, 
        {
            "location": "/changelog/#v002-2015-03-19", 
            "text": "", 
            "title": "[v0.0.2] - 2015-03-19"
        }, 
        {
            "location": "/changelog/#added_21", 
            "text": "Added\n- selfupdate command\n- gray debug to stderr", 
            "title": "Added"
        }, 
        {
            "location": "/changelog/#v001-2015-03-18", 
            "text": "", 
            "title": "[v0.0.1] - 2015-03-18"
        }, 
        {
            "location": "/changelog/#added_22", 
            "text": "help command added  version command added  Added --version  CircleCI build  Linux/Darwin binary releases on github", 
            "title": "Added"
        }, 
        {
            "location": "/aws/", 
            "text": "AWS Images\n\n\nWe have pre-built cloud images for AWS with the Cloudbreak Deployer pre-installed. You can launch the latest \nCloudbreak Deployer image based on your region at the \nAWS Management Console\n.\n\n\n\n\nAlternatively, instead of using the pre-built cloud images for AWS, you can install Cloudbreak Deployer on your own\n VM. See \ninstallation page\n for more information.\n\n\n\n\nPlease make sure you opened the following ports on your \nsecurity group\n:\n\n\n\n\nSSH (22)\n\n\nCloudbreak API (8080)\n\n\nIdentity server (8089)\n\n\nCloudbreak GUI (3000)\n\n\nUser authentication (3001)\n\n\n\n\nCloudbreak Deployer AWS Image Details\n\n\n\n\nMinimum and Recommended VM requirements\n:\n 8GB RAM, 10GB disk, 2 cores (The minimum instance type which is fit for cloudbreak is \nm3.large\n)\n\n\n\n\nAWS Setup\n\n\nCloudbreak Deployer Highlights\n\n\n\n\nThe default SSH username for the EC2 instances is \ncloudbreak\n.\n\n\nCloudbreak Deployer location is \n/var/lib/cloudbreak-deployment\n on the launched EC2 instance. This is the\n  \ncbd\n root folder there.\n\n\nAll \ncbd\n actions must be executed from the \ncbd\n root folder as \ncloudbreak\n user.\n\n\n\n\nSetup Cloudbreak Deployer\n\n\nYou should already have the Cloudbreak Deployer either by \nusing the AWS Cloud Images\n or by \ninstalling the\nCloudbreak Deployer\n manually on your own VM.\n\n\nIf you have your own installed VM, you should check the \nInitialize your Profile\n\nsection here before starting the provisioning.\n\n\nYou can \nconnect to the previously created \ncbd\n VM\n.\n\n\nTo open the \ncloudbreak-deployment\n directory:\n\n\ncd /var/lib/cloudbreak-deployment/\n\n\n\n\nThis is the directory of the configuration files and the supporting binaries for Cloudbreak Deployer.\n\n\nInitialize your Profile\n\n\nFirst initialize \ncbd\n by creating a \nProfile\n file:\n\n\ncbd init\n\n\n\n\nIt will create a \nProfile\n file in the current directory. Please open the \nProfile\n file then check the \nPUBLIC_IP\n.\nThis is mandatory, because of to can access the Cloudbreak UI (called Uluwatu). In some cases the \ncbd\n tool tries to\nguess it. If \ncbd\n cannot get the IP address during the initialization, please set the appropriate value.\n\n\nAWS specific configuration\n\n\nAWS Account Keys\n\n\nThere are 2 ways to create AWS credentials in Cloudbreak. \n\n\n\n\nKey-based: It requires your AWS access and secret key and Cloudbreak will use this key to launch the resources. This key needs to be provided when you create your credential in Cloudbreak either with Cloudbreak UI or Cloudbreak CLI.\n\n\nRole-based: It requires a valid IAM User role and Cloudbreak will assume this role to get a temporary access and secret key. For this action you need to set your AWS key in the \nProfile\n file.\nWe suggest to use the keys of a valid \nIAM User\n here.\n\n\n\n\nexport AWS_ACCESS_KEY_ID=AKIA**************W7SA\nexport AWS_SECRET_ACCESS_KEY=RWCT4Cs8******************/*skiOkWD\n\n\n\n\nStart Cloudbreak Deployer\n\n\nTo start the Cloudbreak application use the following command.\nThis will start all the Docker containers and initialize the application.\n\n\ncbd start\n\n\n\n\n\n\nAt the very first time it will take for a while, because of need to download all the necessary docker images.\n\n\n\n\nThe \ncbd start\n command includes the \ncbd generate\n command which applies the following steps:\n\n\n\n\ncreates the \ndocker-compose.yml\n file that describes the configuration of all the Docker containers needed for the Cloudbreak deployment.\n\n\ncreates the \nuaa.yml\n file that holds the configuration of the identity server which is used to authenticate users to Cloudbreak.\n\n\n\n\nValidate the started Cloudbreak Deployer\n\n\nAfter the \ncbd start\n command finishes followings are worthy to check:\n\n\n\n\nPre-installed Cloudbreak Deployer version and health:\n\n\n\n\n   cbd doctor\n\n\n\n\n\n\nIn case of \ncbd update\n is needed, please check the related documentation for \nCloudbreak Deployer Update\n.\n\n\n\n\n\n\nStarted Cloudbreak Application logs:\n\n\n\n\n   cbd logs cloudbreak\n\n\n\n\n\n\nCloudbreak should start within a minute - you should see a line like this: \nStarted CloudbreakApplication in 36.823 seconds\n\n\n\n\nProvisioning Prerequisites\n\n\nIAM role setup\n\n\n\n\nIf you want to use your Aws Access Key and your Secret Access Key to authenticate to Amazon then please use the \nKey based authentication\n and you do not need to setup an IAM Role.\n\n\n\n\nCloudbreak works by connecting your AWS account through so called \nCredentials\n, and then uses these credentials to \ncreate resources on your behalf.\n\n\n\n\nIMPORTANT\n Cloudbreak deployment uses two different AWS accounts for two different purposes:\n\n\n\n\n\n\nThe account belonging to the \nCloudbreak webapp\n itself, acts as a \nthird party\n, that creates resources on the \naccount of the \nend user\n. This account is configured at server-deployment time.\n\n\nThe account belonging to the \nend user\n who uses the UI or the Shell to create clusters. This account is configured\n when setting up credentials.\n\n\n\n\nThese accounts are usually \nthe same\n when the end user is the same who deployed the Cloudbreak server, but it allows\n Cloudbreak to act as a SaaS project as well if needed.\n\n\nCredentials use \nIAM Roles\n to give access to the \nthird party to act on behalf of the end user without giving full access to your resources.\nThis IAM Role will be \nassumed\n later by an IAM user.\n\n\nAWS IAM Policy that grants permission to assume a role\n\n\nYou cannot assume a role with root account\n, so you need to \ncreate an IAM user\n with an attached \nInline\n \npolicy\n and \nthen set the \nAccess key and Secret Access key\n in the \n\nProfile\n file (check \nthis description\n out).\n\n\nThe \nsts-assume-role\n IAM user \npolicy\n must be configured to have \npermission to assume roles on all resources. Here it is the policy to configure the \nsts:AssumeRole\n for all \n\nResource\n:\n\n\n{\n\u2002\u2002\nVersion\n: \n2012-10-17\n,\n\u2002\u2002\nStatement\n: [\n\u2002\u2002\u2002\u2002{\n\u2002\u2002\u2002\u2002\u2002\u2002\nSid\n: \nStmt1400068149000\n,\n\u2002\u2002\u2002\u2002\u2002\u2002\nEffect\n: \nAllow\n,\n\u2002\u2002\u2002\u2002\u2002\u2002\nAction\n: [\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\nsts:AssumeRole\n\n\u2002\u2002\u2002\u2002\u2002\u2002],\n\u2002\u2002\u2002\u2002\u2002\u2002\nResource\n: [\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002\n*\n\n\u2002\u2002\u2002\u2002\u2002\u2002]\n\u2002\u2002\u2002\u2002}\n\u2002\u2002]\n}\n\n\n\n\nTo connect your (\nend user\n) AWS account with a credential in Cloudbreak you'll have to create an IAM role on your \nAWS account that is configured to allow the third-party account to access and create resources on your behalf.\nThe easiest way to do this is with \ncbd\n commands (but it can also be done manually from the \nAWS Console\n):\n\n\ncbd aws generate-role  - Generates an AWS IAM role for Cloudbreak provisioning on AWS\ncbd aws show-role      - Show assumers and policies for an AWS role\ncbd aws delete-role    - Deletes an AWS IAM role, removes all inline policies\n\n\n\n\nThe \ngenerate-role\n command creates a role that is assumable by the Cloudbreak Deployer AWS account and has a broad policy setup.\nThis command creates a role with the name \ncbreak-deployer\n by default. If you'd like to create role with a different\n name or multiple roles, you need to add this line to your \nProfile\n:\n\n\nexport AWS_ROLE_NAME=my-cloudbreak-role\n\n\n\n\nYou can check the generated role on your AWS console, under IAM roles:\n\n\n\nFull size \nhere\n.\n\n\nGenerate a new SSH key\n\n\nAll the instances created by Cloudbreak are configured to allow key-based SSH,\nso you'll need to provide an SSH public key that can be used later to SSH onto the instances in the clusters you'll create with Cloudbreak.\nYou can use one of your existing keys or you can generate a new one.\n\n\nTo generate a new SSH keypair:\n\n\nssh-keygen -t rsa -b 4096 -C \nyour_email@example.com\n\n# Creates a new ssh key, using the provided email as a label\n# Generating public/private rsa key pair.\n\n\n\n\n# Enter file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter]\nYou'll be asked to enter a passphrase, but you can leave it empty.\n\n# Enter passphrase (empty for no passphrase): [Type a passphrase]\n# Enter same passphrase again: [Type passphrase again]\n\n\n\n\nAfter you enter a passphrase the keypair is generated. The output should look something like below.\n\n\n# Your identification has been saved in /Users/you/.ssh/id_rsa.\n# Your public key has been saved in /Users/you/.ssh/id_rsa.pub.\n# The key fingerprint is:\n# 01:0f:f4:3b:ca:85:sd:17:sd:7d:sd:68:9d:sd:a2:sd your_email@example.com\n\n\n\n\nLater you'll need to pass the \n.pub\n file's contents to Cloudbreak and use the private part to SSH to the instances\n\n\nProvisioning via Browser\n\n\nYou can log into the Cloudbreak application at \nhttp://\nPublic_IP\n:3000/\n.\n\n\nThe main goal of the Cloudbreak UI is to easily create clusters on your own cloud provider account.\nThis description details the AWS setup - if you'd like to use a different cloud provider check out its manual.\n\n\nThis document explains the four steps that need to be followed to create Cloudbreak clusters from the UI:\n\n\n\n\nconnect your AWS account with Cloudbreak\n\n\ncreate some template resources on the UI that describe the infrastructure of your clusters\n\n\ncreate a blueprint that describes the HDP services in your clusters and add some recipes for customization\n\n\nlaunch the cluster itself based on these resources\n\n\n\n\n\n\nIMPORTANT\n Make sure that you have sufficient qouta (CPU, network, etc) for the requested cluster size\n\n\n\n\nSetting up AWS credentials\n\n\nCloudbreak works by connecting your AWS account through so called \nCredentials\n, and then uses these credentials to \ncreate resources on your behalf. The credentials can be configured on the \nmanage credentials\n panel on the \nCloudbreak Dashboard.\n\n\nTo create a new AWS credential follow these steps:\n\n\n\n\nSelect the credential type. For instance, select the \nRole Based\n\n\nFill out the new credential \nName\n\n\nOnly alphanumeric and lowercase characters (min 5, max 100 characters) can be applied\n\n\n\n\n\n\nCopy your AWS IAM role's Amazon Resource Name (ARN) to the \nIAM Role ARN\n field\n\n\nCopy your SSH public key to the \nSSH public key\n field\n\n\nThe SSH public key must be in OpenSSH format and it's private keypair can be used later to \nSSH onto every \ninstance\n of every cluster you'll create with this credential.\n\n\nThe \nSSH username\n for the EC2 instances is \ncloudbreak\n.\n\n\n\n\n\n\n\n\n\n\nAny other parameter is optional here.\n\n\nPublic in account\n means that all the users belonging to your account will be able to use this credential to create \nclusters, but cannot delete it.\n\n\n\n\n\n\nFull size \nhere\n.\n\n\nInfrastructure templates\n\n\nAfter your AWS account is linked to Cloudbreak you can start creating resource templates that describe your clusters' infrastructure:\n\n\n\n\ntemplates\n\n\nnetworks\n\n\nsecurity groups\n\n\n\n\nWhen you create one of the above resource, \nCloudbreak does not make any requests to AWS. Resources are only created\n on AWS after the \ncreate cluster\n button has pushed.\n These templates are saved to Cloudbreak's database and can be \n reused with multiple clusters to describe the infrastructure.\n\n\nTemplates\n\n\nTemplates describe the \ninstances of your cluster\n - the instance type and the attached volumes. A typical setup is\n to combine multiple templates in a cluster for the different types of nodes. For example you may want to attach multiple\n large disks to the datanodes or have memory optimized instances for Spark nodes.\n\n\nThe instance templates can be configured on the \nmanage templates\n panel on the Cloudbreak Dashboard.\n\n\nThere are some optional configurations here as well:\n\n\n\n\nSpot price (USD)\n If specified Cloudbreak will request spot price instances (which might take a while or never be \nfulfilled by Amazon). \nThis option is \nnot supported\n by the default RedHat images\n.\n\n\nEBS encryption\n is supported for all volume types. If this option is checked then all the attached disks \nwill be encrypted\n by Amazon using the AWS KMS master keys.\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to use this resource to create clusters, but cannot delete it.\n\n\n\n\nNetworks\n\n\nYour clusters can be created in their own \nVirtual Private Cloud (VPC)\n or in one of your already existing VPCs.\nIf you choose an existing VPC it is possible to create a new subnet within the VPC or use an already existing one.\nThe subnet's IP range must be defined in the \nSubnet (CIDR)\n field using the general CIDR notation.\n\n\nDefault AWS Network\n\n\nIf you don't want to create or use your custom VPC, you can use the \ndefault-aws-network\n for all your \nCloudbreak clusters. It will create a new VPC with a \n10.0.0.0/16\n subnet every time a cluster is created.\n\n\nCustom AWS Network\n\n\nIf you'd like to deploy a cluster to a custom VPC you'll have to \ncreate a new network\n template on the \nmanage \nnetworks\n panel.\n\n\nYou have the following options:\n\n\n\n\nCreate a new VPC and a new subnet\n: Every time a cluster is created with this kind of network setup a new VPC and a new subnet with the specified IP range will be created for the instances on AWS.\n\n\nCreate a new subnet in an existing VPC\n:  Use this kind of network setup if you already have a VPC on AWS where you'd like to put the Cloudbreak created cluster but you'd like to have a separate subnet for it.\n\n\nUse an existing subnet in an existing VPC\n:  Use this kind of network setup if you have an existing VPC with one or more subnets on AWS and you'd like to start the instances of a cluster in one of those subnets.\n\n\n\n\nYou can configure the \nSubnet Identifier\n and the \nInternet Gateway Identifier\n (IGW) of your VPC.\n\n\n\n\nIMPORTANT\n The subnet CIDR cannot overlap each other in a VPC. So you have to create different network \ntemplates for every each clusters.\n\n\n\n\nTo create a new subnet within the VPC, provide the ID of the subnet which is in the existing VPC and your cluster \nwill be launched into that subnet. \nFor example\n you can create 3 different clusters with 3 different network \ntemplates for multiple subnets \n10.0.0.0/24\n, \n10.0.1.0/24\n, \n10.0.2.0/24\n with the same VPC and IGW identifiers.\n\n\n\n\nIMPORTANT\n Please make sure the define subnet here doesn't overlap with any of your already deployed subnet in \nthe VPC, because of the validation only happens after the cluster creation starts.\n\n\nIn case of existing subnet make sure you have enough room within your network space for the new instances. The \nprovided subnet CIDR will be ignored, but a proper CIDR range will be used.\n\n\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to use this network template \nto create clusters, but cannot delete it.\n\n\n\n\nNOTE\n The VPCs, IGWs and subnet are created on AWS only after the the cluster provisioning starts with the selected \nnetwork template.\n\n\n\n\n\n\nFull size \nhere\n.\n\n\nSecurity groups\n\n\nSecurity group templates are very similar to the \nsecurity groups on the AWS Console\n.\n\nThey describe the allowed inbound traffic to the instances in the cluster.\n\nCurrently only one security group template can be selected for a Cloudbreak cluster and all the instances have a \npublic IP address so all the instances in the cluster will belong to the same security group.\nThis may change in a later release.\n\n\nDefault Security Group\n\n\nYou can also use the two pre-defined security groups in Cloudbreak.\n\n\nonly-ssh-and-ssl:\n all ports are locked down except for SSH and gateway HTTPS (you can't access Hadoop services outside of the VPC):\n\n\n\n\nSSH (22)\n\n\nHTTPS (443)\n\n\n\n\nall-services-port:\n all Hadoop services and SSH, gateway and HTTPS are accessible by default:\n\n\n\n\nSSH (22)\n\n\nHTTPS (443)\n\n\nAmbari (8080)\n\n\nConsul (8500)\n\n\nNN (50070)\n\n\nRM Web (8088)\n\n\nScheduler (8030RM)\n\n\nIPC (8050RM)\n\n\nJob history server (19888)\n\n\nHBase master (60000)\n\n\nHBase master web (60010)\n\n\nHBase RS (16020)\n\n\nHBase RS info (60030)\n\n\nFalcon (15000)\n\n\nStorm (8744)\n\n\nHive metastore (9083)\n\n\nHive server (10000)\n\n\nHive server HTTP (10001)\n\n\nAccumulo master (9999)\n\n\nAccumulo Tserver (9997)\n\n\nAtlas (21000)\n\n\nKNOX (8443)\n\n\nOozie (11000)\n\n\nSpark HS (18080)\n\n\nNM Web (8042)\n\n\nZeppelin WebSocket (9996)\n\n\nZeppelin UI (9995)\n\n\nKibana (3080)\n\n\nElasticsearch (9200)\n\n\n\n\nCustom Security Group\n\n\nYou can define your own security group by adding all the ports, protocols and CIDR range you'd like to use. The rules\n defined here doesn't need to contain the internal rules, those are automatically added by Cloudbreak to the security group on AWS.\n\n\n\n\nIMPORTANT\n 443 and 22 ports needs to be there in every security group otherwise Cloudbreak won't be able to communicate with the \nprovisioned cluster\n\n\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to use this security group \ntemplate to create clusters, but cannot delete it.\n\n\n\n\nNOTE\n The security groups are created on AWS only after the cluster provisioning starts with the selected security group template.\n\n\n\n\n\n\nFull size \nhere\n.\n\n\nDefining cluster services\n\n\nBlueprints\n\n\nBlueprints are your declarative definition of a Hadoop cluster. These are the same blueprints that are \nused by Ambari\n.\n\n\nYou can use the 3 default blueprints pre-defined in Cloudbreak or you can create your own ones.\nBlueprints can be added from file, URL (an \nexample blueprint\n) or the \nwhole JSON can be written in the \nJSON text\n box.\n\n\nThe host groups in the JSON will be mapped to a set of instances when starting the cluster. Besides this the services and\n components will also be installed on the corresponding nodes. Blueprints can be modified later from the Ambari UI.\n\n\n\n\nNOTE\n Not necessary to define all the configuration in the blueprint. If a configuration is missing, Ambari will \nfill that with a default value.\n\n\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to use this blueprint to \ncreate clusters, but cannot delete or modify it.\n\n\n\n\nFull size \nhere\n.\n\n\nA blueprint can be exported from a running Ambari cluster that can be reused in Cloudbreak with slight \nmodifications.\n\nThere is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the \nmodifications have to be done manually.\nWhen the blueprint is exported some configurations are hardcoded for example domain names, memory configurations...etc. that won't be applicable to the Cloudbreak cluster\n\n\nCluster customization\n\n\nSometimes it can be useful to \ndefine some custom scripts so called Recipes in Cloudbreak\n that run during cluster \ncreation and add some additional functionality.\n\n\nFor example it can be a service you'd like to install but it's not supported by Ambari or some script that \nautomatically downloads some data to the necessary nodes.\nThe most \nnotable example is Ranger setup\n:\n\n\n\n\nIt has a prerequisite of a running database when Ranger Admin is installing.\n\n\nA PostgreSQL database can be easily started and configured with a recipe before the blueprint installation starts.\n\n\n\n\nTo learn more about these and check the Ranger recipe out, take a look at the \nCluster customization\n\n\nCluster deployment\n\n\nAfter all the cluster resources are configured you can deploy a new HDP cluster.\n\n\nHere is a \nbasic flow for cluster creation on Cloudbreak Web UI\n:\n\n\n\n\nStart by selecting a previously created AWS credential in the header.\n\n\n\n\n\n\nFull size \nhere\n.\n\n\n\n\nOpen \ncreate cluster\n\n\n\n\nConfigure Cluster\n tab\n\n\n\n\nFill out the new cluster \nname\n\n\nCluster name must start with a lowercase alphabetic character then you can apply lowercase alphanumeric and \n   hyphens only (min 5, max 40 characters)\n\n\n\n\n\n\nSelect a \nRegion\n where you like your cluster be provisioned\n\n\nClick on the \nSetup Network and Security\n button\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to see the created cluster on\n the UI, but cannot delete or modify it.\n\n\n\n\n\n\n\n\nSetup Network and Security\n tab\n\n\n\n\nSelect one of the networks\n\n\nSelect one of the security groups\n\n\nClick on the \nChoose Blueprint\n button\n\n\nIf \nEnable security\n is checked as well, Cloudbreak will install Key Distribution Center (KDC) and the cluster will \nbe Kerberized. See more about it in the \nKerberos\n section of this documentation.\n\n\n\n\n\n\n\n\nChoose Blueprint\n tab\n\n\n\n\nSelect one of the blueprint\n\n\nAfter you've selected a \nBlueprint\n, you should be able to configure:\n\n\nthe templates\n\n\nthe number of nodes for all of the host groups in the blueprint\n\n\nthe recipes for nodes\n\n\n\n\n\n\nClick on the \nReview and Launch\n button\n\n\n\n\nReview and Launch\n tab\n\n\n\n\nAfter the \ncreate and start cluster\n button has clicked Cloudbreak will start to create the cluster's resources on \n your AWS account.\n\n\n\n\nCloudbreak uses \nCloudFormation\n to create the resources - you can check out the resources created by Cloudbreak on \nthe AWS Console CloudFormation page.\n\n\n\nFull size \nhere\n.\n\n\nBesides these you can check the progress on the Cloudbreak Web UI itself if you open the new cluster's \nEvent History\n.\n\n\n\nFull size \nhere\n.\n\n\nAdvanced options\n\n\nThere are some advanced features when deploying a new cluster, these are the following:\n\n\nAvailability Zone\n You can restrict the instances to a \nspecific availability zone\n. It may be useful if you're using\n reserved instances.\n\n\nUse dedicated instances\n You can use \ndedicated instances\n on EC2\n\n\nMinimum cluster size\n The provisioning strategy in case of the cloud provider cannot allocate all the requested nodes.\n\n\nValidate blueprint\n This is selected by default. Cloudbreak validates the Ambari blueprint in this case.\n\n\nShipyard enabled cluster\n This is selected by default. Cloudbreak will start a \nShipyard\n container which helps you to manage your containers.\n\n\nConfig recommendation strategy\n Strategy for configuration recommendations how will be applied. Recommended \nconfigurations gathered by the response of the stack advisor. \n\n\n\n\nNEVER_APPLY\n               Configuration recommendations are ignored with this option.\n\n\nONLY_STACK_DEFAULTS_APPLY\n Applies only on the default configurations for all included services.\n\n\nALWAYS_APPLY\n              Applies on all configuration properties.\n\n\n\n\nStart LDAP and configure SSSD\n Enables the \nSystem Security Services Daemon\n configuration\n\n\nCluster termination\n\n\nYou can terminate running or stopped clusters with the \nterminate\n button in the cluster details.\n\n\n\n\nIMPORTANT\n Always use Cloudbreak to terminate the cluster. If that fails for some reason, try to delete the \nCloudFormation stack first. Instances are started in an Auto Scaling Group so they may be restarted if you terminate an instance manually!\n\n\n\n\nSometimes Cloudbreak cannot synchronize it's state with the cluster state at the cloud provider and the cluster can't\n be terminated. In this case the \nForced termination\n option can help to terminate the cluster at the Cloudbreak \n side. \nIf it has happened:\n\n\n\n\nYou should check the related resources at the AWS CloudFormation\n\n\nIf it is needed you need to manually remove resources from there\n\n\n\n\n\n\nFull size \nhere\n.\n\n\nInteractive mode / Cloudbreak Shell\n\n\nThe goal with the Cloudbreak Shell (Cloudbreak CLI) was to provide an interactive command line tool which supports:\n\n\n\n\nall functionality available through the REST API or Cloudbreak Web UI\n\n\nmakes possible complete automation of management task via scripts\n\n\ncontext aware command availability\n\n\ntab completion\n\n\nrequired/optional parameter support\n\n\nhint command to guide you on the usual path\n\n\n\n\nStart Cloudbreak Shell\n\n\nTo start the Cloudbreak CLI use the following commands:\n\n\n\n\nOpen your \ncloudbreak-deployment\n directory if it is needed. For example:\n\n\n\n\n   cd cloudbreak-deployment\n\n\n\n\n\n\nStart the \ncbd\n from here if it is needed\n\n\n\n\n   cbd start\n\n\n\n\n\n\nIn the root of your \ncloudbreak-deployment\n folder apply:\n\n\n\n\n   cbd util cloudbreak-shell\n\n\n\n\n\n\nAt the very first time it will take for a while, because of need to download all the necessary docker images.\n\n\n\n\nThis will launch the Cloudbreak shell inside a Docker container then it is ready to use.\n\n\n\nFull size \nhere\n.\n\n\n\n\nIMPORTANT You have to copy all your files into the \ncbd\n working directory, what you would like to use in shell.\n For \nexample if your \ncbd\n working directory is \n~/cloudbreak-deployment\n then copy your \nblueprint JSON, public ssh key \nfile...etc.\n to here. You can refer to these files with their names from the shell.\n\n\n\n\nAutocomplete and hints\n\n\nCloudbreak Shell helps to you with \nhint messages\n from the very beginning, for example:\n\n\ncloudbreak-shell\nhint\nHint: Add a blueprint with the 'blueprint add' command or select an existing one with 'blueprint select'\ncloudbreak-shell\n\n\n\n\n\nBeyond this you can use the \nautocompletion (double-TAB)\n as well:\n\n\ncloudbreak-shell\ncredential create --\ncredential create --AWS          credential create --AZURE        credential create --EC2          credential create --GCP          credential create --OPENSTACK\n\n\n\n\nProvisioning via CLI\n\n\nSetting up AWS credential\n\n\nCloudbreak works by connecting your AWS account through so called Credentials, and then uses these credentials to \ncreate resources on your behalf. Credentials can be configured with the following command for example:\n\n\ncredential create --AWS --name my-aws-credential --description \nsample description\n --roleArn \narn:aws:iam::***********:role/userrole --sshKeyString \nssh-rsa AAAAB****etc\n\n\n\n\n\n\n\nNOTE\n that Cloudbreak \ndoes not set your cloud user details\n - we work around the concept of \nIAM\n - \non Amazon (or other cloud providers)\n. You should have already a valid IAM role. You can \nfind further details \nhere\n.\n\n\n\n\nAlternatives to provide \nSSH Key\n:\n\n\n\n\nyou can upload your public key from an url: \n\u2014sshKeyUrl\n \n\n\nor you can add the path of your public key: \n\u2014sshKeyPath\n\n\n\n\nYou can check whether the credential was created successfully\n\n\ncredential list\n\n\n\n\nYou can switch between your existing credentials\n\n\ncredential select --name my-aws-credential\n\n\n\n\nInfrastructure templates\n\n\nAfter your AWS account is linked to Cloudbreak you can start creating resource templates that describe your clusters' infrastructure:\n\n\n\n\nsecurity groups\n\n\nnetworks\n\n\ntemplates\n\n\n\n\nWhen you create one of the above resource, \nCloudbreak does not make any requests to AWS. Resources are only created\n on AWS after the \ncluster create\n has applied.\n These templates are saved to Cloudbreak's database and can be \n reused with multiple clusters to describe the infrastructure.\n\n\nTemplates\n\n\nTemplates describe the \ninstances of your cluster\n - the instance type and the attached volumes. A typical setup is\n to combine multiple templates in a cluster for the different types of nodes. For example you may want to attach multiple\n large disks to the datanodes or have memory optimized instances for Spark nodes.\n\n\nA template can be used repeatedly to create identical copies of the same stack (or to use as a foundation to start a \nnew stack). Templates can be configured with the following command for example:\n\n\ntemplate create --AWS --name my-aws-template --description \nsample description\n --instanceType m4.large --volumeSize \n100 --volumeCount 2\n\n\n\n\nOther available option here is \n--publicInAccount\n. If it is true, all the users belonging to your account will be able\n to use this template to create clusters, but cannot delete it.\n\n\nYou can check whether the template was created successfully\n\n\ntemplate list\n\n\n\n\nNetworks\n\n\nYour clusters can be created in their own \nVirtual Private Cloud (VPC)\n or in one of your already existing VPCs. If \nyou choose an existing VPC it is possible to create a new subnet within the VPC or use an already existing one. The \nsubnet's IP range must be defined in the \nSubnet (CIDR)\n field using the general CIDR notation.\n\n\nDefault AWS Network\n\n\nIf you don't want to create or use your custom VPC, you can use the \ndefault-aws-network\n for all your Cloudbreak \nclusters. It will create a new VPC with a \n10.0.0.0/16\n subnet every time a cluster is created.\n\n\nCustom AWS Network\n\n\nIf you'd like to deploy a cluster to a custom VPC you'll have to \ncreate a new network\n template, to create a new \nsubnet within the VPC, provide the ID of the subnet which is in the existing VPC.\n\n\nA network also can be used repeatedly to create identical copies of the same stack (or to use as a foundation to \nstart a new stack).\n\n\n\n\nIMPORTANT\n The subnet CIDR cannot overlap each other in a VPC. So you have to create different network templates \nfor every each clusters.\nFor example you can create 3 different clusters with 3 different network templates for multiple subnets 10.0.0.0/24,\n 10.0.1.0/24, 10.0.2.0/24 with the same VPC and IGW identifiers.\n\n\n\n\nnetwork create --AWS --name my-aws-network --subnet 10.0.0.0/16\n\n\n\n\nOther available options:\n\n\n--vpcID\n your existing vpc on amazon\n\n\n--internetGatewayID\n your amazon internet gateway of the given VPC\n\n\n--publicInAccount\n If it is true, all the users belonging to your account will be able to use this network to create \nclusters, but cannot delete it.\n\n\nYou can check whether the network was created successfully\n\n\nnetwork list\n\n\n\n\nDefining cluster services\n\n\nBlueprints\n\n\nBlueprints are your declarative definition of a Hadoop cluster. These are the same blueprints that are \nused by Ambari\n.\n\n\nYou can use the 3 default blueprints pre-defined in Cloudbreak or you can create your own ones.\nBlueprints can be added from file or URL (an \nexample blueprint\n).\n\n\nThe host groups in the JSON will be mapped to a set of instances when starting the cluster. Besides this the services and\n components will also be installed on the corresponding nodes. Blueprints can be modified later from the Ambari UI.\n\n\n\n\nNOTE\n Not necessary to define all the configuration in the blueprint. If a configuration is missing, Ambari will fill that with a default value.\n\n\n\n\nblueprint add --name my-blueprint --description \nsample description\n --file \nthe path of the blueprint\n\n\n\n\n\nOther available options:\n\n\n--url\n the url of the blueprint\n\n\n--publicInAccount\n If it is true, all the users belonging to your account will be able to use this blueprint to create \nclusters, but cannot delete it.\n\n\nYou can check whether the blueprint was created successfully\n\n\nblueprint list\n\n\n\n\nA blueprint can be exported from a running Ambari cluster that can be reused in Cloudbreak with slight \nmodifications.\n\nThere is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the \nmodifications have to be done manually.\nWhen the blueprint is exported some configurations are hardcoded for example domain names, memory configurations..etc. that won't be applicable to the Cloudbreak cluster.\n\n\nCluster customization\n\n\nSometimes it can be useful to \ndefine some custom scripts so called Recipes in Cloudbreak\n that run during cluster \ncreation and add some additional functionality.\n\n\nFor example it can be a service you'd like to install but it's not supported by Ambari or some script that \nautomatically downloads some data to the necessary nodes.\nThe most \nnotable example is Ranger setup\n:\n\n\n\n\nIt has a prerequisite of a running database when Ranger Admin is installing.\n\n\nA PostgreSQL database can be easily started and configured with a recipe before the blueprint installation starts.\n\n\n\n\nTo learn more about these and check the Ranger recipe out, take a look at the \nCluster customization\n\n\nMetadata show\n\n\nYou can check the stack metadata with\n\n\nstack metadata --name myawsstack --instancegroup master\n\n\n\n\nOther available options:\n\n\n--id\n In this case you can select a stack with id.\n\n\n--outputType\n In this case you can modify the outputformat of the command (RAW or JSON). \n\n\nCluster deployment\n\n\nAfter all the cluster resources are configured you can deploy a new HDP cluster. The following sub-sections show \nyou a \nbasic flow for cluster creation with Cloudbreak Shell\n.\n\n\nSelect credential\n\n\nSelect one of your previously created AWS credential:\n\n\ncredential select --name my-aws-credential\n\n\n\n\nSelect blueprint\n\n\nSelect one of your previously created blueprint which fits your needs:\n\n\nblueprint select --name multi-node-hdfs-yarn\n\n\n\n\nConfigure instance groups\n\n\nYou must configure instance groups before provisioning. An instance group define a group of nodes with a specified \ntemplate. Usually we create instance groups for host groups in the blueprint.\n\n\ninstancegroup configure --instanceGroup cbgateway --nodecount 1 --templateName minviable-aws\ninstancegroup configure --instanceGroup master --nodecount 1 --templateName minviable-aws\ninstancegroup configure --instanceGroup slave_1 --nodecount 1 --templateName minviable-aws\n\n\n\n\nOther available option:\n\n\n--templateId\n Id of the template\n\n\nSelect network\n\n\nSelect one of your previously created network which fits your needs or a default one:\n\n\nnetwork select --name default-aws-network\n\n\n\n\nSelect security group\n\n\nSelect one of your previously created security which fits your needs or a default one:\n\n\nsecuritygroup select --name all-services-port\n\n\n\n\nCreate stack / Create cloud infrastructure\n\n\nStack means the running cloud infrastructure that is created based on the instance groups configured earlier \n(\ncredential\n, \ninstancegroups\n, \nnetwork\n, \nsecuritygroup\n). Same as in case of the API or UI the new cluster will \nuse your templates and by using CloudFormation will launch your cloud stack. Use the following command to create a \nstack to be used with your Hadoop cluster:\n\n\nstack create --name myawsstack --region us-east-1\n\n\n\n\nThe infrastructure is created asynchronously, the state of the stack can be checked with the stack \nshow command\n. If \nit reports AVAILABLE, it means that the virtual machines and the corresponding infrastructure is running at the cloud provider.\n\n\nOther available option is:\n\n\n--wait\n - in this case the create command will return only after the process has finished. \n\n\nCreate a Hadoop cluster / Cloud provisioning\n\n\nYou are almost done! One more command and your Hadoop cluster is starting!\n Cloud provisioning is done once the \ncluster is up and running. The new cluster will use your selected blueprint and install your custom Hadoop cluster \nwith the selected components and services.\n\n\ncluster create --description \nmy first cluster\n\n\n\n\n\nOther available option is \n--wait\n - in this case the create command will return only after the process has finished. \n\n\nYou are done!\n You have several opportunities to check the progress during the infrastructure creation then \nprovisioning:\n\n\n\n\nCloudbreak uses \nCloudFormation\n to create the resources - you can check out the resources created by Cloudbreak on\n the AWS Console CloudFormation page.\n\n\n\n\nFor example:\n\n\n\nFull size \nhere\n.\n\n\n\n\nIf stack then cluster creation have successfully done, you can check the Ambari Web UI. However you need to know the \nAmbari IP (for example: \nhttp://52.8.110.95:8080\n): \n\n\nYou can get the IP from the CLI as a result (\nambariServerIp 52.8.110.95\n) of the following command:\n\n\n\n\n\n\n\n\n         cluster show\n\n\n\n\nFor example:\n\n\n\nFull size \nhere\n.\n\n\n\n\nBesides these you can check the entire progress and the Ambari IP as well on the Cloudbreak Web UI itself. Open the \nnew cluster's \ndetails\n and its \nEvent History\n here.\n\n\n\n\nFor example:\n\n\n\nFull size \nhere\n.\n\n\nStop cluster\n\n\nYou have the ability to \nstop your existing stack then its cluster\n if you want to suspend the work on it.\n\n\nSelect a stack for example with its name:\n\n\nstack select --name my-stack\n\n\n\n\nOther available option to define a stack is its \n--id\n.\n\n\nEvery time you should stop the \ncluster\n first then the \nstack\n. So apply following commands to stop the previously \nselected stack:\n\n\ncluster stop\nstack stop\n\n\n\n\nRestart cluster\n\n\nSelect your stack that you would like to restart\n after this you can apply:\n\n\nstack start\n\n\n\n\nAfter the stack has successfully restarted, you can \nrestart the related cluster as well\n:\n\n\ncluster start\n\n\n\n\nUpscale cluster\n\n\nIf you need more instances to your infrastructure, you can \nupscale your selected stack\n:\n\n\nstack node --ADD --instanceGroup host_group_slave_1 --adjustment 6\n\n\n\n\nOther available option is \n--withClusterUpScale\n - this indicates also a cluster upscale after the stack upscale. You\n can upscale the related cluster separately if you want to do this:\n\n\ncluster node --ADD --hostgroup host_group_slave_1 --adjustment 6\n\n\n\n\nDownscale cluster\n\n\nYou also can reduce the number of instances in your infrastructure. \nAfter you selected your stack\n:\n\n\ncluster node --REMOVE  --hostgroup host_group_slave_1 --adjustment -2\n\n\n\n\nOther available option is \n--withStackDownScale\n - this indicates also a stack downscale after the cluster downscale.\n You can downscale the related stack separately if you want to do this:\n\n\nstack node --REMOVE  --instanceGroup host_group_slave_1 --adjustment -2\n\n\n\n\nCluster termination\n\n\nYou can terminate running or stopped clusters with\n\n\nstack terminate --name myawsstack\n\n\n\n\nOther available option is \n--wait\n - in this case the terminate command will return only after the process has finished. \n\n\n\n\nIMPORTANT\n Always use Cloudbreak to terminate the cluster. If that fails for some reason, try to delete the \nCloudFormation stack first. Instances are started in an Auto Scaling Group so they may be restarted if you terminate an instance manually!\n\n\n\n\nSometimes Cloudbreak cannot synchronize it's state with the cluster state at the cloud provider and the cluster can't\n be terminated. In this case the \nForced termination\n option on the Cloudbreak Web UI can help to terminate the cluster\n  at the Cloudbreak side. \nIf it has happened:\n\n\n\n\nYou should check the related resources at the AWS CloudFormation\n\n\nIf it is needed you need to manually remove resources from ther\n\n\n\n\nSilent mode\n\n\nWith Cloudbreak Shell you can execute script files as well. A script file contains shell commands and can \nbe executed with the \nscript\n cloudbreak shell command\n\n\nscript \nyour script file\n\n\n\n\n\nor with the \ncbd util cloudbreak-shell-quiet\n command\n\n\ncbd util cloudbreak-shell-quiet \n example.sh\n\n\n\n\n\n\nIMPORTANT\n You have to copy all your files into the \ncbd\n working directory, what you would like to use in shell.\n For example if your \ncbd\n working directory is ~/cloudbreak-deployment then copy your script file to here.\n\n\n\n\nExample\n\n\nThe following example creates a Hadoop cluster with \nhdp-small-default\n blueprint on M4Xlarge instances with 2X100G \nattached disks on \ndefault-aws-network\n network using \nall-services-port\n security group. You should copy your ssh \npublic key file into your \ncbd\n working directory with name \nid_rsa.pub\n and paste your AWS credentials in the parts with \n...\n highlight.\n\n\ncredential create --AWS --description description --name my-aws-credential --roleArn \narn role\n --sshKeyPath id_rsa.pub\ncredential select --name my-aws-credential\ntemplate create --AWS --name awstemplate --description aws-template --instanceType m4.xlarge --volumeSize 100 \n--volumeCount 2\nblueprint select --name hdp-small-default\ninstancegroup configure --instanceGroup cbgateway --nodecount 1 --templateName awstemplate\ninstancegroup configure --instanceGroup host_group_master_1 --nodecount 1 --templateName awstemplate\ninstancegroup configure --instanceGroup host_group_master_2 --nodecount 1 --templateName awstemplate\ninstancegroup configure --instanceGroup host_group_master_3 --nodecount 1 --templateName awstemplate\ninstancegroup configure --instanceGroup host_group_client_1  --nodecount 1 --templateName awstemplate\ninstancegroup configure --instanceGroup host_group_slave_1 --nodecount 3 --templateName awstemplate\nnetwork select --name default-aws-network\nsecuritygroup select --name all-services-port\nstack create --name my-first-stack --region us-east-1\ncluster create --description \nMy first cluster\n\n\n\n\n\nCongratulations!\n Your cluster should now be up and running on this way as well. To learn more about Cloudbreak and \nprovisioning, we have some \ninteresting insights\n for you", 
            "title": "AWS"
        }, 
        {
            "location": "/aws/#aws-images", 
            "text": "We have pre-built cloud images for AWS with the Cloudbreak Deployer pre-installed. You can launch the latest \nCloudbreak Deployer image based on your region at the  AWS Management Console .   Alternatively, instead of using the pre-built cloud images for AWS, you can install Cloudbreak Deployer on your own\n VM. See  installation page  for more information.   Please make sure you opened the following ports on your  security group :   SSH (22)  Cloudbreak API (8080)  Identity server (8089)  Cloudbreak GUI (3000)  User authentication (3001)", 
            "title": "AWS Images"
        }, 
        {
            "location": "/aws/#cloudbreak-deployer-aws-image-details", 
            "text": "Minimum and Recommended VM requirements :  8GB RAM, 10GB disk, 2 cores (The minimum instance type which is fit for cloudbreak is  m3.large )", 
            "title": "Cloudbreak Deployer AWS Image Details"
        }, 
        {
            "location": "/aws/#aws-setup", 
            "text": "Cloudbreak Deployer Highlights   The default SSH username for the EC2 instances is  cloudbreak .  Cloudbreak Deployer location is  /var/lib/cloudbreak-deployment  on the launched EC2 instance. This is the\n   cbd  root folder there.  All  cbd  actions must be executed from the  cbd  root folder as  cloudbreak  user.", 
            "title": "AWS Setup"
        }, 
        {
            "location": "/aws/#setup-cloudbreak-deployer", 
            "text": "You should already have the Cloudbreak Deployer either by  using the AWS Cloud Images  or by  installing the\nCloudbreak Deployer  manually on your own VM.  If you have your own installed VM, you should check the  Initialize your Profile \nsection here before starting the provisioning.  You can  connect to the previously created  cbd  VM .  To open the  cloudbreak-deployment  directory:  cd /var/lib/cloudbreak-deployment/  This is the directory of the configuration files and the supporting binaries for Cloudbreak Deployer.", 
            "title": "Setup Cloudbreak Deployer"
        }, 
        {
            "location": "/aws/#initialize-your-profile", 
            "text": "First initialize  cbd  by creating a  Profile  file:  cbd init  It will create a  Profile  file in the current directory. Please open the  Profile  file then check the  PUBLIC_IP .\nThis is mandatory, because of to can access the Cloudbreak UI (called Uluwatu). In some cases the  cbd  tool tries to\nguess it. If  cbd  cannot get the IP address during the initialization, please set the appropriate value.", 
            "title": "Initialize your Profile"
        }, 
        {
            "location": "/aws/#aws-specific-configuration", 
            "text": "AWS Account Keys  There are 2 ways to create AWS credentials in Cloudbreak.    Key-based: It requires your AWS access and secret key and Cloudbreak will use this key to launch the resources. This key needs to be provided when you create your credential in Cloudbreak either with Cloudbreak UI or Cloudbreak CLI.  Role-based: It requires a valid IAM User role and Cloudbreak will assume this role to get a temporary access and secret key. For this action you need to set your AWS key in the  Profile  file.\nWe suggest to use the keys of a valid  IAM User  here.   export AWS_ACCESS_KEY_ID=AKIA**************W7SA\nexport AWS_SECRET_ACCESS_KEY=RWCT4Cs8******************/*skiOkWD", 
            "title": "AWS specific configuration"
        }, 
        {
            "location": "/aws/#start-cloudbreak-deployer", 
            "text": "To start the Cloudbreak application use the following command.\nThis will start all the Docker containers and initialize the application.  cbd start   At the very first time it will take for a while, because of need to download all the necessary docker images.   The  cbd start  command includes the  cbd generate  command which applies the following steps:   creates the  docker-compose.yml  file that describes the configuration of all the Docker containers needed for the Cloudbreak deployment.  creates the  uaa.yml  file that holds the configuration of the identity server which is used to authenticate users to Cloudbreak.", 
            "title": "Start Cloudbreak Deployer"
        }, 
        {
            "location": "/aws/#validate-the-started-cloudbreak-deployer", 
            "text": "After the  cbd start  command finishes followings are worthy to check:   Pre-installed Cloudbreak Deployer version and health:      cbd doctor   In case of  cbd update  is needed, please check the related documentation for  Cloudbreak Deployer Update .    Started Cloudbreak Application logs:      cbd logs cloudbreak   Cloudbreak should start within a minute - you should see a line like this:  Started CloudbreakApplication in 36.823 seconds", 
            "title": "Validate the started Cloudbreak Deployer"
        }, 
        {
            "location": "/aws/#provisioning-prerequisites", 
            "text": "", 
            "title": "Provisioning Prerequisites"
        }, 
        {
            "location": "/aws/#iam-role-setup", 
            "text": "If you want to use your Aws Access Key and your Secret Access Key to authenticate to Amazon then please use the  Key based authentication  and you do not need to setup an IAM Role.   Cloudbreak works by connecting your AWS account through so called  Credentials , and then uses these credentials to \ncreate resources on your behalf.   IMPORTANT  Cloudbreak deployment uses two different AWS accounts for two different purposes:    The account belonging to the  Cloudbreak webapp  itself, acts as a  third party , that creates resources on the \naccount of the  end user . This account is configured at server-deployment time.  The account belonging to the  end user  who uses the UI or the Shell to create clusters. This account is configured\n when setting up credentials.   These accounts are usually  the same  when the end user is the same who deployed the Cloudbreak server, but it allows\n Cloudbreak to act as a SaaS project as well if needed.  Credentials use  IAM Roles  to give access to the \nthird party to act on behalf of the end user without giving full access to your resources.\nThis IAM Role will be  assumed  later by an IAM user.  AWS IAM Policy that grants permission to assume a role  You cannot assume a role with root account , so you need to  create an IAM user  with an attached  Inline  \npolicy  and  then set the \nAccess key and Secret Access key  in the  Profile  file (check  this description  out).  The  sts-assume-role  IAM user  policy  must be configured to have \npermission to assume roles on all resources. Here it is the policy to configure the  sts:AssumeRole  for all  Resource :  {\n\u2002\u2002 Version :  2012-10-17 ,\n\u2002\u2002 Statement : [\n\u2002\u2002\u2002\u2002{\n\u2002\u2002\u2002\u2002\u2002\u2002 Sid :  Stmt1400068149000 ,\n\u2002\u2002\u2002\u2002\u2002\u2002 Effect :  Allow ,\n\u2002\u2002\u2002\u2002\u2002\u2002 Action : [\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002 sts:AssumeRole \n\u2002\u2002\u2002\u2002\u2002\u2002],\n\u2002\u2002\u2002\u2002\u2002\u2002 Resource : [\n\u2002\u2002\u2002\u2002\u2002\u2002\u2002\u2002 * \n\u2002\u2002\u2002\u2002\u2002\u2002]\n\u2002\u2002\u2002\u2002}\n\u2002\u2002]\n}  To connect your ( end user ) AWS account with a credential in Cloudbreak you'll have to create an IAM role on your \nAWS account that is configured to allow the third-party account to access and create resources on your behalf.\nThe easiest way to do this is with  cbd  commands (but it can also be done manually from the  AWS Console ):  cbd aws generate-role  - Generates an AWS IAM role for Cloudbreak provisioning on AWS\ncbd aws show-role      - Show assumers and policies for an AWS role\ncbd aws delete-role    - Deletes an AWS IAM role, removes all inline policies  The  generate-role  command creates a role that is assumable by the Cloudbreak Deployer AWS account and has a broad policy setup.\nThis command creates a role with the name  cbreak-deployer  by default. If you'd like to create role with a different\n name or multiple roles, you need to add this line to your  Profile :  export AWS_ROLE_NAME=my-cloudbreak-role  You can check the generated role on your AWS console, under IAM roles:  Full size  here .", 
            "title": "IAM role setup"
        }, 
        {
            "location": "/aws/#generate-a-new-ssh-key", 
            "text": "All the instances created by Cloudbreak are configured to allow key-based SSH,\nso you'll need to provide an SSH public key that can be used later to SSH onto the instances in the clusters you'll create with Cloudbreak.\nYou can use one of your existing keys or you can generate a new one.  To generate a new SSH keypair:  ssh-keygen -t rsa -b 4096 -C  your_email@example.com \n# Creates a new ssh key, using the provided email as a label\n# Generating public/private rsa key pair.  # Enter file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter]\nYou'll be asked to enter a passphrase, but you can leave it empty.\n\n# Enter passphrase (empty for no passphrase): [Type a passphrase]\n# Enter same passphrase again: [Type passphrase again]  After you enter a passphrase the keypair is generated. The output should look something like below.  # Your identification has been saved in /Users/you/.ssh/id_rsa.\n# Your public key has been saved in /Users/you/.ssh/id_rsa.pub.\n# The key fingerprint is:\n# 01:0f:f4:3b:ca:85:sd:17:sd:7d:sd:68:9d:sd:a2:sd your_email@example.com  Later you'll need to pass the  .pub  file's contents to Cloudbreak and use the private part to SSH to the instances", 
            "title": "Generate a new SSH key"
        }, 
        {
            "location": "/aws/#provisioning-via-browser", 
            "text": "You can log into the Cloudbreak application at  http:// Public_IP :3000/ .  The main goal of the Cloudbreak UI is to easily create clusters on your own cloud provider account.\nThis description details the AWS setup - if you'd like to use a different cloud provider check out its manual.  This document explains the four steps that need to be followed to create Cloudbreak clusters from the UI:   connect your AWS account with Cloudbreak  create some template resources on the UI that describe the infrastructure of your clusters  create a blueprint that describes the HDP services in your clusters and add some recipes for customization  launch the cluster itself based on these resources    IMPORTANT  Make sure that you have sufficient qouta (CPU, network, etc) for the requested cluster size", 
            "title": "Provisioning via Browser"
        }, 
        {
            "location": "/aws/#setting-up-aws-credentials", 
            "text": "Cloudbreak works by connecting your AWS account through so called  Credentials , and then uses these credentials to \ncreate resources on your behalf. The credentials can be configured on the  manage credentials  panel on the \nCloudbreak Dashboard.  To create a new AWS credential follow these steps:   Select the credential type. For instance, select the  Role Based  Fill out the new credential  Name  Only alphanumeric and lowercase characters (min 5, max 100 characters) can be applied    Copy your AWS IAM role's Amazon Resource Name (ARN) to the  IAM Role ARN  field  Copy your SSH public key to the  SSH public key  field  The SSH public key must be in OpenSSH format and it's private keypair can be used later to  SSH onto every \ninstance  of every cluster you'll create with this credential.  The  SSH username  for the EC2 instances is  cloudbreak .      Any other parameter is optional here.  Public in account  means that all the users belonging to your account will be able to use this credential to create \nclusters, but cannot delete it.    Full size  here .", 
            "title": "Setting up AWS credentials"
        }, 
        {
            "location": "/aws/#infrastructure-templates", 
            "text": "After your AWS account is linked to Cloudbreak you can start creating resource templates that describe your clusters' infrastructure:   templates  networks  security groups   When you create one of the above resource,  Cloudbreak does not make any requests to AWS. Resources are only created\n on AWS after the  create cluster  button has pushed.  These templates are saved to Cloudbreak's database and can be \n reused with multiple clusters to describe the infrastructure.  Templates  Templates describe the  instances of your cluster  - the instance type and the attached volumes. A typical setup is\n to combine multiple templates in a cluster for the different types of nodes. For example you may want to attach multiple\n large disks to the datanodes or have memory optimized instances for Spark nodes.  The instance templates can be configured on the  manage templates  panel on the Cloudbreak Dashboard.  There are some optional configurations here as well:   Spot price (USD)  If specified Cloudbreak will request spot price instances (which might take a while or never be \nfulfilled by Amazon).  This option is  not supported  by the default RedHat images .  EBS encryption  is supported for all volume types. If this option is checked then all the attached disks  will be encrypted  by Amazon using the AWS KMS master keys.  If  Public in account  is checked all the users belonging to your account will be able to use this resource to create clusters, but cannot delete it.   Networks  Your clusters can be created in their own  Virtual Private Cloud (VPC)  or in one of your already existing VPCs.\nIf you choose an existing VPC it is possible to create a new subnet within the VPC or use an already existing one.\nThe subnet's IP range must be defined in the  Subnet (CIDR)  field using the general CIDR notation.  Default AWS Network  If you don't want to create or use your custom VPC, you can use the  default-aws-network  for all your \nCloudbreak clusters. It will create a new VPC with a  10.0.0.0/16  subnet every time a cluster is created.  Custom AWS Network  If you'd like to deploy a cluster to a custom VPC you'll have to  create a new network  template on the  manage \nnetworks  panel.  You have the following options:   Create a new VPC and a new subnet : Every time a cluster is created with this kind of network setup a new VPC and a new subnet with the specified IP range will be created for the instances on AWS.  Create a new subnet in an existing VPC :  Use this kind of network setup if you already have a VPC on AWS where you'd like to put the Cloudbreak created cluster but you'd like to have a separate subnet for it.  Use an existing subnet in an existing VPC :  Use this kind of network setup if you have an existing VPC with one or more subnets on AWS and you'd like to start the instances of a cluster in one of those subnets.   You can configure the  Subnet Identifier  and the  Internet Gateway Identifier  (IGW) of your VPC.   IMPORTANT  The subnet CIDR cannot overlap each other in a VPC. So you have to create different network \ntemplates for every each clusters.   To create a new subnet within the VPC, provide the ID of the subnet which is in the existing VPC and your cluster \nwill be launched into that subnet.  For example  you can create 3 different clusters with 3 different network \ntemplates for multiple subnets  10.0.0.0/24 ,  10.0.1.0/24 ,  10.0.2.0/24  with the same VPC and IGW identifiers.   IMPORTANT  Please make sure the define subnet here doesn't overlap with any of your already deployed subnet in \nthe VPC, because of the validation only happens after the cluster creation starts.  In case of existing subnet make sure you have enough room within your network space for the new instances. The \nprovided subnet CIDR will be ignored, but a proper CIDR range will be used.   If  Public in account  is checked all the users belonging to your account will be able to use this network template \nto create clusters, but cannot delete it.   NOTE  The VPCs, IGWs and subnet are created on AWS only after the the cluster provisioning starts with the selected \nnetwork template.    Full size  here .  Security groups  Security group templates are very similar to the  security groups on the AWS Console . They describe the allowed inbound traffic to the instances in the cluster. \nCurrently only one security group template can be selected for a Cloudbreak cluster and all the instances have a \npublic IP address so all the instances in the cluster will belong to the same security group.\nThis may change in a later release.  Default Security Group  You can also use the two pre-defined security groups in Cloudbreak.  only-ssh-and-ssl:  all ports are locked down except for SSH and gateway HTTPS (you can't access Hadoop services outside of the VPC):   SSH (22)  HTTPS (443)   all-services-port:  all Hadoop services and SSH, gateway and HTTPS are accessible by default:   SSH (22)  HTTPS (443)  Ambari (8080)  Consul (8500)  NN (50070)  RM Web (8088)  Scheduler (8030RM)  IPC (8050RM)  Job history server (19888)  HBase master (60000)  HBase master web (60010)  HBase RS (16020)  HBase RS info (60030)  Falcon (15000)  Storm (8744)  Hive metastore (9083)  Hive server (10000)  Hive server HTTP (10001)  Accumulo master (9999)  Accumulo Tserver (9997)  Atlas (21000)  KNOX (8443)  Oozie (11000)  Spark HS (18080)  NM Web (8042)  Zeppelin WebSocket (9996)  Zeppelin UI (9995)  Kibana (3080)  Elasticsearch (9200)   Custom Security Group  You can define your own security group by adding all the ports, protocols and CIDR range you'd like to use. The rules\n defined here doesn't need to contain the internal rules, those are automatically added by Cloudbreak to the security group on AWS.   IMPORTANT  443 and 22 ports needs to be there in every security group otherwise Cloudbreak won't be able to communicate with the \nprovisioned cluster   If  Public in account  is checked all the users belonging to your account will be able to use this security group \ntemplate to create clusters, but cannot delete it.   NOTE  The security groups are created on AWS only after the cluster provisioning starts with the selected security group template.    Full size  here .", 
            "title": "Infrastructure templates"
        }, 
        {
            "location": "/aws/#defining-cluster-services", 
            "text": "Blueprints  Blueprints are your declarative definition of a Hadoop cluster. These are the same blueprints that are  used by Ambari .  You can use the 3 default blueprints pre-defined in Cloudbreak or you can create your own ones.\nBlueprints can be added from file, URL (an  example blueprint ) or the \nwhole JSON can be written in the  JSON text  box.  The host groups in the JSON will be mapped to a set of instances when starting the cluster. Besides this the services and\n components will also be installed on the corresponding nodes. Blueprints can be modified later from the Ambari UI.   NOTE  Not necessary to define all the configuration in the blueprint. If a configuration is missing, Ambari will \nfill that with a default value.   If  Public in account  is checked all the users belonging to your account will be able to use this blueprint to \ncreate clusters, but cannot delete or modify it.   Full size  here .  A blueprint can be exported from a running Ambari cluster that can be reused in Cloudbreak with slight \nmodifications. \nThere is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the \nmodifications have to be done manually.\nWhen the blueprint is exported some configurations are hardcoded for example domain names, memory configurations...etc. that won't be applicable to the Cloudbreak cluster  Cluster customization  Sometimes it can be useful to  define some custom scripts so called Recipes in Cloudbreak  that run during cluster \ncreation and add some additional functionality.  For example it can be a service you'd like to install but it's not supported by Ambari or some script that \nautomatically downloads some data to the necessary nodes.\nThe most  notable example is Ranger setup :   It has a prerequisite of a running database when Ranger Admin is installing.  A PostgreSQL database can be easily started and configured with a recipe before the blueprint installation starts.   To learn more about these and check the Ranger recipe out, take a look at the  Cluster customization", 
            "title": "Defining cluster services"
        }, 
        {
            "location": "/aws/#cluster-deployment", 
            "text": "After all the cluster resources are configured you can deploy a new HDP cluster.  Here is a  basic flow for cluster creation on Cloudbreak Web UI :   Start by selecting a previously created AWS credential in the header.    Full size  here .   Open  create cluster   Configure Cluster  tab   Fill out the new cluster  name  Cluster name must start with a lowercase alphabetic character then you can apply lowercase alphanumeric and \n   hyphens only (min 5, max 40 characters)    Select a  Region  where you like your cluster be provisioned  Click on the  Setup Network and Security  button  If  Public in account  is checked all the users belonging to your account will be able to see the created cluster on\n the UI, but cannot delete or modify it.     Setup Network and Security  tab   Select one of the networks  Select one of the security groups  Click on the  Choose Blueprint  button  If  Enable security  is checked as well, Cloudbreak will install Key Distribution Center (KDC) and the cluster will \nbe Kerberized. See more about it in the  Kerberos  section of this documentation.     Choose Blueprint  tab   Select one of the blueprint  After you've selected a  Blueprint , you should be able to configure:  the templates  the number of nodes for all of the host groups in the blueprint  the recipes for nodes    Click on the  Review and Launch  button   Review and Launch  tab   After the  create and start cluster  button has clicked Cloudbreak will start to create the cluster's resources on \n your AWS account.   Cloudbreak uses  CloudFormation  to create the resources - you can check out the resources created by Cloudbreak on \nthe AWS Console CloudFormation page.  Full size  here .  Besides these you can check the progress on the Cloudbreak Web UI itself if you open the new cluster's  Event History .  Full size  here .  Advanced options  There are some advanced features when deploying a new cluster, these are the following:  Availability Zone  You can restrict the instances to a  specific availability zone . It may be useful if you're using\n reserved instances.  Use dedicated instances  You can use  dedicated instances  on EC2  Minimum cluster size  The provisioning strategy in case of the cloud provider cannot allocate all the requested nodes.  Validate blueprint  This is selected by default. Cloudbreak validates the Ambari blueprint in this case.  Shipyard enabled cluster  This is selected by default. Cloudbreak will start a  Shipyard  container which helps you to manage your containers.  Config recommendation strategy  Strategy for configuration recommendations how will be applied. Recommended \nconfigurations gathered by the response of the stack advisor.    NEVER_APPLY                Configuration recommendations are ignored with this option.  ONLY_STACK_DEFAULTS_APPLY  Applies only on the default configurations for all included services.  ALWAYS_APPLY               Applies on all configuration properties.   Start LDAP and configure SSSD  Enables the  System Security Services Daemon  configuration", 
            "title": "Cluster deployment"
        }, 
        {
            "location": "/aws/#cluster-termination", 
            "text": "You can terminate running or stopped clusters with the  terminate  button in the cluster details.   IMPORTANT  Always use Cloudbreak to terminate the cluster. If that fails for some reason, try to delete the \nCloudFormation stack first. Instances are started in an Auto Scaling Group so they may be restarted if you terminate an instance manually!   Sometimes Cloudbreak cannot synchronize it's state with the cluster state at the cloud provider and the cluster can't\n be terminated. In this case the  Forced termination  option can help to terminate the cluster at the Cloudbreak \n side.  If it has happened:   You should check the related resources at the AWS CloudFormation  If it is needed you need to manually remove resources from there    Full size  here .", 
            "title": "Cluster termination"
        }, 
        {
            "location": "/aws/#interactive-mode-cloudbreak-shell", 
            "text": "The goal with the Cloudbreak Shell (Cloudbreak CLI) was to provide an interactive command line tool which supports:   all functionality available through the REST API or Cloudbreak Web UI  makes possible complete automation of management task via scripts  context aware command availability  tab completion  required/optional parameter support  hint command to guide you on the usual path", 
            "title": "Interactive mode / Cloudbreak Shell"
        }, 
        {
            "location": "/aws/#start-cloudbreak-shell", 
            "text": "To start the Cloudbreak CLI use the following commands:   Open your  cloudbreak-deployment  directory if it is needed. For example:      cd cloudbreak-deployment   Start the  cbd  from here if it is needed      cbd start   In the root of your  cloudbreak-deployment  folder apply:      cbd util cloudbreak-shell   At the very first time it will take for a while, because of need to download all the necessary docker images.   This will launch the Cloudbreak shell inside a Docker container then it is ready to use.  Full size  here .   IMPORTANT You have to copy all your files into the  cbd  working directory, what you would like to use in shell.  For \nexample if your  cbd  working directory is  ~/cloudbreak-deployment  then copy your  blueprint JSON, public ssh key \nfile...etc.  to here. You can refer to these files with their names from the shell.", 
            "title": "Start Cloudbreak Shell"
        }, 
        {
            "location": "/aws/#autocomplete-and-hints", 
            "text": "Cloudbreak Shell helps to you with  hint messages  from the very beginning, for example:  cloudbreak-shell hint\nHint: Add a blueprint with the 'blueprint add' command or select an existing one with 'blueprint select'\ncloudbreak-shell   Beyond this you can use the  autocompletion (double-TAB)  as well:  cloudbreak-shell credential create --\ncredential create --AWS          credential create --AZURE        credential create --EC2          credential create --GCP          credential create --OPENSTACK", 
            "title": "Autocomplete and hints"
        }, 
        {
            "location": "/aws/#provisioning-via-cli", 
            "text": "", 
            "title": "Provisioning via CLI"
        }, 
        {
            "location": "/aws/#setting-up-aws-credential", 
            "text": "Cloudbreak works by connecting your AWS account through so called Credentials, and then uses these credentials to \ncreate resources on your behalf. Credentials can be configured with the following command for example:  credential create --AWS --name my-aws-credential --description  sample description  --roleArn \narn:aws:iam::***********:role/userrole --sshKeyString  ssh-rsa AAAAB****etc    NOTE  that Cloudbreak  does not set your cloud user details  - we work around the concept of  IAM  -  on Amazon (or other cloud providers) . You should have already a valid IAM role. You can \nfind further details  here .   Alternatives to provide  SSH Key :   you can upload your public key from an url:  \u2014sshKeyUrl    or you can add the path of your public key:  \u2014sshKeyPath   You can check whether the credential was created successfully  credential list  You can switch between your existing credentials  credential select --name my-aws-credential", 
            "title": "Setting up AWS credential"
        }, 
        {
            "location": "/aws/#infrastructure-templates_1", 
            "text": "After your AWS account is linked to Cloudbreak you can start creating resource templates that describe your clusters' infrastructure:   security groups  networks  templates   When you create one of the above resource,  Cloudbreak does not make any requests to AWS. Resources are only created\n on AWS after the  cluster create  has applied.  These templates are saved to Cloudbreak's database and can be \n reused with multiple clusters to describe the infrastructure.  Templates  Templates describe the  instances of your cluster  - the instance type and the attached volumes. A typical setup is\n to combine multiple templates in a cluster for the different types of nodes. For example you may want to attach multiple\n large disks to the datanodes or have memory optimized instances for Spark nodes.  A template can be used repeatedly to create identical copies of the same stack (or to use as a foundation to start a \nnew stack). Templates can be configured with the following command for example:  template create --AWS --name my-aws-template --description  sample description  --instanceType m4.large --volumeSize \n100 --volumeCount 2  Other available option here is  --publicInAccount . If it is true, all the users belonging to your account will be able\n to use this template to create clusters, but cannot delete it.  You can check whether the template was created successfully  template list  Networks  Your clusters can be created in their own  Virtual Private Cloud (VPC)  or in one of your already existing VPCs. If \nyou choose an existing VPC it is possible to create a new subnet within the VPC or use an already existing one. The \nsubnet's IP range must be defined in the  Subnet (CIDR)  field using the general CIDR notation.  Default AWS Network  If you don't want to create or use your custom VPC, you can use the  default-aws-network  for all your Cloudbreak \nclusters. It will create a new VPC with a  10.0.0.0/16  subnet every time a cluster is created.  Custom AWS Network  If you'd like to deploy a cluster to a custom VPC you'll have to  create a new network  template, to create a new \nsubnet within the VPC, provide the ID of the subnet which is in the existing VPC.  A network also can be used repeatedly to create identical copies of the same stack (or to use as a foundation to \nstart a new stack).   IMPORTANT  The subnet CIDR cannot overlap each other in a VPC. So you have to create different network templates \nfor every each clusters.\nFor example you can create 3 different clusters with 3 different network templates for multiple subnets 10.0.0.0/24,\n 10.0.1.0/24, 10.0.2.0/24 with the same VPC and IGW identifiers.   network create --AWS --name my-aws-network --subnet 10.0.0.0/16  Other available options:  --vpcID  your existing vpc on amazon  --internetGatewayID  your amazon internet gateway of the given VPC  --publicInAccount  If it is true, all the users belonging to your account will be able to use this network to create \nclusters, but cannot delete it.  You can check whether the network was created successfully  network list", 
            "title": "Infrastructure templates"
        }, 
        {
            "location": "/aws/#defining-cluster-services_1", 
            "text": "Blueprints  Blueprints are your declarative definition of a Hadoop cluster. These are the same blueprints that are  used by Ambari .  You can use the 3 default blueprints pre-defined in Cloudbreak or you can create your own ones.\nBlueprints can be added from file or URL (an  example blueprint ).  The host groups in the JSON will be mapped to a set of instances when starting the cluster. Besides this the services and\n components will also be installed on the corresponding nodes. Blueprints can be modified later from the Ambari UI.   NOTE  Not necessary to define all the configuration in the blueprint. If a configuration is missing, Ambari will fill that with a default value.   blueprint add --name my-blueprint --description  sample description  --file  the path of the blueprint   Other available options:  --url  the url of the blueprint  --publicInAccount  If it is true, all the users belonging to your account will be able to use this blueprint to create \nclusters, but cannot delete it.  You can check whether the blueprint was created successfully  blueprint list  A blueprint can be exported from a running Ambari cluster that can be reused in Cloudbreak with slight \nmodifications. \nThere is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the \nmodifications have to be done manually.\nWhen the blueprint is exported some configurations are hardcoded for example domain names, memory configurations..etc. that won't be applicable to the Cloudbreak cluster.  Cluster customization  Sometimes it can be useful to  define some custom scripts so called Recipes in Cloudbreak  that run during cluster \ncreation and add some additional functionality.  For example it can be a service you'd like to install but it's not supported by Ambari or some script that \nautomatically downloads some data to the necessary nodes.\nThe most  notable example is Ranger setup :   It has a prerequisite of a running database when Ranger Admin is installing.  A PostgreSQL database can be easily started and configured with a recipe before the blueprint installation starts.   To learn more about these and check the Ranger recipe out, take a look at the  Cluster customization", 
            "title": "Defining cluster services"
        }, 
        {
            "location": "/aws/#metadata-show", 
            "text": "You can check the stack metadata with  stack metadata --name myawsstack --instancegroup master  Other available options:  --id  In this case you can select a stack with id.  --outputType  In this case you can modify the outputformat of the command (RAW or JSON).", 
            "title": "Metadata show"
        }, 
        {
            "location": "/aws/#cluster-deployment_1", 
            "text": "After all the cluster resources are configured you can deploy a new HDP cluster. The following sub-sections show \nyou a  basic flow for cluster creation with Cloudbreak Shell .  Select credential  Select one of your previously created AWS credential:  credential select --name my-aws-credential  Select blueprint  Select one of your previously created blueprint which fits your needs:  blueprint select --name multi-node-hdfs-yarn  Configure instance groups  You must configure instance groups before provisioning. An instance group define a group of nodes with a specified \ntemplate. Usually we create instance groups for host groups in the blueprint.  instancegroup configure --instanceGroup cbgateway --nodecount 1 --templateName minviable-aws\ninstancegroup configure --instanceGroup master --nodecount 1 --templateName minviable-aws\ninstancegroup configure --instanceGroup slave_1 --nodecount 1 --templateName minviable-aws  Other available option:  --templateId  Id of the template  Select network  Select one of your previously created network which fits your needs or a default one:  network select --name default-aws-network  Select security group  Select one of your previously created security which fits your needs or a default one:  securitygroup select --name all-services-port  Create stack / Create cloud infrastructure  Stack means the running cloud infrastructure that is created based on the instance groups configured earlier \n( credential ,  instancegroups ,  network ,  securitygroup ). Same as in case of the API or UI the new cluster will \nuse your templates and by using CloudFormation will launch your cloud stack. Use the following command to create a \nstack to be used with your Hadoop cluster:  stack create --name myawsstack --region us-east-1  The infrastructure is created asynchronously, the state of the stack can be checked with the stack  show command . If \nit reports AVAILABLE, it means that the virtual machines and the corresponding infrastructure is running at the cloud provider.  Other available option is:  --wait  - in this case the create command will return only after the process has finished.   Create a Hadoop cluster / Cloud provisioning  You are almost done! One more command and your Hadoop cluster is starting!  Cloud provisioning is done once the \ncluster is up and running. The new cluster will use your selected blueprint and install your custom Hadoop cluster \nwith the selected components and services.  cluster create --description  my first cluster   Other available option is  --wait  - in this case the create command will return only after the process has finished.   You are done!  You have several opportunities to check the progress during the infrastructure creation then \nprovisioning:   Cloudbreak uses  CloudFormation  to create the resources - you can check out the resources created by Cloudbreak on\n the AWS Console CloudFormation page.   For example:  Full size  here .   If stack then cluster creation have successfully done, you can check the Ambari Web UI. However you need to know the \nAmbari IP (for example:  http://52.8.110.95:8080 ):   You can get the IP from the CLI as a result ( ambariServerIp 52.8.110.95 ) of the following command:              cluster show  For example:  Full size  here .   Besides these you can check the entire progress and the Ambari IP as well on the Cloudbreak Web UI itself. Open the \nnew cluster's  details  and its  Event History  here.   For example:  Full size  here .  Stop cluster  You have the ability to  stop your existing stack then its cluster  if you want to suspend the work on it.  Select a stack for example with its name:  stack select --name my-stack  Other available option to define a stack is its  --id .  Every time you should stop the  cluster  first then the  stack . So apply following commands to stop the previously \nselected stack:  cluster stop\nstack stop  Restart cluster  Select your stack that you would like to restart  after this you can apply:  stack start  After the stack has successfully restarted, you can  restart the related cluster as well :  cluster start  Upscale cluster  If you need more instances to your infrastructure, you can  upscale your selected stack :  stack node --ADD --instanceGroup host_group_slave_1 --adjustment 6  Other available option is  --withClusterUpScale  - this indicates also a cluster upscale after the stack upscale. You\n can upscale the related cluster separately if you want to do this:  cluster node --ADD --hostgroup host_group_slave_1 --adjustment 6  Downscale cluster  You also can reduce the number of instances in your infrastructure.  After you selected your stack :  cluster node --REMOVE  --hostgroup host_group_slave_1 --adjustment -2  Other available option is  --withStackDownScale  - this indicates also a stack downscale after the cluster downscale.\n You can downscale the related stack separately if you want to do this:  stack node --REMOVE  --instanceGroup host_group_slave_1 --adjustment -2", 
            "title": "Cluster deployment"
        }, 
        {
            "location": "/aws/#cluster-termination_1", 
            "text": "You can terminate running or stopped clusters with  stack terminate --name myawsstack  Other available option is  --wait  - in this case the terminate command will return only after the process has finished.    IMPORTANT  Always use Cloudbreak to terminate the cluster. If that fails for some reason, try to delete the \nCloudFormation stack first. Instances are started in an Auto Scaling Group so they may be restarted if you terminate an instance manually!   Sometimes Cloudbreak cannot synchronize it's state with the cluster state at the cloud provider and the cluster can't\n be terminated. In this case the  Forced termination  option on the Cloudbreak Web UI can help to terminate the cluster\n  at the Cloudbreak side.  If it has happened:   You should check the related resources at the AWS CloudFormation  If it is needed you need to manually remove resources from ther", 
            "title": "Cluster termination"
        }, 
        {
            "location": "/aws/#silent-mode", 
            "text": "With Cloudbreak Shell you can execute script files as well. A script file contains shell commands and can \nbe executed with the  script  cloudbreak shell command  script  your script file   or with the  cbd util cloudbreak-shell-quiet  command  cbd util cloudbreak-shell-quiet   example.sh   IMPORTANT  You have to copy all your files into the  cbd  working directory, what you would like to use in shell.\n For example if your  cbd  working directory is ~/cloudbreak-deployment then copy your script file to here.", 
            "title": "Silent mode"
        }, 
        {
            "location": "/aws/#example", 
            "text": "The following example creates a Hadoop cluster with  hdp-small-default  blueprint on M4Xlarge instances with 2X100G \nattached disks on  default-aws-network  network using  all-services-port  security group. You should copy your ssh \npublic key file into your  cbd  working directory with name  id_rsa.pub  and paste your AWS credentials in the parts with  ...  highlight.  credential create --AWS --description description --name my-aws-credential --roleArn  arn role  --sshKeyPath id_rsa.pub\ncredential select --name my-aws-credential\ntemplate create --AWS --name awstemplate --description aws-template --instanceType m4.xlarge --volumeSize 100 \n--volumeCount 2\nblueprint select --name hdp-small-default\ninstancegroup configure --instanceGroup cbgateway --nodecount 1 --templateName awstemplate\ninstancegroup configure --instanceGroup host_group_master_1 --nodecount 1 --templateName awstemplate\ninstancegroup configure --instanceGroup host_group_master_2 --nodecount 1 --templateName awstemplate\ninstancegroup configure --instanceGroup host_group_master_3 --nodecount 1 --templateName awstemplate\ninstancegroup configure --instanceGroup host_group_client_1  --nodecount 1 --templateName awstemplate\ninstancegroup configure --instanceGroup host_group_slave_1 --nodecount 3 --templateName awstemplate\nnetwork select --name default-aws-network\nsecuritygroup select --name all-services-port\nstack create --name my-first-stack --region us-east-1\ncluster create --description  My first cluster   Congratulations!  Your cluster should now be up and running on this way as well. To learn more about Cloudbreak and \nprovisioning, we have some  interesting insights  for you", 
            "title": "Example"
        }, 
        {
            "location": "/azure/", 
            "text": "Azure Setup\n\n\nOn other cloud providers, we provide \u201cpublic images\u201d that are pre-built with the Cloudbreak Deployer. But on Azure, its a different process.\nWe provide a way to launch Cloudbreak Deployer based on the new \nAzure Resource Manager \nTemplates\n.\n\n\nDeploy using the Azure Portal\n\n\nTo get started using the Azure Resource Manager template to install Cloudbreak, it is as simple as clicking here: \n  \n \n\n\n\n\nMinimum and Recommended VM requirements\n:\n 8GB RAM, 10GB disk, 2 cores (The minimum instance type which is fit for cloudbreak is \nD2\n)\n\n\n\n\nThe following parameters are mandatory (beyond to the default values) for the new \ncbd\n Template!\n\n\nOn the \nCustom deployment\n panel:\n\n\n\n\nPlease create a new \nResource group\n\n\nSelect an appropriate \nResource group location\n\n\n\n\nOn the \nParameters\n panel:\n\n\n\n\nSelect the same \nLOCATION\n as for the resource group\n\n\nPASSWORD\n must be between 6-72 characters long and must satisfy \n  at least 3 of password complexity requirements from the following:\n\n\nContains an uppercase character\n\n\nContains a lowercase character\n\n\nContains a numeric digit\n\n\nContains a special character\n\n\n\n\n\n\n\n\nFinally\n you should review the \nLegal terms\n from the \nCustom deployment\n panel:\n\n\n\n\nIf you agree with the terms and conditions, just click on \nCreate\n \nbutton of this panel\n\n\nAlso click on the \nCreate\n button on the \nCustom deployment\n \n\n\n\n\n\n\nDeployment takes about \n15-20 minutes\n. You can track the \nprogress on the resource group details. If any issue has occurred, open the \nAudit logs\n from the settings. \nWe have faced an interesting behaviour on the Azure Portal: \nAll operations were successful on template deployment, \nbut overall fail\n.\n\n\n\n\n\n\nOnce it's successful done, you can reach the Cloudbreak UI \nat:\nhttp://\nVM Public IP\n:3000/\n\n\nemail: admin@example.com\n\n\npassword: cloudbreak\n\n\n\n\n\n\n\n\nUnder the hood\n\n\nMeanwhile Azure is creating the deployment, here is some information about what happens in the background:\n\n\n\n\nStart an instance from the official CentOS image\n\n\nSo no custom image copy is needed, which would take about 30 \n   minutes\n\n\n\n\n\n\nUse \nDocker VM Extension\n to install Docker\n\n\nUse \nCustomScript Extension\n to install \nCloudbreak Deployer (\ncbd\n)\n\n\n\n\nCloudbreak Deployer Highlights\n\n\n\n\nThe default SSH username for the Azure VMs is \ncloudbreak\n.\n\n\nCloudbreak Deployer location is \n/var/lib/cloudbreak-deployment\n on the launched \ncbd\n VM. This is the \n      \ncbd\n root folder there.\n\n\nAll \ncbd\n actions must be executed from the \ncbd\n root folder.\n\n\nMost of the \ncbd\n commands require \nroot\n permissions. So it would be worth if you apply the \nsudo su\n.\n\n\n\n\nValidate the started Cloudbreak Deployer\n\n\n\n\n\n\nSSH to the launched Azure VM.\n\n\n\n\n\n\nMost of the \ncbd\n commands require \nroot\n permissions. So it would be worth if you apply the:\n\n\n\n\n\n\n  sudo su\n\n\n\n\n\n\nThis is a MUST on Azure because the \nCustomscript Extension\n which basically creates everything running as sudo and this is not modifiable.\n\n\n\n\n\n\nOpen the \ncloudbreak-deployment\n directory:\n\n\n\n\n  cd /var/lib/cloudbreak-deployment\n\n\n\n\n\n\nPre-installed Cloudbreak Deployer version and health:\n\n\n\n\n  cbd doctor\n\n\n\n\n\n\nIn case of \ncbd update\n is needed, please check the related documentation for \nCloudbreak Deployer Update\n. Most of the \ncbd\n commands require \nroot\n permissions.\n\n\n\n\n\n\nStarted Cloudbreak Application logs:\n\n\n\n\n   cbd logs cloudbreak\n\n\n\n\n\n\nCloudbreak should start within a minute - you should see a line like this: \nStarted CloudbreakApplication in 36.823 seconds\n\n\n\n\nProvisioning Prerequisites\n\n\nWe use the new \nAzure ARM\n in \norder to launch clusters. In order to work we need to create an \nActive Directory\n application with the configured name and password and adds the permissions that are needed to call the Azure Resource Manager API. Cloudbreak Deployer automates all this for you.\n\n\n\n\nIf you forget to configure these steps you will not able to create any resource with Cloudbreak\n\n\n\n\nAzure access setup\n\n\nIf you do not have an \nActive Directory (AD)\n user then you have to configure it before deploying a cluster with \nCloudbreak:\n\n\n\n\nWhy you need this? Read more \nhere\n\n\n\n\n\n\nGo to \nmanage.windowsazure.com\n \n \nActive Directory\n\n\nSelect one of your AD where you would like to create the new user\n\n\nYou can configure your AD users on \nYour active directory\n \n \nUsers\n menu\n\n\n\n\n\n\nFull size \nhere\n.\n\n\n\n\nHere you can add the new user to AD. Simply click on \nAdd User\n in the bottom of the page\n\n\nTYPE OF USER\n: select \nNew user in your organization\n\n\nUSER NAME\n: type the new user name into the box\n\n\nFill out the name fields for the new user on the second page of the ADD USER window\n\n\nSubmit the new user creation on the third window with the big green button\n\n\nCopy the password \nFolo4965\n\n\nClick on the tick button in the bottom of the the ADD USER window\n\n\n\n\n\n\nYou will see the new user in the \nUSERS\n list\n\n\n\n\n\n\nYou have got a temporary password so you have to change it before you start using the new user.\n\n\n\n\n\n\nYou need to add your AD user to the \nmanage.windowsazure.com\n \n \nSettings\n \n \nAdministrators\n\n\n\n\n\n\nFull size \nhere\n.\n\n\n\n\nHere you can add the new user to Administrators. Simply click on \nAdd\n in the bottom of the page\n\n\nEMAIL ADDRESS\n: copy the previously created user email address here\n\n\nSelect the appropriate \nSUBSCRIPTION\n for the user\n\n\nClick on the tick button in the bottom of the the ADD A CO-ADMINISTRATOR window\n\n\n\n\n\n\nYou will see the new co-administrator a in the \nADMINISTRATORS\n list\n\n\n\n\nAzure application setup with Cloudbreak Deployer\n\n\nIn order for Cloudbreak to be able to launch clusters on Azure on your behalf you need to set up your \nAzure ARM \napplication\n. If you do not want to create your ARM application via the Azure Web UI, \nwe automated the related Azure \nconfigurations in the Cloudbreak Deployer\n.\n\n\nIf you use our \nAzure Template for Cloudbreak Deployer\n, you should:\n\n\n\n\nSSH to the Cloudbreak Deployer Virtual machine\n\n\ncbd\n location is \n/var/lib/cloudbreak-deployment\n\n\nall \ncbd\n actions must be executed from the \ncbd\n folder\n\n\n\n\n\n\nMost of the \ncbd\n commands require \nroot\n permissions. \nSo \nsudo su\n here would be worth for you.\n\n\n\n\nYou can setup your Azure Application with the following \ncbd\n command:\n\n\n\n\nWhy you need this? Read more \nhere\n\n\n\n\ncbd azure configure-arm --app_name myapp --app_password password123 --subscription_id 1234-abcd-efgh-1234\n\n\n\n\nOther available options:\n\n\n--app_name\n your new application name, \napp\n by default\n\n\n--app_password\n your application password, \npassword\n by default\n\n\n--subscription_id\n your Azure subscription ID\n\n\n--username\n your Azure username\n\n\n--password\n your Azure password\n\n\nThe command applies the following steps:\n\n\n\n\nIt creates an Active Directory application with the configured name, password\n\n\nIt grants permissions to call the Azure Resource Manager API\n\n\n\n\nPlease use the output of the command when you creating your Azure credential in Cloudbreak.\n The major part of \nthe output should be like this example:\n\n\nSubscription ID: sdf324-26b3-sdf234-ad10-234dfsdfsd\nApp ID: 234sdf-c469-sdf234-9062-dsf324\nPassword: password123\nApp Owner Tenant ID: sdwerwe1-d98e-dsf12-dsf123-df123232\n\n\n\n\nFile system configuration\n\n\nWhen starting a cluster with Cloudbreak on Azure, the default file system is \u201cWindows Azure Blob Storage\u201d. Hadoop has \nbuilt-in support for the \nWASB file system\n so it can be\nused easily as HDFS.\n\n\nDisks and blob storage\n\n\nIn Azure every data disk attached to a virtual machine \nis stored\n as a virtual hard disk (VHD) in a page blob inside an Azure storage account. Because these are not local disks and the operations must be done on the VHD files it causes degraded performance when used as HDFS.\nWhen WASB is used as a Hadoop file system the files are full-value blobs in a storage account. It means better performance compared to the data disks and the WASB file system can be configured very easily but Azure storage accounts have their own \nlimitations\n as well. There is a space limitation for TB per storage account (500 TB) as well but the real bottleneck is the total request rate that is only 20000 IOPS where Azure will start to throw errors when trying to do an I/O operation.\nTo bypass those limits Microsoft created a small service called \nDASH\n. DASH itself is a service that imitates the API of the Azure Blob Storage API and it can be deployed as a Microsoft Azure Cloud Service. Because its API is the same as the standard blob storage API it can be used \nalmost\n in the same way as the default WASB file system from a Hadoop deployment.\nDASH works by sharding the storage access across multiple storage accounts. It can be configured to distribute storage account load to at most 15 \nscaleout\n storage accounts. It needs one more \nnamespace\n storage account where it keeps track of where the data is stored.\nWhen configuring a WASB file system with Hadoop, the only required config entries are the ones where the access details are described. To access a storage account Azure generates an access key that is displayed on the Azure portal or can be queried through the API while the account name is the name of the storage account itself. A DASH service has a similar account name and key, those can be configured in the configuration file while deploying the cloud service.\n\n\n\n\nDeploying a DASH service with Cloudbreak Deployer\n\n\nWe automated the deployment of DASH service in Cloudbreak Deployer. After \ncbd\n is installed, simply run the \nfollowing command to deploy a DASH cloud service with 5 scale out storage accounts:\n\n\ncbd azure deploy-dash --accounts 5 --prefix dash --location \nWest Europe\n --instances 3\n\n\n\n\nThe command applies the following steps:\n\n\n\n\nIt creates the namespace account and the scale out storage accounts\n\n\nIt builds the \n.cscfg\n configuration file based on the created storage account names and keys\n\n\nIt generates an Account Name and an Account Key for the DASH service\n\n\nFinally it deploys the cloud service package file to a new cloud service\n\n\n\n\nThe WASB file system configured with DASH can be used as a data lake - when multiple clusters are deployed with the \nsame DASH file system configuration the same data can be accessed from all the clusters, but every cluster can have a \ndifferent service configured as well. In that case deploy as many DASH services with \ncbd\n as clusters with \nCloudbreak and configure them accordingly.\n\n\nContainers within the storage account\n\n\nCloudbreak creates a new container in the configured storage account for each cluster with the following name \npattern \ncloudbreak-UNIQUE_ID\n. Re-using existing containers in the same account is not supported as dirty data can \nlead to failing cluster installations. In order to take advantage of the WASB file system your data does not have to \nbe in the same storage account nor in the same container. You can add as many accounts as you wish through Ambari, by\n setting the properties described \nhere\n. Once you \n added the appropriate properties you can use those storage accounts with the pre-existing data, like:\n\n\nhadoop fs -ls wasb://data@youraccount.blob.core.windows.net/terasort-input/\n\n\n\n\n\n\nIMPORTANT\n Make sure that your cloud account can launch instances using the new Azure ARM (a.k.a. V2) API and \nyou have sufficient qouta (CPU, network, etc) for the requested cluster size.\n\n\n\n\nGenerate a new SSH key\n\n\nAll the instances created by Cloudbreak are configured to allow key-based SSH,\nso you'll need to provide an SSH public key that can be used later to SSH onto the instances in the clusters you'll create with Cloudbreak.\nYou can use one of your existing keys or you can generate a new one.\n\n\nTo generate a new SSH keypair:\n\n\nssh-keygen -t rsa -b 4096 -C \nyour_email@example.com\n\n# Creates a new ssh key, using the provided email as a label\n# Generating public/private rsa key pair.\n\n\n\n\n# Enter file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter]\nYou'll be asked to enter a passphrase, but you can leave it empty.\n\n# Enter passphrase (empty for no passphrase): [Type a passphrase]\n# Enter same passphrase again: [Type passphrase again]\n\n\n\n\nAfter you enter a passphrase the keypair is generated. The output should look something like below.\n\n\n# Your identification has been saved in /Users/you/.ssh/id_rsa.\n# Your public key has been saved in /Users/you/.ssh/id_rsa.pub.\n# The key fingerprint is:\n# 01:0f:f4:3b:ca:85:sd:17:sd:7d:sd:68:9d:sd:a2:sd your_email@example.com\n\n\n\n\nLater you'll need to pass the \n.pub\n file's contents to Cloudbreak and use the private part to SSH to the instances\n\n\nProvisioning via Browser\n\n\nYou can log into the Cloudbreak application at \nhttp://\nPublic_IP\n:3000/\n.\n\n\nThe main goal of the Cloudbreak UI is to easily create clusters on your own cloud provider account.\nThis description details the AZURE setup - if you'd like to use a different cloud provider check out its manual.\n\n\nThis document explains the four steps that need to be followed to create Cloudbreak clusters from the UI:\n\n\n\n\nconnect your AZURE account with Cloudbreak\n\n\ncreate some template resources on the UI that describe the infrastructure of your clusters\n\n\ncreate a blueprint that describes the HDP services in your clusters and add some recipes for customization\n\n\nlaunch the cluster itself based on these template resource\n\n\n\n\nSetting up Azure credentials\n\n\nCloudbreak works by connecting your AZURE account through so called \nCredentials\n, and then uses these credentials to \ncreate resources on your behalf. The credentials can be configured on the \nmanage credentials\n panel on the \nCloudbreak Dashboard.\n\n\n\n\nPlease read the \nProvisioning prerequisites\n where you \ncan find the steps how can get the mandatory \nSubscription ID\n, \nApp ID\n, \nPassword\n and \nApp Owner Tenant ID\n for \nyour Cloudbreak credential.\n\n\n\n\nTo create a new AZURE credential follow these steps:\n\n\n\n\nFill out the new credential \nName\n\n\nOnly alphanumeric and lowercase characters (min 5, max 100 characters) can be applied\n\n\n\n\n\n\nCopy your AZURE Subscription ID to the \nSubscription Id\n field\n\n\n\n\n\n\nFull size \nhere\n.\n\n\n\n\nCopy your AZURE Active Directory Application:\n\n\nID to the \nApp Id\n field\n\n\npassword to the \nPassword\n field\n\n\nApp Owner Tenant Id\n field\n\n\n\n\n\n\n\n\n\n\nFull size \nhere\n.\n\n\n\n\nCopy your SSH public key to the \nSSH public key\n field\n\n\nThe SSH public key must be in OpenSSH format and it's private keypair can be used later to \nSSH onto every \ninstance\n of every cluster you'll create with this credential.\n\n\nThe \nSSH username\n for the AZURE instances is \ncloudbreak\n.\n\n\n\n\n\n\n\n\n\n\nAny other parameter is optional here.\n\n\nPublic in account\n means that all the users belonging to your account will be able to use this credential to create \nclusters, but cannot delete it.\n\n\nCloudbreak is supporting simple rsa public key instead of X509 certificate file after 1.0.4 version\n\n\n\n\n\n\nFull size \nhere\n.\n\n\nInfrastructure templates\n\n\nAfter your AZURE account is linked to Cloudbreak you can start creating resource templates that describe your clusters' \ninfrastructure:\n\n\n\n\ntemplates\n\n\nnetworks\n\n\nsecurity groups\n\n\n\n\nWhen you create one of the above resource, \nCloudbreak does not make any requests to AZURE. Resources are only created\n on AZURE after the \ncreate cluster\n button has pushed.\n These templates are saved to Cloudbreak's database and can be \n reused with multiple clusters to describe the infrastructure\n\n\nTemplates\n\n\nTemplates describe the \ninstances of your cluster\n - the instance type and the attached volumes. A typical setup is\n to combine multiple templates in a cluster for the different types of nodes. For example you may want to attach multiple\n large disks to the datanodes or have memory optimized instances for Spark nodes.\n\n\nThe instance templates can be configured on the \nmanage templates\n panel on the Cloudbreak Dashboard.\n\n\nThe \nVolume Type\n describes the \nStorage Account type\n which will be used for the attached disks. The only constraint is that the \nPremium storage\n can only be used\nfor \nDS\n instance types. For more details about the premium storage read \nthis\n.\n\n\nIf \nPublic in account\nis checked all the users belonging to your account will be able to use this resource to create \nclusters, but cannot delete it\n\n\nNetworks\n\n\nYour clusters can be created in their own \nnetworks\n or in one of your already existing one. The subnet's IP range must be defined in \nthe \nSubnet (CIDR)\n field using the general CIDR notation.\n\n\nDefault AZURE Network\n\n\nIf you don't want to create or use your custom network, you can use the \ndefault-azure-network\n for all your \nCloudbreak clusters. It will create a new network with a \n10.0.0.0/16\n subnet every time a cluster is created.\n\n\nCustom AZURE Network\n\n\nIf you'd like to deploy a cluster to a custom network you'll have to \ncreate a new network\n template on the \nmanage \nnetworks\n panel.\n\n\nYou have the following options:\n\n\n\n\nCreate a new virtual network and a new subnet\n:  Every time a cluster is created with this kind of network setup a new virtual network and a new subnet with the specified IP range will be created for the instances on Azure.\n\n\nUse an existing subnet in an existing virtual network\n: Use this kind of network setup if you have an existing virtual network with one or more subnets on Azure and you'd like to start the instances of a cluster in one of those subnets. In this case you can define the \nSubnet Identifier\n and the \nVirtual Network Identifier\n and the \nResource Group Identifier\n of your network. The \nResource Group Identifier\n identifies the resource group which contains your existing virtual network. The \nVirtual Network Identifier\n and the \nSubnet Identifier\n will tell Cloudbreak which network and subnet to use to launch the new instances.\n\n\n\n\n\n\nIMPORTANT\n In case of existing subnet make sure you have enough room within your network space for the new instances. The \nprovided subnet CIDR will be ignored, but the existing subnet's CIDR range will be used. The security group behavior will be changed in this case as well\ndescribed in the security group section below.\n\n\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to use this network template \nto create clusters, but cannot delete it.\n\n\n\n\nNOTE\n The new networks are created on AZURE only after the the cluster provisioning starts with the selected \nnetwork template.\n\n\n\n\n\n\nFull size \nhere\n.\n\n\nSecurity groups\n\n\nSecurity group templates are very similar to the \nsecurity groups on Azure\n.\n\nThey describe the allowed inbound traffic to the instances in the cluster.\n\nCurrently only one security group template can be selected for a Cloudbreak cluster and all the instances have a \npublic IP address so all the instances in the cluster will belong to the same security group.\nThis may change in a later release.\n\n\nDefault Security Group\n\n\nYou can also use the two pre-defined security groups in Cloudbreak.\n\n\nonly-ssh-and-ssl:\n all ports are locked down except for SSH and gateway HTTPS (you can't access Hadoop services \noutside of the virtual network):\n\n\n\n\nSSH (22)\n\n\nHTTPS (443)\n\n\n\n\nall-services-port:\n all Hadoop services and SSH, gateway and HTTPS are accessible by default:\n\n\n\n\nSSH (22)\n\n\nHTTPS (443)\n\n\nAmbari (8080)\n\n\nConsul (8500)\n\n\nNN (50070)\n\n\nRM Web (8088)\n\n\nScheduler (8030RM)\n\n\nIPC (8050RM)\n\n\nJob history server (19888)\n\n\nHBase master (60000)\n\n\nHBase master web (60010)\n\n\nHBase RS (16020)\n\n\nHBase RS info (60030)\n\n\nFalcon (15000)\n\n\nStorm (8744)\n\n\nHive metastore (9083)\n\n\nHive server (10000)\n\n\nHive server HTTP (10001)\n\n\nAccumulo master (9999)\n\n\nAccumulo Tserver (9997)\n\n\nAtlas (21000)\n\n\nKNOX (8443)\n\n\nOozie (11000)\n\n\nSpark HS (18080)\n\n\nNM Web (8042)\n\n\nZeppelin WebSocket (9996)\n\n\nZeppelin UI (9995)\n\n\nKibana (3080)\n\n\nElasticsearch (9200)\n\n\n\n\nCustom Security Group\n\n\nYou can define your own security group by adding all the ports, protocols and CIDR range you'd like to use. The rules\n defined here doesn't need to contain the internal rules, those are automatically added by Cloudbreak to the security\n  group on Azure.\n\n\n\n\nIMPORTANT\n 443 and 22 ports needs to be there in every security group otherwise Cloudbreak won't be able to communicate with the \nprovisioned cluster\n\n\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to use this security group \ntemplate to create clusters, but cannot delete it.\n\n\n\n\nNOTE\n The security groups are created on Azure only after the cluster provisioning starts with the selected \nsecurity group template.\n\n\nIMPORTANT\n If you use and existing virtual network and subnet the selected security group will only be applied to the \nCloudbreak gateway node\n due to the lack of\ncapability to attach multiple security groups to an existing subnet. If you'd like to open ports for Hadoop you must do it on your existing security group.\n\n\n\n\n\n\nFull size \nhere\n.\n/sub\n\n\nDefining cluster services\n\n\nBlueprints\n\n\nBlueprints are your declarative definition of a Hadoop cluster. These are the same blueprints that are \nused by Ambari\n.\n\n\nYou can use the 3 default blueprints pre-defined in Cloudbreak or you can create your own ones.\nBlueprints can be added from file, URL (an \nexample blueprint\n) or the \nwhole JSON can be written in the \nJSON text\n box.\n\n\nThe host groups in the JSON will be mapped to a set of instances when starting the cluster. Besides this the services and\n components will also be installed on the corresponding nodes. Blueprints can be modified later from the Ambari UI.\n\n\n\n\nNOTE\n Not necessary to define all the configuration in the blueprint. If a configuration is missing, Ambari will \nfill that with a default value.\n\n\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to use this blueprint to \ncreate clusters, but cannot delete or modify it.\n\n\n\n\nFull size \nhere\n.\n\n\nA blueprint can be exported from a running Ambari cluster that can be reused in Cloudbreak with slight \nmodifications.\n\nThere is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the \nmodifications have to be done manually.\nWhen the blueprint is exported some configurations are hardcoded for example domain names, memory configurations...etc. that won't be applicable to the Cloudbreak cluster\n\n\nCluster customization\n\n\nSometimes it can be useful to \ndefine some custom scripts so called Recipes in Cloudbreak\n that run during cluster \ncreation and add some additional functionality.\n\n\nFor example it can be a service you'd like to install but it's not supported by Ambari or some script that \nautomatically downloads some data to the necessary nodes.\nThe most \nnotable example is Ranger setup\n:\n\n\n\n\nIt has a prerequisite of a running database when Ranger Admin is installing.\n\n\nA PostgreSQL database can be easily started and configured with a recipe before the blueprint installation starts.\n\n\n\n\nTo learn more about these and check the Ranger recipe out, take a look at the \nCluster customization\n\n\nCluster customization\n\n\nSometimes it can be useful to \ndefine some custom scripts so called Recipes in Cloudbreak\n that run during cluster \ncreation and add some additional functionality.\n\n\nFor example it can be a service you'd like to install but it's not supported by Ambari or some script that \nautomatically downloads some data to the necessary nodes.\nThe most \nnotable example is Ranger setup\n:\n\n\n\n\nIt has a prerequisite of a running database when Ranger Admin is installing.\n\n\nA PostgreSQL database can be easily started and configured with a recipe before the blueprint installation starts.\n\n\n\n\nTo learn more about these and check the Ranger recipe out, take a look at the \nCluster customization\n.\n\n\nCluster deployment\n\n\nAfter all the cluster resources are configured you can deploy a new HDP cluster.\n\n\nHere is a \nbasic flow for cluster creation on Cloudbreak Web UI\n:\n\n\n\n\nStart by selecting a previously created Azure credential in the header.\n\n\n\n\n\n\nFull size \nhere\n.\n\n\n\n\nOpen \ncreate cluster\n\n\n\n\nConfigure Cluster\n tab\n\n\n\n\nFill out the new cluster \nname\n\n\nCluster name must start with a lowercase alphabetic character then you can apply lowercase alphanumeric and \n   hyphens only (min 5, max 40 characters)\n\n\n\n\n\n\nSelect a \nRegion\n where you like your cluster be provisioned\n\n\nClick on the \nSetup Network and Security\n button\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to see the created cluster on\n the UI, but cannot delete or modify it.\n\n\n\n\n\n\n\n\nSetup Network and Security\n tab\n\n\n\n\nSelect one of the networks\n\n\nSelect one of the security groups\n\n\nClick on the \nChoose Blueprint\n button\n\n\nIf \nEnable security\n is checked as well, Cloudbreak will install Key Distribution Center (KDC) and the cluster will \nbe Kerberized. See more about it in the \nKerberos\n section of this documentation.\n\n\n\n\n\n\n\n\nChoose Blueprint\n tab\n\n\n\n\nSelect one of the blueprint\n\n\nAfter you've selected a \nBlueprint\n, you should be able to configure:\n\n\nthe templates\n\n\nthe number of nodes for all of the host groups in the blueprint\n\n\nthe recipes for nodes\n\n\n\n\n\n\nClick on the \nAdd File System\n button\n\n\n\n\nAdd File System\n tab\n\n\n\n\nSelect one of the file system that fits your needs\n\n\nAfter you've selected \nWASB\n or \nDASH\n, you should configure:\n\n\nStorage Account Name\n\n\nStorage Account Access Key\n\n\n\n\n\n\nClick on the \nReview and Launch\n button\n\n\nFile system\n is a mandatory configuration for Azure. You can read more about WASB and DASH in the \nFile System Configuration section\n.\n\n\n\n\n\n\n\n\nReview and Launch\n tab\n\n\n\n\nAfter the \ncreate and start cluster\n button has clicked Cloudbreak will start to create the cluster's resources on \n your Azure account.\n\n\n\n\nCloudbreak uses \nAzure Resource Manager\n to create the resources - you can check out the resources created by Cloudbreak\n on \nthe \nAzure Portal Resource groups\n page.\n\n\n\nFull size \nhere\n.\n\n\nBesides these you can check the progress on the Cloudbreak Web UI itself if you open the new cluster's \nEvent History\n.\n\n\n\nFull size \nhere\n.\n\n\nAdvanced options\n\n\nThere are some advanced features when deploying a new cluster, these are the following:\n\n\nMinimum cluster size\n The provisioning strategy in case of the cloud provider cannot allocate all the requested nodes.\n\n\nValidate blueprint\n This is selected by default. Cloudbreak validates the Ambari blueprint in this case.\n\n\nRelocate docker\n This is selected by default. In startup time the \n/var/lib/docker\n will be relocated to the temporarily attached SSD. (In this case please do not stop your machines on Azure UI because then your data will be lost)\n\n\nShipyard enabled cluster\n This is selected by default. Cloudbreak will start a \nShipyard\n container which helps you to manage your containers.\n\n\nPersistent Storage Name\n This is \ncbstore\n by default. Cloudbreak will copy the image into a storage which is not deleting under the termination. When you starting a new cluster then the provisioning will be much faster because of the existing image.\n\n\nAttached Storage Type\n This is \nsingle storage for all vm\n by default. If are you using the default option then your whole cluster will by in one storage which could be a bottleneck in case of \nAzure\n. If you are using the \nseparated storage for every vm\n then we will deploy as much storage account as many node you have and in this case IOPS limit concern just for one node.\n\n\nConfig recommendation strategy\n Strategy for configuration recommendations how will be applied. Recommended \nconfigurations gathered by the response of the stack advisor. \n\n\n\n\nNEVER_APPLY\n               Configuration recommendations are ignored with this option.\n\n\nONLY_STACK_DEFAULTS_APPLY\n Applies only on the default configurations for all included services.\n\n\nALWAYS_APPLY\n              Applies on all configuration properties.\n\n\n\n\nStart LDAP and configure SSSD\n Enables the \nSystem Security Services Daemon\n configuration.\n\n\nCluster termination\n\n\nYou can terminate running or stopped clusters with the \nterminate\n button in the cluster details.\n\n\n\n\nIMPORTANT\n Always use Cloudbreak to terminate the cluster. If that fails for some reason, try to delete the \nAzure resource group first. Instances are started in an Auto Scaling Group so they may be restarted if you terminate an \ninstance manually!\n\n\n\n\nSometimes Cloudbreak cannot synchronize it's state with the cluster state at the cloud provider and the cluster can't\n be terminated. In this case the \nForced termination\n option can help to terminate the cluster at the Cloudbreak \n side. \nIf it has happened:\n\n\n\n\nYou should check the related resources at the Azure Portal\n\n\nIf it is needed you need to manually remove resources from there\n\n\n\n\n\n\nFull size \nhere\n.\n\n\nInteractive mode / Cloudbreak Shell\n\n\nThe goal with the Cloudbreak Shell (Cloudbreak CLI) was to provide an interactive command line tool which supports:\n\n\n\n\nall functionality available through the REST API or Cloudbreak Web UI\n\n\nmakes possible complete automation of management task via scripts\n\n\ncontext aware command availability\n\n\ntab completion\n\n\nrequired/optional parameter support\n\n\nhint command to guide you on the usual path\n\n\n\n\nStart Cloudbreak Shell\n\n\nTo start the Cloudbreak CLI use the following commands:\n\n\n\n\nOpen your \ncloudbreak-deployment\n directory if it is needed. For example:\n\n\n\n\n   cd cloudbreak-deployment\n\n\n\n\n\n\nStart the \ncbd\n from here if it is needed\n\n\n\n\n   cbd start\n\n\n\n\n\n\nIn the root of your \ncloudbreak-deployment\n folder apply:\n\n\n\n\n   cbd util cloudbreak-shell\n\n\n\n\n\n\nAt the very first time it will take for a while, because of need to download all the necessary docker images.\n\n\n\n\nThis will launch the Cloudbreak shell inside a Docker container then it is ready to use.\n\n\n\nFull size \nhere\n.\n\n\n\n\nIMPORTANT You have to copy all your files into the \ncbd\n working directory, what you would like to use in shell.\n For \nexample if your \ncbd\n working directory is \n~/cloudbreak-deployment\n then copy your \nblueprint JSON, public ssh key \nfile...etc.\n to here. You can refer to these files with their names from the shell.\n\n\n\n\nAutocomplete and hints\n\n\nCloudbreak Shell helps to you with \nhint messages\n from the very beginning, for example:\n\n\ncloudbreak-shell\nhint\nHint: Add a blueprint with the 'blueprint add' command or select an existing one with 'blueprint select'\ncloudbreak-shell\n\n\n\n\n\nBeyond this you can use the \nautocompletion (double-TAB)\n as well:\n\n\ncloudbreak-shell\ncredential create --\ncredential create --AWS          credential create --AZURE        credential create --EC2          credential create --GCP          credential create --OPENSTACK\n\n\n\n\nProvisioning via CLI\n\n\nSetting up Azure credential\n\n\nCloudbreak works by connecting your Azure account through so called Credentials, and then uses these credentials to \ncreate resources on your behalf. Credentials can be configured with the following command for example:\n\n\ncredential create --AZURE --name my-azure-credential --description \nsample credential\n --subscriptionId \nyour-azure-subscription-id --tenantId your-azure-application-tenant-id --appId \nyour-azure-application-id --password YourApplicationPassword --sshKeyString \nssh-rsa AAAAB3***etc.\n\n\n\n\n\n\n\nCloudbreak is supporting simple rsa public key instead of X509 certificate file after 1.0.4 version\n\n\nNOTE\n that Cloudbreak \ndoes not set your cloud user details\n - we work around the concept of Access Control \nService (ACS). You should have already a valid Azure Subscription and Application. You can find further details \nhere\n.\n\n\n\n\nAlternatives to provide \nSSH Key\n:\n\n\n\n\nyou can upload your public key from an url: \n\u2014sshKeyUrl\n \n\n\nor you can add the path of your public key: \n\u2014sshKeyPath\n\n\n\n\nYou can check whether the credential was created successfully\n\n\ncredential list\n\n\n\n\nYou can switch between your existing credentials\n\n\ncredential select --name my-azure-credential\n\n\n\n\nInfrastructure templates\n\n\nAfter your Azure account is linked to Cloudbreak you can start creating resource templates that describe your clusters' \ninfrastructure:\n\n\n\n\nsecurity groups\n\n\nnetworks\n\n\ntemplates\n\n\n\n\nWhen you create one of the above resource, \nCloudbreak does not make any requests to Azure. Resources are only created\n on Azure after the \ncluster create\n has applied.\n These templates are saved to Cloudbreak's database and can be \n reused with multiple clusters to describe the infrastructure.\n\n\nTemplates\n\n\nTemplates describe the \ninstances of your cluster\n - the instance type and the attached volumes. A typical setup is\n to combine multiple templates in a cluster for the different types of nodes. For example you may want to attach multiple\n large disks to the datanodes or have memory optimized instances for Spark nodes.\n\n\nA template can be used repeatedly to create identical copies of the same stack (or to use as a foundation to start a \nnew stack). Templates can be configured with the following command for example:\n\n\ntemplate create --AZURE --name my-azure-template --description \nsample description\n --instanceType Standard_D4 --volumeSize 100 --volumeCount 2 --volumeType Standard_LRS\n\n\n\n\nThe \nVolume Type\n describes the \nStorage Account type\n which will be used for the attached disks. The only constraint is that the \nPremium storage\n can only be used\nfor \nDS\n instance types. For more details about the premium storage read \nthis\n.\n\n\nOther available option here is \n--publicInAccount\n. If it is true, all the users belonging to your account will be able\n to use this template to create clusters, but cannot delete it.\n\n\nYou can check whether the template was created successfully\n\n\ntemplate list\n\n\n\n\nNetworks\n\n\nYour clusters can be created in their own \nnetworks\n or in one of your already existing one. If you choose an \nexisting network, it is possible to create a new subnet within the network. The subnet's IP range must be defined in \nthe \nSubnet (CIDR)\n field using the general CIDR notation.\n\n\nDefault AZURE Network\n\n\nIf you don't want to create or use your custom network, you can use the \ndefault-azure-network\n for all your \nCloudbreak clusters. It will create a new network with a \n10.0.0.0/16\n subnet and \n10.0.0.0/8\n address prefix every \ntime a cluster is created.\n\n\nCustom AZURE Network\n\n\nIf you'd like to deploy a cluster to a custom network you'll have to apply the following command:\n\n\nnetwork create --AZURE --name my-azure-network --addressPrefix 192.168.123.123 --subnet 10.0.0.0/16\n\n\n\n\n\n\nIMPORTANT\n Please make sure the defined subnet and theirs address prefixes here doesn't overlap with any of your \nalready deployed subnet and its already used address prefix in the network, because of the validation only happens \nafter the cluster creation \nstarts.\n\n\nIn case of existing subnet make sure you have enough room within your network space for the new instances. The \nprovided subnet CIDR will be ignored, but a proper CIDR range will be used.\n\n\n\n\nYou can check whether the network was created successfully\n\n\nnetwork list\n\n\n\n\n--addressPrefix\n This list will be appended to the current list of address prefixes.\n\n\n\n\nThe address prefixes in this list should not overlap between them.\n\n\nThe address prefixes in this list should not overlap with existing address prefixes in the network.\n\n\n\n\nYou can find more details about the AZURE Address Prefixes \nhere\n.\n\n\nIf \n--publicInAccount\n is true, all the users belonging to your account will be able to use this network template \nto create clusters, but cannot delete it.\n\n\n\n\nNOTE\n The new networks are created on AZURE only after the the cluster provisioning starts with the selected \nnetwork template.\n\n\n\n\nDefining cluster services\n\n\nBlueprints\n\n\nBlueprints are your declarative definition of a Hadoop cluster. These are the same blueprints that are \nused by Ambari\n.\n\n\nYou can use the 3 default blueprints pre-defined in Cloudbreak or you can create your own ones.\nBlueprints can be added from file or URL (an \nexample blueprint\n).\n\n\nThe host groups in the JSON will be mapped to a set of instances when starting the cluster. Besides this the services and\n components will also be installed on the corresponding nodes. Blueprints can be modified later from the Ambari UI.\n\n\n\n\nNOTE\n Not necessary to define all the configuration in the blueprint. If a configuration is missing, Ambari will fill that with a default value.\n\n\n\n\nblueprint add --name my-blueprint --description \nsample description\n --file \nthe path of the blueprint\n\n\n\n\n\nOther available options:\n\n\n--url\n the url of the blueprint\n\n\n--publicInAccount\n If it is true, all the users belonging to your account will be able to use this blueprint to create \nclusters, but cannot delete it.\n\n\nYou can check whether the blueprint was created successfully\n\n\nblueprint list\n\n\n\n\nA blueprint can be exported from a running Ambari cluster that can be reused in Cloudbreak with slight \nmodifications.\n\nThere is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the \nmodifications have to be done manually.\nWhen the blueprint is exported some configurations are hardcoded for example domain names, memory configurations..etc. that won't be applicable to the Cloudbreak cluster.\n\n\nCluster customization\n\n\nSometimes it can be useful to \ndefine some custom scripts so called Recipes in Cloudbreak\n that run during cluster \ncreation and add some additional functionality.\n\n\nFor example it can be a service you'd like to install but it's not supported by Ambari or some script that \nautomatically downloads some data to the necessary nodes.\nThe most \nnotable example is Ranger setup\n:\n\n\n\n\nIt has a prerequisite of a running database when Ranger Admin is installing.\n\n\nA PostgreSQL database can be easily started and configured with a recipe before the blueprint installation starts.\n\n\n\n\nTo learn more about these and check the Ranger recipe out, take a look at the \nCluster customization\n\n\nMetadata show\n\n\nYou can check the stack metadata with\n\n\nstack metadata --name myawsstack --instancegroup master\n\n\n\n\nOther available options:\n\n\n--id\n In this case you can select a stack with id.\n\n\n--outputType\n In this case you can modify the outputformat of the command (RAW or JSON). \n\n\nCluster deployment\n\n\nAfter all the cluster resources are configured you can deploy a new HDP cluster. The following sub-sections show \nyou a \nbasic flow for cluster creation with Cloudbreak Shell\n.\n\n\nSelect credential\n\n\nSelect one of your previously created Azure credential:\n\n\ncredential select --name my-azure-credential\n\n\n\n\nSelect blueprint\n\n\nSelect one of your previously created blueprint which fits your needs:\n\n\nblueprint select --name multi-node-hdfs-yarn\n\n\n\n\nConfigure instance groups\n\n\nYou must configure instance groups before provisioning. An instance group define a group of nodes with a specified \ntemplate. Usually we create instance groups for host groups in the blueprint.\n\n\ninstancegroup configure --instanceGroup cbgateway --nodecount 1 --templateName minviable-azure\ninstancegroup configure --instanceGroup master --nodecount 1 --templateName minviable-azure\ninstancegroup configure --instanceGroup slave_1 --nodecount 1 --templateName minviable-azure\n\n\n\n\nOther available option:\n\n\n--templateId\n Id of the template\n\n\nSelect network\n\n\nSelect one of your previously created network which fits your needs or a default one:\n\n\nnetwork select --name default-azure-network\n\n\n\n\nSelect security group\n\n\nSelect one of your previously created security which fits your needs or a default one:\n\n\nsecuritygroup select --name all-services-port\n\n\n\n\nCreate stack / Create cloud infrastructure\n\n\nStack means the running cloud infrastructure that is created based on the instance groups configured earlier \n(\ncredential\n, \ninstancegroups\n, \nnetwork\n, \nsecuritygroup\n). Same as in case of the API or UI the new cluster will \nuse your templates and by using Azure ARM will launch your cloud stack. Use the following command to create a \nstack to be used with your Hadoop cluster:\n\n\nstack create --name myazurestack --region \nNorth Europe\n\n\n\n\n\nThe infrastructure is created asynchronously, the state of the stack can be checked with the stack \nshow command\n. If \nit reports AVAILABLE, it means that the virtual machines and the corresponding infrastructure is running at the cloud provider.\n\n\nOther available option is:\n\n\n--wait\n - in this case the create command will return only after the process has finished.\n\n\n--relocateDocker\n - This is true by default. In startup time the \n/var/lib/docker\n will be relocated to the temporarily attached SSD. (In this case please do not stop your machines on Azure UI because then your data will be lost)\n\n\n--persistentStorage\n - This is \ncbstore\n by default. Cloudbreak will copy the image into a storage which is not deleting under the termination. When you starting a new cluster then the provisioning will be much faster because of the existing image.\n\n\n--attachedStorageType\n - This is \nSINGLE\n by default. If you are using the default option then your whole cluster will by in one storage which could be a bottleneck in case of \nAzure\n. If you are using the \nPER_VM\n then we will deploy as much storage account as many node you have and in this case IOPS limit concern just for one node.\n\n\nCreate a Hadoop cluster / Cloud provisioning\n\n\nYou are almost done! One more command and your Hadoop cluster is starting!\n Cloud provisioning is done once the \ncluster is up and running. The new cluster will use your selected blueprint and install your custom Hadoop cluster \nwith the selected components and services.\n\n\ncluster create --description \nmy first cluster\n\n\n\n\n\nOther available option is \n--wait\n - in this case the create command will return only after the process has finished. \n\n\nYou are done!\n You have several opportunities to check the progress during the infrastructure creation then \nprovisioning:\n\n\n\n\nCloudbreak uses \nARM\n to create the resources - you can check out the resources created by Cloudbreak on\n the Azure Portal Resource groups page.\n\n\n\n\n\n\nFull size \nhere\n.\n\n\n\n\nIf stack then cluster creation have successfully done, you can check the Ambari Web UI. However you need to know the \nAmbari IP (for example: \nhttp://23.101.60.49:8080\n): \n\n\nYou can get the IP from the CLI as a result (\nambariServerIp 23.101.60.49\n) of the following command:\n\n\n\n\n\n\n\n\n         cluster show\n\n\n\n\n\n\nFull size \nhere\n.\n\n\n\n\nBesides these you can check the entire progress and the Ambari IP as well on the Cloudbreak Web UI itself. Open the \nnew cluster's \ndetails\n and its \nEvent History\n here.\n\n\n\n\n\n\nFull size \nhere\n.\n\n\nStop cluster\n\n\nYou have the ability to \nstop your existing stack then its cluster\n if you want to suspend the work on it.\n\n\nSelect a stack for example with its name:\n\n\nstack select --name my-stack\n\n\n\n\nOther available option to define a stack is its \n--id\n.\n\n\nEvery time you should stop the \ncluster\n first then the \nstack\n. So apply following commands to stop the previously \nselected stack:\n\n\ncluster stop\nstack stop\n\n\n\n\nRestart cluster\n\n\nSelect your stack that you would like to restart\n after this you can apply:\n\n\nstack start\n\n\n\n\nAfter the stack has successfully restarted, you can \nrestart the related cluster as well\n:\n\n\ncluster start\n\n\n\n\nUpscale cluster\n\n\nIf you need more instances to your infrastructure, you can \nupscale your selected stack\n:\n\n\nstack node --ADD --instanceGroup host_group_slave_1 --adjustment 6\n\n\n\n\nOther available option is \n--withClusterUpScale\n - this indicates also a cluster upscale after the stack upscale. You\n can upscale the related cluster separately if you want to do this:\n\n\ncluster node --ADD --hostgroup host_group_slave_1 --adjustment 6\n\n\n\n\nDownscale cluster\n\n\nYou also can reduce the number of instances in your infrastructure. \nAfter you selected your stack\n:\n\n\ncluster node --REMOVE  --hostgroup host_group_slave_1 --adjustment -2\n\n\n\n\nOther available option is \n--withStackDownScale\n - this indicates also a stack downscale after the cluster downscale.\n You can downscale the related stack separately if you want to do this:\n\n\nstack node --REMOVE  --instanceGroup host_group_slave_1 --adjustment -2\n\n\n\n\nCluster termination\n\n\nYou can terminate running or stopped clusters with\n\n\nstack terminate --name myawsstack\n\n\n\n\nOther available option is \n--wait\n - in this case the terminate command will return only after the process has finished. \n\n\n\n\nIMPORTANT\n Always use Cloudbreak to terminate the cluster. If that fails for some reason, try to delete the \nCloudFormation stack first. Instances are started in an Auto Scaling Group so they may be restarted if you terminate an instance manually!\n\n\n\n\nSometimes Cloudbreak cannot synchronize it's state with the cluster state at the cloud provider and the cluster can't\n be terminated. In this case the \nForced termination\n option on the Cloudbreak Web UI can help to terminate the cluster\n  at the Cloudbreak side. \nIf it has happened:\n\n\n\n\nYou should check the related resources at the AWS CloudFormation\n\n\nIf it is needed you need to manually remove resources from ther\n\n\n\n\nSilent mode\n\n\nWith Cloudbreak Shell you can execute script files as well. A script file contains shell commands and can \nbe executed with the \nscript\n cloudbreak shell command\n\n\nscript \nyour script file\n\n\n\n\n\nor with the \ncbd util cloudbreak-shell-quiet\n command\n\n\ncbd util cloudbreak-shell-quiet \n example.sh\n\n\n\n\n\n\nIMPORTANT\n You have to copy all your files into the \ncbd\n working directory, what you would like to use in shell.\n For example if your \ncbd\n working directory is ~/cloudbreak-deployment then copy your script file to here.\n\n\n\n\nExample\n\n\nThe following example creates a hadoop cluster with \nhdp-small-default\n blueprint on Standard_D3 instances with \n2X100G attached disks on \ndefault-azure-network\n network using \nall-services-port\n security group. You should copy \nyour ssh public key file into your \ncbd\n working directory with name \nid_rsa.pub\n and paste your Azure credentials in \nthe parts with \n...\n highlight.\n\n\ncredential create --AZURE --description \ncredential description\n --name myazurecredential --subscriptionId \nyour Azure subscription id\n --appId \nyour Azure application id\n --tenantId \nyour tenant id\n --password \nyour Azure application password\n --sshKeyPath id_rsa.pub\ncredential select --name myazurecredential\ntemplate create --AZURE --name azuretemplate --description azure-template --instanceType Standard_D3 --volumeSize 100 \n--volumeCount 2\nblueprint select --name hdp-small-default\ninstancegroup configure --instanceGroup cbgateway --nodecount 1 --templateName azuretemplate\ninstancegroup configure --instanceGroup host_group_master_1 --nodecount 1 --templateName azuretemplate\ninstancegroup configure --instanceGroup host_group_master_2 --nodecount 1 --templateName azuretemplate\ninstancegroup configure --instanceGroup host_group_master_3 --nodecount 1 --templateName azuretemplate\ninstancegroup configure --instanceGroup host_group_client_1  --nodecount 1 --templateName azuretemplate\ninstancegroup configure --instanceGroup host_group_slave_1 --nodecount 3 --templateName azuretemplate\nnetwork select --name default-azure-network\nsecuritygroup select --name all-services-port\nstack create --name my-first-stack --region \nWest US\n\ncluster create --description \nMy first cluster\n\n\n\n\n\nCongratulations!\n Your cluster should now be up and running on this way as well. To learn more about Cloudbreak and \nprovisioning, we have some \ninteresting insights\n for you", 
            "title": "Azure"
        }, 
        {
            "location": "/azure/#azure-setup", 
            "text": "On other cloud providers, we provide \u201cpublic images\u201d that are pre-built with the Cloudbreak Deployer. But on Azure, its a different process.\nWe provide a way to launch Cloudbreak Deployer based on the new  Azure Resource Manager \nTemplates .", 
            "title": "Azure Setup"
        }, 
        {
            "location": "/azure/#deploy-using-the-azure-portal", 
            "text": "To get started using the Azure Resource Manager template to install Cloudbreak, it is as simple as clicking here:         Minimum and Recommended VM requirements :  8GB RAM, 10GB disk, 2 cores (The minimum instance type which is fit for cloudbreak is  D2 )   The following parameters are mandatory (beyond to the default values) for the new  cbd  Template!  On the  Custom deployment  panel:   Please create a new  Resource group  Select an appropriate  Resource group location   On the  Parameters  panel:   Select the same  LOCATION  as for the resource group  PASSWORD  must be between 6-72 characters long and must satisfy \n  at least 3 of password complexity requirements from the following:  Contains an uppercase character  Contains a lowercase character  Contains a numeric digit  Contains a special character     Finally  you should review the  Legal terms  from the  Custom deployment  panel:   If you agree with the terms and conditions, just click on  Create  \nbutton of this panel  Also click on the  Create  button on the  Custom deployment      Deployment takes about  15-20 minutes . You can track the \nprogress on the resource group details. If any issue has occurred, open the  Audit logs  from the settings. \nWe have faced an interesting behaviour on the Azure Portal:  All operations were successful on template deployment, \nbut overall fail .    Once it's successful done, you can reach the Cloudbreak UI \nat: http:// VM Public IP :3000/  email: admin@example.com  password: cloudbreak", 
            "title": "Deploy using the Azure Portal"
        }, 
        {
            "location": "/azure/#under-the-hood", 
            "text": "Meanwhile Azure is creating the deployment, here is some information about what happens in the background:   Start an instance from the official CentOS image  So no custom image copy is needed, which would take about 30 \n   minutes    Use  Docker VM Extension  to install Docker  Use  CustomScript Extension  to install \nCloudbreak Deployer ( cbd )   Cloudbreak Deployer Highlights   The default SSH username for the Azure VMs is  cloudbreak .  Cloudbreak Deployer location is  /var/lib/cloudbreak-deployment  on the launched  cbd  VM. This is the \n       cbd  root folder there.  All  cbd  actions must be executed from the  cbd  root folder.  Most of the  cbd  commands require  root  permissions. So it would be worth if you apply the  sudo su .", 
            "title": "Under the hood"
        }, 
        {
            "location": "/azure/#validate-the-started-cloudbreak-deployer", 
            "text": "SSH to the launched Azure VM.    Most of the  cbd  commands require  root  permissions. So it would be worth if you apply the:      sudo su   This is a MUST on Azure because the  Customscript Extension  which basically creates everything running as sudo and this is not modifiable.    Open the  cloudbreak-deployment  directory:     cd /var/lib/cloudbreak-deployment   Pre-installed Cloudbreak Deployer version and health:     cbd doctor   In case of  cbd update  is needed, please check the related documentation for  Cloudbreak Deployer Update . Most of the  cbd  commands require  root  permissions.    Started Cloudbreak Application logs:      cbd logs cloudbreak   Cloudbreak should start within a minute - you should see a line like this:  Started CloudbreakApplication in 36.823 seconds", 
            "title": "Validate the started Cloudbreak Deployer"
        }, 
        {
            "location": "/azure/#provisioning-prerequisites", 
            "text": "We use the new  Azure ARM  in \norder to launch clusters. In order to work we need to create an  Active Directory  application with the configured name and password and adds the permissions that are needed to call the Azure Resource Manager API. Cloudbreak Deployer automates all this for you.   If you forget to configure these steps you will not able to create any resource with Cloudbreak", 
            "title": "Provisioning Prerequisites"
        }, 
        {
            "location": "/azure/#azure-access-setup", 
            "text": "If you do not have an  Active Directory (AD)  user then you have to configure it before deploying a cluster with \nCloudbreak:   Why you need this? Read more  here    Go to  manage.windowsazure.com     Active Directory  Select one of your AD where you would like to create the new user  You can configure your AD users on  Your active directory     Users  menu    Full size  here .   Here you can add the new user to AD. Simply click on  Add User  in the bottom of the page  TYPE OF USER : select  New user in your organization  USER NAME : type the new user name into the box  Fill out the name fields for the new user on the second page of the ADD USER window  Submit the new user creation on the third window with the big green button  Copy the password  Folo4965  Click on the tick button in the bottom of the the ADD USER window    You will see the new user in the  USERS  list    You have got a temporary password so you have to change it before you start using the new user.    You need to add your AD user to the  manage.windowsazure.com     Settings     Administrators    Full size  here .   Here you can add the new user to Administrators. Simply click on  Add  in the bottom of the page  EMAIL ADDRESS : copy the previously created user email address here  Select the appropriate  SUBSCRIPTION  for the user  Click on the tick button in the bottom of the the ADD A CO-ADMINISTRATOR window    You will see the new co-administrator a in the  ADMINISTRATORS  list", 
            "title": "Azure access setup"
        }, 
        {
            "location": "/azure/#azure-application-setup-with-cloudbreak-deployer", 
            "text": "In order for Cloudbreak to be able to launch clusters on Azure on your behalf you need to set up your  Azure ARM \napplication . If you do not want to create your ARM application via the Azure Web UI,  we automated the related Azure \nconfigurations in the Cloudbreak Deployer .  If you use our  Azure Template for Cloudbreak Deployer , you should:   SSH to the Cloudbreak Deployer Virtual machine  cbd  location is  /var/lib/cloudbreak-deployment  all  cbd  actions must be executed from the  cbd  folder    Most of the  cbd  commands require  root  permissions.  So  sudo su  here would be worth for you.   You can setup your Azure Application with the following  cbd  command:   Why you need this? Read more  here   cbd azure configure-arm --app_name myapp --app_password password123 --subscription_id 1234-abcd-efgh-1234  Other available options:  --app_name  your new application name,  app  by default  --app_password  your application password,  password  by default  --subscription_id  your Azure subscription ID  --username  your Azure username  --password  your Azure password  The command applies the following steps:   It creates an Active Directory application with the configured name, password  It grants permissions to call the Azure Resource Manager API   Please use the output of the command when you creating your Azure credential in Cloudbreak.  The major part of \nthe output should be like this example:  Subscription ID: sdf324-26b3-sdf234-ad10-234dfsdfsd\nApp ID: 234sdf-c469-sdf234-9062-dsf324\nPassword: password123\nApp Owner Tenant ID: sdwerwe1-d98e-dsf12-dsf123-df123232", 
            "title": "Azure application setup with Cloudbreak Deployer"
        }, 
        {
            "location": "/azure/#file-system-configuration", 
            "text": "When starting a cluster with Cloudbreak on Azure, the default file system is \u201cWindows Azure Blob Storage\u201d. Hadoop has \nbuilt-in support for the  WASB file system  so it can be\nused easily as HDFS.", 
            "title": "File system configuration"
        }, 
        {
            "location": "/azure/#disks-and-blob-storage", 
            "text": "In Azure every data disk attached to a virtual machine  is stored  as a virtual hard disk (VHD) in a page blob inside an Azure storage account. Because these are not local disks and the operations must be done on the VHD files it causes degraded performance when used as HDFS.\nWhen WASB is used as a Hadoop file system the files are full-value blobs in a storage account. It means better performance compared to the data disks and the WASB file system can be configured very easily but Azure storage accounts have their own  limitations  as well. There is a space limitation for TB per storage account (500 TB) as well but the real bottleneck is the total request rate that is only 20000 IOPS where Azure will start to throw errors when trying to do an I/O operation.\nTo bypass those limits Microsoft created a small service called  DASH . DASH itself is a service that imitates the API of the Azure Blob Storage API and it can be deployed as a Microsoft Azure Cloud Service. Because its API is the same as the standard blob storage API it can be used  almost  in the same way as the default WASB file system from a Hadoop deployment.\nDASH works by sharding the storage access across multiple storage accounts. It can be configured to distribute storage account load to at most 15  scaleout  storage accounts. It needs one more  namespace  storage account where it keeps track of where the data is stored.\nWhen configuring a WASB file system with Hadoop, the only required config entries are the ones where the access details are described. To access a storage account Azure generates an access key that is displayed on the Azure portal or can be queried through the API while the account name is the name of the storage account itself. A DASH service has a similar account name and key, those can be configured in the configuration file while deploying the cloud service.", 
            "title": "Disks and blob storage"
        }, 
        {
            "location": "/azure/#deploying-a-dash-service-with-cloudbreak-deployer", 
            "text": "We automated the deployment of DASH service in Cloudbreak Deployer. After  cbd  is installed, simply run the \nfollowing command to deploy a DASH cloud service with 5 scale out storage accounts:  cbd azure deploy-dash --accounts 5 --prefix dash --location  West Europe  --instances 3  The command applies the following steps:   It creates the namespace account and the scale out storage accounts  It builds the  .cscfg  configuration file based on the created storage account names and keys  It generates an Account Name and an Account Key for the DASH service  Finally it deploys the cloud service package file to a new cloud service   The WASB file system configured with DASH can be used as a data lake - when multiple clusters are deployed with the \nsame DASH file system configuration the same data can be accessed from all the clusters, but every cluster can have a \ndifferent service configured as well. In that case deploy as many DASH services with  cbd  as clusters with \nCloudbreak and configure them accordingly.", 
            "title": "Deploying a DASH service with Cloudbreak Deployer"
        }, 
        {
            "location": "/azure/#containers-within-the-storage-account", 
            "text": "Cloudbreak creates a new container in the configured storage account for each cluster with the following name \npattern  cloudbreak-UNIQUE_ID . Re-using existing containers in the same account is not supported as dirty data can \nlead to failing cluster installations. In order to take advantage of the WASB file system your data does not have to \nbe in the same storage account nor in the same container. You can add as many accounts as you wish through Ambari, by\n setting the properties described  here . Once you \n added the appropriate properties you can use those storage accounts with the pre-existing data, like:  hadoop fs -ls wasb://data@youraccount.blob.core.windows.net/terasort-input/   IMPORTANT  Make sure that your cloud account can launch instances using the new Azure ARM (a.k.a. V2) API and \nyou have sufficient qouta (CPU, network, etc) for the requested cluster size.", 
            "title": "Containers within the storage account"
        }, 
        {
            "location": "/azure/#generate-a-new-ssh-key", 
            "text": "All the instances created by Cloudbreak are configured to allow key-based SSH,\nso you'll need to provide an SSH public key that can be used later to SSH onto the instances in the clusters you'll create with Cloudbreak.\nYou can use one of your existing keys or you can generate a new one.  To generate a new SSH keypair:  ssh-keygen -t rsa -b 4096 -C  your_email@example.com \n# Creates a new ssh key, using the provided email as a label\n# Generating public/private rsa key pair.  # Enter file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter]\nYou'll be asked to enter a passphrase, but you can leave it empty.\n\n# Enter passphrase (empty for no passphrase): [Type a passphrase]\n# Enter same passphrase again: [Type passphrase again]  After you enter a passphrase the keypair is generated. The output should look something like below.  # Your identification has been saved in /Users/you/.ssh/id_rsa.\n# Your public key has been saved in /Users/you/.ssh/id_rsa.pub.\n# The key fingerprint is:\n# 01:0f:f4:3b:ca:85:sd:17:sd:7d:sd:68:9d:sd:a2:sd your_email@example.com  Later you'll need to pass the  .pub  file's contents to Cloudbreak and use the private part to SSH to the instances", 
            "title": "Generate a new SSH key"
        }, 
        {
            "location": "/azure/#provisioning-via-browser", 
            "text": "You can log into the Cloudbreak application at  http:// Public_IP :3000/ .  The main goal of the Cloudbreak UI is to easily create clusters on your own cloud provider account.\nThis description details the AZURE setup - if you'd like to use a different cloud provider check out its manual.  This document explains the four steps that need to be followed to create Cloudbreak clusters from the UI:   connect your AZURE account with Cloudbreak  create some template resources on the UI that describe the infrastructure of your clusters  create a blueprint that describes the HDP services in your clusters and add some recipes for customization  launch the cluster itself based on these template resource", 
            "title": "Provisioning via Browser"
        }, 
        {
            "location": "/azure/#setting-up-azure-credentials", 
            "text": "Cloudbreak works by connecting your AZURE account through so called  Credentials , and then uses these credentials to \ncreate resources on your behalf. The credentials can be configured on the  manage credentials  panel on the \nCloudbreak Dashboard.   Please read the  Provisioning prerequisites  where you \ncan find the steps how can get the mandatory  Subscription ID ,  App ID ,  Password  and  App Owner Tenant ID  for \nyour Cloudbreak credential.   To create a new AZURE credential follow these steps:   Fill out the new credential  Name  Only alphanumeric and lowercase characters (min 5, max 100 characters) can be applied    Copy your AZURE Subscription ID to the  Subscription Id  field    Full size  here .   Copy your AZURE Active Directory Application:  ID to the  App Id  field  password to the  Password  field  App Owner Tenant Id  field      Full size  here .   Copy your SSH public key to the  SSH public key  field  The SSH public key must be in OpenSSH format and it's private keypair can be used later to  SSH onto every \ninstance  of every cluster you'll create with this credential.  The  SSH username  for the AZURE instances is  cloudbreak .      Any other parameter is optional here.  Public in account  means that all the users belonging to your account will be able to use this credential to create \nclusters, but cannot delete it.  Cloudbreak is supporting simple rsa public key instead of X509 certificate file after 1.0.4 version    Full size  here .", 
            "title": "Setting up Azure credentials"
        }, 
        {
            "location": "/azure/#infrastructure-templates", 
            "text": "After your AZURE account is linked to Cloudbreak you can start creating resource templates that describe your clusters' \ninfrastructure:   templates  networks  security groups   When you create one of the above resource,  Cloudbreak does not make any requests to AZURE. Resources are only created\n on AZURE after the  create cluster  button has pushed.  These templates are saved to Cloudbreak's database and can be \n reused with multiple clusters to describe the infrastructure  Templates  Templates describe the  instances of your cluster  - the instance type and the attached volumes. A typical setup is\n to combine multiple templates in a cluster for the different types of nodes. For example you may want to attach multiple\n large disks to the datanodes or have memory optimized instances for Spark nodes.  The instance templates can be configured on the  manage templates  panel on the Cloudbreak Dashboard.  The  Volume Type  describes the  Storage Account type  which will be used for the attached disks. The only constraint is that the  Premium storage  can only be used\nfor  DS  instance types. For more details about the premium storage read  this .  If  Public in account is checked all the users belonging to your account will be able to use this resource to create \nclusters, but cannot delete it  Networks  Your clusters can be created in their own  networks  or in one of your already existing one. The subnet's IP range must be defined in \nthe  Subnet (CIDR)  field using the general CIDR notation.  Default AZURE Network  If you don't want to create or use your custom network, you can use the  default-azure-network  for all your \nCloudbreak clusters. It will create a new network with a  10.0.0.0/16  subnet every time a cluster is created.  Custom AZURE Network  If you'd like to deploy a cluster to a custom network you'll have to  create a new network  template on the  manage \nnetworks  panel.  You have the following options:   Create a new virtual network and a new subnet :  Every time a cluster is created with this kind of network setup a new virtual network and a new subnet with the specified IP range will be created for the instances on Azure.  Use an existing subnet in an existing virtual network : Use this kind of network setup if you have an existing virtual network with one or more subnets on Azure and you'd like to start the instances of a cluster in one of those subnets. In this case you can define the  Subnet Identifier  and the  Virtual Network Identifier  and the  Resource Group Identifier  of your network. The  Resource Group Identifier  identifies the resource group which contains your existing virtual network. The  Virtual Network Identifier  and the  Subnet Identifier  will tell Cloudbreak which network and subnet to use to launch the new instances.    IMPORTANT  In case of existing subnet make sure you have enough room within your network space for the new instances. The \nprovided subnet CIDR will be ignored, but the existing subnet's CIDR range will be used. The security group behavior will be changed in this case as well\ndescribed in the security group section below.   If  Public in account  is checked all the users belonging to your account will be able to use this network template \nto create clusters, but cannot delete it.   NOTE  The new networks are created on AZURE only after the the cluster provisioning starts with the selected \nnetwork template.    Full size  here .  Security groups  Security group templates are very similar to the  security groups on Azure . They describe the allowed inbound traffic to the instances in the cluster. \nCurrently only one security group template can be selected for a Cloudbreak cluster and all the instances have a \npublic IP address so all the instances in the cluster will belong to the same security group.\nThis may change in a later release.  Default Security Group  You can also use the two pre-defined security groups in Cloudbreak.  only-ssh-and-ssl:  all ports are locked down except for SSH and gateway HTTPS (you can't access Hadoop services \noutside of the virtual network):   SSH (22)  HTTPS (443)   all-services-port:  all Hadoop services and SSH, gateway and HTTPS are accessible by default:   SSH (22)  HTTPS (443)  Ambari (8080)  Consul (8500)  NN (50070)  RM Web (8088)  Scheduler (8030RM)  IPC (8050RM)  Job history server (19888)  HBase master (60000)  HBase master web (60010)  HBase RS (16020)  HBase RS info (60030)  Falcon (15000)  Storm (8744)  Hive metastore (9083)  Hive server (10000)  Hive server HTTP (10001)  Accumulo master (9999)  Accumulo Tserver (9997)  Atlas (21000)  KNOX (8443)  Oozie (11000)  Spark HS (18080)  NM Web (8042)  Zeppelin WebSocket (9996)  Zeppelin UI (9995)  Kibana (3080)  Elasticsearch (9200)   Custom Security Group  You can define your own security group by adding all the ports, protocols and CIDR range you'd like to use. The rules\n defined here doesn't need to contain the internal rules, those are automatically added by Cloudbreak to the security\n  group on Azure.   IMPORTANT  443 and 22 ports needs to be there in every security group otherwise Cloudbreak won't be able to communicate with the \nprovisioned cluster   If  Public in account  is checked all the users belonging to your account will be able to use this security group \ntemplate to create clusters, but cannot delete it.   NOTE  The security groups are created on Azure only after the cluster provisioning starts with the selected \nsecurity group template.  IMPORTANT  If you use and existing virtual network and subnet the selected security group will only be applied to the  Cloudbreak gateway node  due to the lack of\ncapability to attach multiple security groups to an existing subnet. If you'd like to open ports for Hadoop you must do it on your existing security group.    Full size  here . /sub", 
            "title": "Infrastructure templates"
        }, 
        {
            "location": "/azure/#defining-cluster-services", 
            "text": "Blueprints  Blueprints are your declarative definition of a Hadoop cluster. These are the same blueprints that are  used by Ambari .  You can use the 3 default blueprints pre-defined in Cloudbreak or you can create your own ones.\nBlueprints can be added from file, URL (an  example blueprint ) or the \nwhole JSON can be written in the  JSON text  box.  The host groups in the JSON will be mapped to a set of instances when starting the cluster. Besides this the services and\n components will also be installed on the corresponding nodes. Blueprints can be modified later from the Ambari UI.   NOTE  Not necessary to define all the configuration in the blueprint. If a configuration is missing, Ambari will \nfill that with a default value.   If  Public in account  is checked all the users belonging to your account will be able to use this blueprint to \ncreate clusters, but cannot delete or modify it.   Full size  here .  A blueprint can be exported from a running Ambari cluster that can be reused in Cloudbreak with slight \nmodifications. \nThere is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the \nmodifications have to be done manually.\nWhen the blueprint is exported some configurations are hardcoded for example domain names, memory configurations...etc. that won't be applicable to the Cloudbreak cluster  Cluster customization  Sometimes it can be useful to  define some custom scripts so called Recipes in Cloudbreak  that run during cluster \ncreation and add some additional functionality.  For example it can be a service you'd like to install but it's not supported by Ambari or some script that \nautomatically downloads some data to the necessary nodes.\nThe most  notable example is Ranger setup :   It has a prerequisite of a running database when Ranger Admin is installing.  A PostgreSQL database can be easily started and configured with a recipe before the blueprint installation starts.   To learn more about these and check the Ranger recipe out, take a look at the  Cluster customization  Cluster customization  Sometimes it can be useful to  define some custom scripts so called Recipes in Cloudbreak  that run during cluster \ncreation and add some additional functionality.  For example it can be a service you'd like to install but it's not supported by Ambari or some script that \nautomatically downloads some data to the necessary nodes.\nThe most  notable example is Ranger setup :   It has a prerequisite of a running database when Ranger Admin is installing.  A PostgreSQL database can be easily started and configured with a recipe before the blueprint installation starts.   To learn more about these and check the Ranger recipe out, take a look at the  Cluster customization .", 
            "title": "Defining cluster services"
        }, 
        {
            "location": "/azure/#cluster-deployment", 
            "text": "After all the cluster resources are configured you can deploy a new HDP cluster.  Here is a  basic flow for cluster creation on Cloudbreak Web UI :   Start by selecting a previously created Azure credential in the header.    Full size  here .   Open  create cluster   Configure Cluster  tab   Fill out the new cluster  name  Cluster name must start with a lowercase alphabetic character then you can apply lowercase alphanumeric and \n   hyphens only (min 5, max 40 characters)    Select a  Region  where you like your cluster be provisioned  Click on the  Setup Network and Security  button  If  Public in account  is checked all the users belonging to your account will be able to see the created cluster on\n the UI, but cannot delete or modify it.     Setup Network and Security  tab   Select one of the networks  Select one of the security groups  Click on the  Choose Blueprint  button  If  Enable security  is checked as well, Cloudbreak will install Key Distribution Center (KDC) and the cluster will \nbe Kerberized. See more about it in the  Kerberos  section of this documentation.     Choose Blueprint  tab   Select one of the blueprint  After you've selected a  Blueprint , you should be able to configure:  the templates  the number of nodes for all of the host groups in the blueprint  the recipes for nodes    Click on the  Add File System  button   Add File System  tab   Select one of the file system that fits your needs  After you've selected  WASB  or  DASH , you should configure:  Storage Account Name  Storage Account Access Key    Click on the  Review and Launch  button  File system  is a mandatory configuration for Azure. You can read more about WASB and DASH in the  File System Configuration section .     Review and Launch  tab   After the  create and start cluster  button has clicked Cloudbreak will start to create the cluster's resources on \n your Azure account.   Cloudbreak uses  Azure Resource Manager  to create the resources - you can check out the resources created by Cloudbreak\n on \nthe  Azure Portal Resource groups  page.  Full size  here .  Besides these you can check the progress on the Cloudbreak Web UI itself if you open the new cluster's  Event History .  Full size  here .  Advanced options  There are some advanced features when deploying a new cluster, these are the following:  Minimum cluster size  The provisioning strategy in case of the cloud provider cannot allocate all the requested nodes.  Validate blueprint  This is selected by default. Cloudbreak validates the Ambari blueprint in this case.  Relocate docker  This is selected by default. In startup time the  /var/lib/docker  will be relocated to the temporarily attached SSD. (In this case please do not stop your machines on Azure UI because then your data will be lost)  Shipyard enabled cluster  This is selected by default. Cloudbreak will start a  Shipyard  container which helps you to manage your containers.  Persistent Storage Name  This is  cbstore  by default. Cloudbreak will copy the image into a storage which is not deleting under the termination. When you starting a new cluster then the provisioning will be much faster because of the existing image.  Attached Storage Type  This is  single storage for all vm  by default. If are you using the default option then your whole cluster will by in one storage which could be a bottleneck in case of  Azure . If you are using the  separated storage for every vm  then we will deploy as much storage account as many node you have and in this case IOPS limit concern just for one node.  Config recommendation strategy  Strategy for configuration recommendations how will be applied. Recommended \nconfigurations gathered by the response of the stack advisor.    NEVER_APPLY                Configuration recommendations are ignored with this option.  ONLY_STACK_DEFAULTS_APPLY  Applies only on the default configurations for all included services.  ALWAYS_APPLY               Applies on all configuration properties.   Start LDAP and configure SSSD  Enables the  System Security Services Daemon  configuration.", 
            "title": "Cluster deployment"
        }, 
        {
            "location": "/azure/#cluster-termination", 
            "text": "You can terminate running or stopped clusters with the  terminate  button in the cluster details.   IMPORTANT  Always use Cloudbreak to terminate the cluster. If that fails for some reason, try to delete the \nAzure resource group first. Instances are started in an Auto Scaling Group so they may be restarted if you terminate an \ninstance manually!   Sometimes Cloudbreak cannot synchronize it's state with the cluster state at the cloud provider and the cluster can't\n be terminated. In this case the  Forced termination  option can help to terminate the cluster at the Cloudbreak \n side.  If it has happened:   You should check the related resources at the Azure Portal  If it is needed you need to manually remove resources from there    Full size  here .", 
            "title": "Cluster termination"
        }, 
        {
            "location": "/azure/#interactive-mode-cloudbreak-shell", 
            "text": "The goal with the Cloudbreak Shell (Cloudbreak CLI) was to provide an interactive command line tool which supports:   all functionality available through the REST API or Cloudbreak Web UI  makes possible complete automation of management task via scripts  context aware command availability  tab completion  required/optional parameter support  hint command to guide you on the usual path", 
            "title": "Interactive mode / Cloudbreak Shell"
        }, 
        {
            "location": "/azure/#start-cloudbreak-shell", 
            "text": "To start the Cloudbreak CLI use the following commands:   Open your  cloudbreak-deployment  directory if it is needed. For example:      cd cloudbreak-deployment   Start the  cbd  from here if it is needed      cbd start   In the root of your  cloudbreak-deployment  folder apply:      cbd util cloudbreak-shell   At the very first time it will take for a while, because of need to download all the necessary docker images.   This will launch the Cloudbreak shell inside a Docker container then it is ready to use.  Full size  here .   IMPORTANT You have to copy all your files into the  cbd  working directory, what you would like to use in shell.  For \nexample if your  cbd  working directory is  ~/cloudbreak-deployment  then copy your  blueprint JSON, public ssh key \nfile...etc.  to here. You can refer to these files with their names from the shell.", 
            "title": "Start Cloudbreak Shell"
        }, 
        {
            "location": "/azure/#autocomplete-and-hints", 
            "text": "Cloudbreak Shell helps to you with  hint messages  from the very beginning, for example:  cloudbreak-shell hint\nHint: Add a blueprint with the 'blueprint add' command or select an existing one with 'blueprint select'\ncloudbreak-shell   Beyond this you can use the  autocompletion (double-TAB)  as well:  cloudbreak-shell credential create --\ncredential create --AWS          credential create --AZURE        credential create --EC2          credential create --GCP          credential create --OPENSTACK", 
            "title": "Autocomplete and hints"
        }, 
        {
            "location": "/azure/#provisioning-via-cli", 
            "text": "", 
            "title": "Provisioning via CLI"
        }, 
        {
            "location": "/azure/#setting-up-azure-credential", 
            "text": "Cloudbreak works by connecting your Azure account through so called Credentials, and then uses these credentials to \ncreate resources on your behalf. Credentials can be configured with the following command for example:  credential create --AZURE --name my-azure-credential --description  sample credential  --subscriptionId \nyour-azure-subscription-id --tenantId your-azure-application-tenant-id --appId \nyour-azure-application-id --password YourApplicationPassword --sshKeyString  ssh-rsa AAAAB3***etc.    Cloudbreak is supporting simple rsa public key instead of X509 certificate file after 1.0.4 version  NOTE  that Cloudbreak  does not set your cloud user details  - we work around the concept of Access Control \nService (ACS). You should have already a valid Azure Subscription and Application. You can find further details  here .   Alternatives to provide  SSH Key :   you can upload your public key from an url:  \u2014sshKeyUrl    or you can add the path of your public key:  \u2014sshKeyPath   You can check whether the credential was created successfully  credential list  You can switch between your existing credentials  credential select --name my-azure-credential", 
            "title": "Setting up Azure credential"
        }, 
        {
            "location": "/azure/#infrastructure-templates_1", 
            "text": "After your Azure account is linked to Cloudbreak you can start creating resource templates that describe your clusters' \ninfrastructure:   security groups  networks  templates   When you create one of the above resource,  Cloudbreak does not make any requests to Azure. Resources are only created\n on Azure after the  cluster create  has applied.  These templates are saved to Cloudbreak's database and can be \n reused with multiple clusters to describe the infrastructure.  Templates  Templates describe the  instances of your cluster  - the instance type and the attached volumes. A typical setup is\n to combine multiple templates in a cluster for the different types of nodes. For example you may want to attach multiple\n large disks to the datanodes or have memory optimized instances for Spark nodes.  A template can be used repeatedly to create identical copies of the same stack (or to use as a foundation to start a \nnew stack). Templates can be configured with the following command for example:  template create --AZURE --name my-azure-template --description  sample description  --instanceType Standard_D4 --volumeSize 100 --volumeCount 2 --volumeType Standard_LRS  The  Volume Type  describes the  Storage Account type  which will be used for the attached disks. The only constraint is that the  Premium storage  can only be used\nfor  DS  instance types. For more details about the premium storage read  this .  Other available option here is  --publicInAccount . If it is true, all the users belonging to your account will be able\n to use this template to create clusters, but cannot delete it.  You can check whether the template was created successfully  template list  Networks  Your clusters can be created in their own  networks  or in one of your already existing one. If you choose an \nexisting network, it is possible to create a new subnet within the network. The subnet's IP range must be defined in \nthe  Subnet (CIDR)  field using the general CIDR notation.  Default AZURE Network  If you don't want to create or use your custom network, you can use the  default-azure-network  for all your \nCloudbreak clusters. It will create a new network with a  10.0.0.0/16  subnet and  10.0.0.0/8  address prefix every \ntime a cluster is created.  Custom AZURE Network  If you'd like to deploy a cluster to a custom network you'll have to apply the following command:  network create --AZURE --name my-azure-network --addressPrefix 192.168.123.123 --subnet 10.0.0.0/16   IMPORTANT  Please make sure the defined subnet and theirs address prefixes here doesn't overlap with any of your \nalready deployed subnet and its already used address prefix in the network, because of the validation only happens \nafter the cluster creation \nstarts.  In case of existing subnet make sure you have enough room within your network space for the new instances. The \nprovided subnet CIDR will be ignored, but a proper CIDR range will be used.   You can check whether the network was created successfully  network list  --addressPrefix  This list will be appended to the current list of address prefixes.   The address prefixes in this list should not overlap between them.  The address prefixes in this list should not overlap with existing address prefixes in the network.   You can find more details about the AZURE Address Prefixes  here .  If  --publicInAccount  is true, all the users belonging to your account will be able to use this network template \nto create clusters, but cannot delete it.   NOTE  The new networks are created on AZURE only after the the cluster provisioning starts with the selected \nnetwork template.", 
            "title": "Infrastructure templates"
        }, 
        {
            "location": "/azure/#defining-cluster-services_1", 
            "text": "Blueprints  Blueprints are your declarative definition of a Hadoop cluster. These are the same blueprints that are  used by Ambari .  You can use the 3 default blueprints pre-defined in Cloudbreak or you can create your own ones.\nBlueprints can be added from file or URL (an  example blueprint ).  The host groups in the JSON will be mapped to a set of instances when starting the cluster. Besides this the services and\n components will also be installed on the corresponding nodes. Blueprints can be modified later from the Ambari UI.   NOTE  Not necessary to define all the configuration in the blueprint. If a configuration is missing, Ambari will fill that with a default value.   blueprint add --name my-blueprint --description  sample description  --file  the path of the blueprint   Other available options:  --url  the url of the blueprint  --publicInAccount  If it is true, all the users belonging to your account will be able to use this blueprint to create \nclusters, but cannot delete it.  You can check whether the blueprint was created successfully  blueprint list  A blueprint can be exported from a running Ambari cluster that can be reused in Cloudbreak with slight \nmodifications. \nThere is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the \nmodifications have to be done manually.\nWhen the blueprint is exported some configurations are hardcoded for example domain names, memory configurations..etc. that won't be applicable to the Cloudbreak cluster.  Cluster customization  Sometimes it can be useful to  define some custom scripts so called Recipes in Cloudbreak  that run during cluster \ncreation and add some additional functionality.  For example it can be a service you'd like to install but it's not supported by Ambari or some script that \nautomatically downloads some data to the necessary nodes.\nThe most  notable example is Ranger setup :   It has a prerequisite of a running database when Ranger Admin is installing.  A PostgreSQL database can be easily started and configured with a recipe before the blueprint installation starts.   To learn more about these and check the Ranger recipe out, take a look at the  Cluster customization", 
            "title": "Defining cluster services"
        }, 
        {
            "location": "/azure/#metadata-show", 
            "text": "You can check the stack metadata with  stack metadata --name myawsstack --instancegroup master  Other available options:  --id  In this case you can select a stack with id.  --outputType  In this case you can modify the outputformat of the command (RAW or JSON).", 
            "title": "Metadata show"
        }, 
        {
            "location": "/azure/#cluster-deployment_1", 
            "text": "After all the cluster resources are configured you can deploy a new HDP cluster. The following sub-sections show \nyou a  basic flow for cluster creation with Cloudbreak Shell .  Select credential  Select one of your previously created Azure credential:  credential select --name my-azure-credential  Select blueprint  Select one of your previously created blueprint which fits your needs:  blueprint select --name multi-node-hdfs-yarn  Configure instance groups  You must configure instance groups before provisioning. An instance group define a group of nodes with a specified \ntemplate. Usually we create instance groups for host groups in the blueprint.  instancegroup configure --instanceGroup cbgateway --nodecount 1 --templateName minviable-azure\ninstancegroup configure --instanceGroup master --nodecount 1 --templateName minviable-azure\ninstancegroup configure --instanceGroup slave_1 --nodecount 1 --templateName minviable-azure  Other available option:  --templateId  Id of the template  Select network  Select one of your previously created network which fits your needs or a default one:  network select --name default-azure-network  Select security group  Select one of your previously created security which fits your needs or a default one:  securitygroup select --name all-services-port  Create stack / Create cloud infrastructure  Stack means the running cloud infrastructure that is created based on the instance groups configured earlier \n( credential ,  instancegroups ,  network ,  securitygroup ). Same as in case of the API or UI the new cluster will \nuse your templates and by using Azure ARM will launch your cloud stack. Use the following command to create a \nstack to be used with your Hadoop cluster:  stack create --name myazurestack --region  North Europe   The infrastructure is created asynchronously, the state of the stack can be checked with the stack  show command . If \nit reports AVAILABLE, it means that the virtual machines and the corresponding infrastructure is running at the cloud provider.  Other available option is:  --wait  - in this case the create command will return only after the process has finished.  --relocateDocker  - This is true by default. In startup time the  /var/lib/docker  will be relocated to the temporarily attached SSD. (In this case please do not stop your machines on Azure UI because then your data will be lost)  --persistentStorage  - This is  cbstore  by default. Cloudbreak will copy the image into a storage which is not deleting under the termination. When you starting a new cluster then the provisioning will be much faster because of the existing image.  --attachedStorageType  - This is  SINGLE  by default. If you are using the default option then your whole cluster will by in one storage which could be a bottleneck in case of  Azure . If you are using the  PER_VM  then we will deploy as much storage account as many node you have and in this case IOPS limit concern just for one node.  Create a Hadoop cluster / Cloud provisioning  You are almost done! One more command and your Hadoop cluster is starting!  Cloud provisioning is done once the \ncluster is up and running. The new cluster will use your selected blueprint and install your custom Hadoop cluster \nwith the selected components and services.  cluster create --description  my first cluster   Other available option is  --wait  - in this case the create command will return only after the process has finished.   You are done!  You have several opportunities to check the progress during the infrastructure creation then \nprovisioning:   Cloudbreak uses  ARM  to create the resources - you can check out the resources created by Cloudbreak on\n the Azure Portal Resource groups page.    Full size  here .   If stack then cluster creation have successfully done, you can check the Ambari Web UI. However you need to know the \nAmbari IP (for example:  http://23.101.60.49:8080 ):   You can get the IP from the CLI as a result ( ambariServerIp 23.101.60.49 ) of the following command:              cluster show   Full size  here .   Besides these you can check the entire progress and the Ambari IP as well on the Cloudbreak Web UI itself. Open the \nnew cluster's  details  and its  Event History  here.    Full size  here .  Stop cluster  You have the ability to  stop your existing stack then its cluster  if you want to suspend the work on it.  Select a stack for example with its name:  stack select --name my-stack  Other available option to define a stack is its  --id .  Every time you should stop the  cluster  first then the  stack . So apply following commands to stop the previously \nselected stack:  cluster stop\nstack stop  Restart cluster  Select your stack that you would like to restart  after this you can apply:  stack start  After the stack has successfully restarted, you can  restart the related cluster as well :  cluster start  Upscale cluster  If you need more instances to your infrastructure, you can  upscale your selected stack :  stack node --ADD --instanceGroup host_group_slave_1 --adjustment 6  Other available option is  --withClusterUpScale  - this indicates also a cluster upscale after the stack upscale. You\n can upscale the related cluster separately if you want to do this:  cluster node --ADD --hostgroup host_group_slave_1 --adjustment 6  Downscale cluster  You also can reduce the number of instances in your infrastructure.  After you selected your stack :  cluster node --REMOVE  --hostgroup host_group_slave_1 --adjustment -2  Other available option is  --withStackDownScale  - this indicates also a stack downscale after the cluster downscale.\n You can downscale the related stack separately if you want to do this:  stack node --REMOVE  --instanceGroup host_group_slave_1 --adjustment -2", 
            "title": "Cluster deployment"
        }, 
        {
            "location": "/azure/#cluster-termination_1", 
            "text": "You can terminate running or stopped clusters with  stack terminate --name myawsstack  Other available option is  --wait  - in this case the terminate command will return only after the process has finished.    IMPORTANT  Always use Cloudbreak to terminate the cluster. If that fails for some reason, try to delete the \nCloudFormation stack first. Instances are started in an Auto Scaling Group so they may be restarted if you terminate an instance manually!   Sometimes Cloudbreak cannot synchronize it's state with the cluster state at the cloud provider and the cluster can't\n be terminated. In this case the  Forced termination  option on the Cloudbreak Web UI can help to terminate the cluster\n  at the Cloudbreak side.  If it has happened:   You should check the related resources at the AWS CloudFormation  If it is needed you need to manually remove resources from ther", 
            "title": "Cluster termination"
        }, 
        {
            "location": "/azure/#silent-mode", 
            "text": "With Cloudbreak Shell you can execute script files as well. A script file contains shell commands and can \nbe executed with the  script  cloudbreak shell command  script  your script file   or with the  cbd util cloudbreak-shell-quiet  command  cbd util cloudbreak-shell-quiet   example.sh   IMPORTANT  You have to copy all your files into the  cbd  working directory, what you would like to use in shell.\n For example if your  cbd  working directory is ~/cloudbreak-deployment then copy your script file to here.", 
            "title": "Silent mode"
        }, 
        {
            "location": "/azure/#example", 
            "text": "The following example creates a hadoop cluster with  hdp-small-default  blueprint on Standard_D3 instances with \n2X100G attached disks on  default-azure-network  network using  all-services-port  security group. You should copy \nyour ssh public key file into your  cbd  working directory with name  id_rsa.pub  and paste your Azure credentials in \nthe parts with  ...  highlight.  credential create --AZURE --description  credential description  --name myazurecredential --subscriptionId  your Azure subscription id  --appId  your Azure application id  --tenantId  your tenant id  --password  your Azure application password  --sshKeyPath id_rsa.pub\ncredential select --name myazurecredential\ntemplate create --AZURE --name azuretemplate --description azure-template --instanceType Standard_D3 --volumeSize 100 \n--volumeCount 2\nblueprint select --name hdp-small-default\ninstancegroup configure --instanceGroup cbgateway --nodecount 1 --templateName azuretemplate\ninstancegroup configure --instanceGroup host_group_master_1 --nodecount 1 --templateName azuretemplate\ninstancegroup configure --instanceGroup host_group_master_2 --nodecount 1 --templateName azuretemplate\ninstancegroup configure --instanceGroup host_group_master_3 --nodecount 1 --templateName azuretemplate\ninstancegroup configure --instanceGroup host_group_client_1  --nodecount 1 --templateName azuretemplate\ninstancegroup configure --instanceGroup host_group_slave_1 --nodecount 3 --templateName azuretemplate\nnetwork select --name default-azure-network\nsecuritygroup select --name all-services-port\nstack create --name my-first-stack --region  West US \ncluster create --description  My first cluster   Congratulations!  Your cluster should now be up and running on this way as well. To learn more about Cloudbreak and \nprovisioning, we have some  interesting insights  for you", 
            "title": "Example"
        }, 
        {
            "location": "/gcp/", 
            "text": "Google Cloud Images\n\n\nWe have pre-built Cloudbreak Deployer cloud image for Google Cloud Platform (GCP). You can launch the latest \nCloudbreak Deployer image at the \nGoogle Developers Console\n.\n\n\n\n\nAlternatively, instead of using the pre-built cloud images for GCP, you can install Cloudbreak Deployer on your own\n VM. See \ninstallation page\n for more information.\n\n\n\n\nPlease make sure you added the following ports to your firewall rules:\n\n\n\n\nSSH (22)\n\n\nCloudbreak API (8080)\n\n\nIdentity server (8089)\n\n\nCloudbreak GUI (3000)\n\n\nUser authentication (3001)\n\n\n\n\nCloudbreak Deployer GCP Image Details\n\n\nImport Cloudbreak Deployer Image\n\n\nYou can import the latest Cloudbreak Deployer image on the \nGoogle Developers Console\n with the help\n of the \nGoogle Cloud Shell\n.\n\n\nJust click on the \nActivate Google Cloud Shell\n icon in the top right corner of the page:\n\n\n\n\nFull size \nhere\n.\n\n\nImages are global resources, so you can use these across zones and projects.\n\n\n\n\nFull size \nhere\n.\n\n\nYou can \ncreate your own Cloudbreak Deployer (cbd) instance from the imported image\n on the Google Developers Console.\n\n\n\n\nMinimum and Recommended VM requirements\n:\n 4GB RAM, 10GB disk, 2 core\n\n\n\n\nGoogle Setup\n\n\nCloudbreak Deployer Highlights\n\n\n\n\nThe default SSH username for the GCP instances is \ncloudbreak\n.\n\n\nCloudbreak Deployer location is \n/var/lib/cloudbreak-deployment\n on the launched \ncbd\n VM. This is the \n      \ncbd\n root folder there.\n\n\nAll \ncbd\n actions must be executed from the \ncbd\n root folder.\n\n\n\n\nSetup Cloudbreak Deployer\n\n\nYou should already have the Cloudbreak Deployer either by \nusing the GCP Cloud Images\n or by \ninstalling the \nCloudbreak Deployer\n manually on your own VM. (The minimum instance type which is fit for cloudbreak is \nn1-standard-2\n)\n\n\nIf you have your own installed VM, you should check the \nInitialize your Profile\n \nsection here before starting the provisioning.\n\n\nYou have several opportunities to \nconnect to the previously created \ncbd\n VM\n.\n\n\n\n\nCloudbreak Deployer location is \n/var/lib/cloudbreak-deployment\n.\n\n\nAll \ncbd\n actions must be executed from the \ncbd\n folder.\n\n\n\n\nOpen the \ncloudbreak-deployment\n directory:\n\n\ncd /var/lib/cloudbreak-deployment\n\n\n\n\nThis is the directory of the configuration files and the supporting binaries for Cloudbreak Deployer.\n\n\nInitialize your Profile\n\n\nFirst initialize \ncbd\n by creating a \nProfile\n file:\n\n\ncbd init\n\n\n\n\nIt will create a \nProfile\n file in the current directory. Please open the \nProfile\n file then check the \nPUBLIC_IP\n. \nThis is mandatory, because of to can access the Cloudbreak UI (called Uluwatu). In some cases the \ncbd\n tool tries to \nguess it. If \ncbd\n cannot get the IP address during the initialization, please set the appropriate value.\n\n\nStart Cloudbreak Deployer\n\n\nTo start the Cloudbreak application use the following command.\nThis will start all the Docker containers and initialize the application.\n\n\ncbd start\n\n\n\n\n\n\nAt the very first time it will take for a while, because of need to download all the necessary docker images.\n\n\n\n\nThe \ncbd start\n command includes the \ncbd generate\n command which applies the following steps:\n\n\n\n\ncreates the \ndocker-compose.yml\n file that describes the configuration of all the Docker containers needed for the Cloudbreak deployment.\n\n\ncreates the \nuaa.yml\n file that holds the configuration of the identity server used to authenticate users to Cloudbreak.\n\n\n\n\nValidate the started Cloudbreak Deployer\n\n\nAfter the \ncbd start\n command finishes followings are worthy to check:\n\n\n\n\nPre-installed Cloudbreak Deployer version and health.\n\n\n\n\n   cbd doctor\n\n\n\n\n\n\nIn case of \ncbd update\n is needed, please check the related documentation for \nCloudbreak Deployer Update\n. Most of the \ncbd\n commands require \nroot\n permissions.\n\n\n\n\n\n\nStarted Cloudbreak Application logs.\n\n\n\n\n   cbd logs cloudbreak\n\n\n\n\n\n\nCloudbreak should start within a minute - you should see a line like this: `Started CloudbreakApplication in 36.823 seconds\n\n\n\n\nProvisioning Prerequisites\n\n\nCreating a Google Cloud Service Account\n\n\nFollow the \ninstructions\n in Google Cloud's documentation to create a \nService account\n and \nGenerate a new P12 key\n.\n\n\nMake sure that at API level (\nAPIs and auth\n menu) you have enabled:\n\n\n\n\nGoogle Compute Engine\n\n\nGoogle Compute Engine Instance Group Manager API\n\n\nGoogle Compute Engine Instance Groups API\n\n\nBigQuery API\n\n\nGoogle Cloud Deployment Manager API\n\n\nGoogle Cloud DNS API\n\n\nGoogle Cloud SQL\n\n\nGoogle Cloud Storage\n\n\nGoogle Cloud Storage JSON API\n\n\n\n\n\n\nIf you have enabled every API then you have to wait about \n10 minutes\n for the provider.\n\n\n\n\nWhen creating GCP credentials \nin Cloudbreak you will have to provide the email address of your \nService Account\n \n(from the Service accounts page of your Google Cloud Platform Permissions) and the \nProject ID\n (from the Dashboard \nof your Google Cloud Platform Home) where the service account is created.\n You'll also have to \nupload the \ngenerated P12 file and provide an OpenSSH formatted public key\n that will be used as an SSH key.\n\n\nGenerate a new SSH key\n\n\nAll the instances created by Cloudbreak are configured to allow key-based SSH,\nso you'll need to provide an SSH public key that can be used later to SSH onto the instances in the clusters you'll create with Cloudbreak.\nYou can use one of your existing keys or you can generate a new one.\n\n\nTo generate a new SSH keypair:\n\n\nssh-keygen -t rsa -b 4096 -C \nyour_email@example.com\n\n# Creates a new ssh key, using the provided email as a label\n# Generating public/private rsa key pair.\n\n\n\n\n# Enter file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter]\nYou'll be asked to enter a passphrase, but you can leave it empty.\n\n# Enter passphrase (empty for no passphrase): [Type a passphrase]\n# Enter same passphrase again: [Type passphrase again]\n\n\n\n\nAfter you enter a passphrase the keypair is generated. The output should look something like below.\n\n\n# Your identification has been saved in /Users/you/.ssh/id_rsa.\n# Your public key has been saved in /Users/you/.ssh/id_rsa.pub.\n# The key fingerprint is:\n# 01:0f:f4:3b:ca:85:sd:17:sd:7d:sd:68:9d:sd:a2:sd your_email@example.com\n\n\n\n\nLater you'll need to pass the \n.pub\n file's contents to Cloudbreak and use the private part to SSH to the instances\n\n\nProvisioning via Browser\n\n\nYou can log into the Cloudbreak application at \nhttp://\nPUBLIC_IP\n:3000\n.\n\n\nThe main goal of the Cloudbreak UI is to easily create clusters on your own cloud provider account.\nThis description details the GCP setup - if you'd like to use a different cloud provider check out its manual.\n\n\nThis document explains the four steps that need to be followed to create Cloudbreak clusters from the UI:\n\n\n\n\nconnect your GCP account with Cloudbreak\n\n\ncreate some template resources on the UI that describe the infrastructure of your clusters\n\n\ncreate a blueprint that describes the HDP services in your clusters and add some recipes for customization\n\n\nlaunch the cluster itself based on these template resources\n\n\n\n\n\n\nIMPORTANT\n Make sure that you have sufficient qouta (CPU, network, etc) for the requested cluster size.\n\n\n\n\nSetting up GCP credentials\n\n\nCloudbreak works by connecting your GCP account through so called \nCredentials\n, and then uses these credentials to \ncreate resources on your behalf. The credentials can be configured on the \nmanage credentials\n panel on the \nCloudbreak Dashboard.\n\n\nTo create a new GCP credential follow these steps:\n\n\n\n\nFill out the new credential \nName\n\n\nOnly alphanumeric and lowercase characters (min 5, max 100 characters) can be applied\n\n\n\n\n\n\nCopy your GCP project ID to the \nProject Id\n field\n\n\nCopy your GCP Service Account email address to the \nService Account Email Address\n field\n\n\nUpload your GCP Service Account private key (generated \np12 Key\n) to the \nService Account Private (p12) Key\n field\n\n\nCopy your SSH public key to the \nSSH public key\n field\n\n\nThe SSH public key must be in OpenSSH format and it's private keypair can be used later to \nSSH onto every instance\n of every cluster you'll create with this credential.\n\n\nThe \nSSH username\n for the GCP instances is \ncloudbreak\n.\n\n\n\n\n\n\n\n\n\n\nAny other parameter is optional here.\n\n\nPublic in account\n means that all the users belonging to your account will be able to use this credential to create \nclusters, but cannot delete it.\n\n\n\n\n\n\nFull size \nhere\n.\n\n\nInfrastructure templates\n\n\nAfter your GCP account is linked to Cloudbreak you can start creating resource templates that describe your clusters' \ninfrastructure:\n\n\n\n\ntemplates\n\n\nnetworks\n\n\nsecurity groups\n\n\n\n\nWhen you create one of the above resource, \nCloudbreak does not make any requests to GCP. Resources are only created \non GCP after the \ncreate cluster\n button has pushed.\n These templates are saved to Cloudbreak's database and can be \nreused with multiple clusters to describe the infrastructure.\n\n\nTemplates\n\n\nTemplates describe the \ninstances of your cluster\n - the instance type and the attached volumes. A typical setup is\n to combine multiple templates in a cluster for the different types of nodes. For example you may want to attach multiple\n large disks to the datanodes or have memory optimized instances for Spark nodes.\n\n\nThe instance templates can be configured on the \nmanage templates\n panel on the Cloudbreak Dashboard.\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to use this resource to create clusters, but cannot delete it\n\n\nNetworks\n\n\nYour clusters can be created in their own \nnetworks\n or in one of your already existing one. If you choose an \nexisting network, it is possible to create a new subnet within the network. The subnet's IP range must be defined in \nthe \nSubnet (CIDR)\n field using the general CIDR notation. You can read more about \nGCP Networks\n and \nSubnet \nnetworks\n.\n\n\nDefault GCP Network\n\n\nIf you don't want to create or use your custom network, you can use the \ndefault-gcp-network\n for all your \nCloudbreak clusters. It will create a new network with a \n10.0.0.0/16\n subnet every time a cluster is created.\n\n\nCustom GCP Network\n\n\nIf you'd like to deploy a cluster to a custom network you'll have to \ncreate a new network\n template on the \nmanage \nnetworks\n panel.\n\n\nYou have the following options:\n\n\n\n\nCreate a new virtual network and a new subnet\n: Every time a cluster is created with this kind of network setup a new virtual network and a new subnet with the specified IP range will be created for the instances on Google Cloud.\n\n\nCreate a new subnet in an existing virtual network\n: Use this kind of network setup if you already have a virtual network on Google Cloud where you'd like to put the Cloudbreak created cluster but you'd like to have a separate subnet for it.\n\n\nUse an existing subnet in an existing virtual network\n: Use this kind of network setup if you have an existing virtual network with one or more subnets on Google Cloud and you'd like to start the instances of a cluster in one of those subnets.\n\n\nUse a legacy network without subnets\n: Use this kind of network setup if you have a legacy virtual network on Google Cloud that doesn't have subnet support and you'd like to start instances in that virtual network directly.\n\n\n\n\n\n\nIMPORTANT\n Please make sure the defined subnet here doesn't overlap with any of your already deployed subnet in the\n network, because of the validation only happens after the cluster creation starts.\n\n\nIn case of existing subnet make sure you have enough room within your network space for the new instances. The \nprovided subnet CIDR will be ignored, but a proper CIDR range will be used.\n\n\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to use this network template \nto create clusters, but cannot delete it.\n\n\n\n\nNOTE\n The new networks are created on GCP only after the the cluster provisioning starts with the selected \nnetwork template.\n\n\n\n\n\n\nFull size \nhere\n.\n\n\nSecurity groups\n\n\nSecurity group templates are very similar to the \nFirewalls on GCP\n. \nThey describe the allowed inbound traffic \nto the instances in the cluster.\n Currently only one security group template can be selected for a Cloudbreak cluster \nand all the instances have a public IP address so all the instances in the cluster will belong to the same security \ngroup. This may change in a later release.\n\n\nDefault Security Group\n\n\nYou can also use the two pre-defined security groups in Cloudbreak.\n\n\nonly-ssh-and-ssl:\n all ports are locked down except for SSH and gateway HTTPS (you can't access Hadoop services \noutside of the virtual network):\n\n\n\n\nSSH (22)\n\n\nHTTPS (443)\n\n\n\n\nall-services-port:\n all Hadoop services and SSH, gateway HTTPS are accessible by default:\n\n\n\n\nSSH (22)\n\n\nHTTPS (443)\n\n\nAmbari (8080)\n\n\nConsul (8500)\n\n\nNN (50070)\n\n\nRM Web (8088)\n\n\nScheduler (8030RM)\n\n\nIPC (8050RM)\n\n\nJob history server (19888)\n\n\nHBase master (60000)\n\n\nHBase master web (60010)\n\n\nHBase RS (16020)\n\n\nHBase RS info (60030)\n\n\nFalcon (15000)\n\n\nStorm (8744)\n\n\nHive metastore (9083)\n\n\nHive server (10000)\n\n\nHive server HTTP (10001)\n\n\nAccumulo master (9999)\n\n\nAccumulo Tserver (9997)\n\n\nAtlas (21000)\n\n\nKNOX (8443)\n\n\nOozie (11000)\n\n\nSpark HS (18080)\n\n\nNM Web (8042)\n\n\nZeppelin WebSocket (9996)\n\n\nZeppelin UI (9995)\n\n\nKibana (3080)\n\n\nElasticsearch (9200)\n\n\n\n\nCustom Security Group\n\n\nYou can define your own security group by adding all the ports, protocols and CIDR range you'd like to use. The rules\n defined here doesn't need to contain the internal rules, those are automatically added by Cloudbreak to the security\n  group on GCP.\n\n\n\n\nIMPORTANT\n 443 and 22 ports needs to be there in every security group otherwise Cloudbreak won't be able to communicate with the \nprovisioned cluster\n\n\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to use this security group \ntemplate to create clusters, but cannot delete it.\n\n\n\n\nNOTE\n The security groups are created on GCP only after the cluster provisioning starts with the selected \nsecurity group template.\n\n\n\n\n\n\nFull size \nhere\n.\n\n\nDefining cluster services\n\n\nBlueprints\n\n\nBlueprints are your declarative definition of a Hadoop cluster. These are the same blueprints that are \nused by Ambari\n.\n\n\nYou can use the 3 default blueprints pre-defined in Cloudbreak or you can create your own ones.\nBlueprints can be added from file, URL (an \nexample blueprint\n) or the \nwhole JSON can be written in the \nJSON text\n box.\n\n\nThe host groups in the JSON will be mapped to a set of instances when starting the cluster. Besides this the services and\n components will also be installed on the corresponding nodes. Blueprints can be modified later from the Ambari UI.\n\n\n\n\nNOTE\n Not necessary to define all the configuration in the blueprint. If a configuration is missing, Ambari will \nfill that with a default value.\n\n\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to use this blueprint to \ncreate clusters, but cannot delete or modify it.\n\n\n\n\nFull size \nhere\n.\n\n\nA blueprint can be exported from a running Ambari cluster that can be reused in Cloudbreak with slight \nmodifications.\n\nThere is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the \nmodifications have to be done manually.\nWhen the blueprint is exported some configurations are hardcoded for example domain names, memory configurations...etc. that won't be applicable to the Cloudbreak cluster\n\n\nCluster customization\n\n\nSometimes it can be useful to \ndefine some custom scripts so called Recipes in Cloudbreak\n that run during cluster \ncreation and add some additional functionality.\n\n\nFor example it can be a service you'd like to install but it's not supported by Ambari or some script that \nautomatically downloads some data to the necessary nodes.\nThe most \nnotable example is Ranger setup\n:\n\n\n\n\nIt has a prerequisite of a running database when Ranger Admin is installing.\n\n\nA PostgreSQL database can be easily started and configured with a recipe before the blueprint installation starts.\n\n\n\n\nTo learn more about these and check the Ranger recipe out, take a look at the \nCluster customization\n\n\nCluster deployment\n\n\nAfter all the cluster resources are configured you can deploy a new HDP cluster.\n\n\nHere is a \nbasic flow for cluster creation on Cloudbreak Web UI\n:\n\n\n\n\nStart by selecting a previously created GCP credential in the header.\n\n\n\n\n\n\nFull size \nhere\n.\n\n\n\n\nOpen \ncreate cluster\n\n\n\n\nConfigure Cluster\n tab\n\n\n\n\nFill out the new cluster \nname\n\n\nCluster name must start with a lowercase alphabetic character then you can apply lowercase alphanumeric and \n   hyphens only (min 5, max 40 characters)\n\n\n\n\n\n\nSelect a \nRegion\n where you like your cluster be provisioned\n\n\nClick on the \nSetup Network and Security\n button\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to see the created cluster on\n the UI, but cannot delete or modify it.\n\n\n\n\n\n\n\n\nSetup Network and Security\n tab\n\n\n\n\nSelect one of the networks\n\n\nSelect one of the security groups\n\n\nClick on the \nChoose Blueprint\n button\n\n\nIf \nEnable security\n is checked as well, Cloudbreak will install Key Distribution Center (KDC) and the cluster will \nbe Kerberized. See more about it in the \nKerberos\n section of this documentation.\n\n\n\n\n\n\n\n\nChoose Blueprint\n tab\n\n\n\n\nSelect one of the blueprint\n\n\nAfter you've selected a \nBlueprint\n, you should be able to configure:\n\n\nthe templates\n\n\nthe number of nodes for all of the host groups in the blueprint\n\n\nthe recipes for nodes\n\n\n\n\n\n\nClick on the \nAdd File System\n button\n\n\n\n\nAdd File System\n tab\n\n\n\n\nSelect one of the file system that fits your needs\n\n\nAfter you've selected \nGCS file system\n, you should configure:\n\n\nDefault Bucket Name\n\n\n\n\n\n\nClick on the \nReview and Launch\n button\n\n\nYou can read more about \nGCS File System\n and \nBucket Naming\n in GCP \nDocumentation.\n\n\n\n\n\n\n\n\nReview and Launch\n tab\n\n\n\n\nAfter the \ncreate and start cluster\n button has clicked Cloudbreak will start to create the cluster's resources on \n your GCP account.\n\n\n\n\nCloudbreak uses \nGoogle Cloud Platform\n to create the resources - you can check out the resources created by Cloudbreak\n on the \nCompute Engine\n page of the \nGoogle Compute Platform\n.\n\n\n\nFull size \nhere\n.\n\n\nBesides these you can check the progress on the Cloudbreak Web UI itself if you open the new cluster's \nEvent History\n.\n\n\n\nFull size \nhere\n.\n\n\nAdvanced options\n\n\nThere are some advanced features when deploying a new cluster, these are the following:\n\n\nAvailability Zone\n You can restrict the instances to a \nspecific availability zone\n. It may be useful if you're using\n reserved instances.\n\n\nMinimum cluster size\n The provisioning strategy in case of the cloud provider cannot allocate all the requested nodes.\n\n\nValidate blueprint\n This is selected by default. Cloudbreak validates the Ambari blueprint in this case.\n\n\nShipyard enabled cluster\n This is selected by default. Cloudbreak will start a \nShipyard\n container which helps you to manage your containers.\n\n\nConfig recommendation strategy\n Strategy for configuration recommendations how will be applied. Recommended \nconfigurations gathered by the response of the stack advisor. \n\n\n\n\nNEVER_APPLY\n               Configuration recommendations are ignored with this option.\n\n\nONLY_STACK_DEFAULTS_APPLY\n Applies only on the default configurations for all included services.\n\n\nALWAYS_APPLY\n              Applies on all configuration properties.\n\n\n\n\nStart LDAP and configure SSSD\n Enables the \nSystem Security Services Daemon\n configuration.\n\n\nCluster termination\n\n\nYou can terminate running or stopped clusters with the \nterminate\n button in the cluster details.\n\n\n\n\nIMPORTANT\n Always use Cloudbreak to terminate the cluster. If that fails for some reason, try to delete the \nGCP instances first. Instances are started in an Auto Scaling Group so they may be restarted if you terminate an \ninstance manually!\n\n\n\n\nSometimes Cloudbreak cannot synchronize it's state with the cluster state at the cloud provider and the cluster can't\n be terminated. In this case the \nForced termination\n option can help to terminate the cluster at the Cloudbreak \n side. \nIf it has happened:\n\n\n\n\nYou should check the related resources at the Google Cloud Platform\n\n\nIf it is needed you need to manually remove resources from there\n\n\n\n\n\n\nFull size \nhere\n.\n\n\nInteractive mode / Cloudbreak Shell\n\n\nThe goal with the Cloudbreak Shell (Cloudbreak CLI) was to provide an interactive command line tool which supports:\n\n\n\n\nall functionality available through the REST API or Cloudbreak Web UI\n\n\nmakes possible complete automation of management task via scripts\n\n\ncontext aware command availability\n\n\ntab completion\n\n\nrequired/optional parameter support\n\n\nhint command to guide you on the usual path\n\n\n\n\nStart Cloudbreak Shell\n\n\nTo start the Cloudbreak CLI use the following commands:\n\n\n\n\nOpen your \ncloudbreak-deployment\n directory if it is needed. For example:\n\n\n\n\n   cd cloudbreak-deployment\n\n\n\n\n\n\nStart the \ncbd\n from here if it is needed\n\n\n\n\n   cbd start\n\n\n\n\n\n\nIn the root of your \ncloudbreak-deployment\n folder apply:\n\n\n\n\n   cbd util cloudbreak-shell\n\n\n\n\n\n\nAt the very first time it will take for a while, because of need to download all the necessary docker images.\n\n\n\n\nThis will launch the Cloudbreak shell inside a Docker container then it is ready to use.\n\n\n\nFull size \nhere\n.\n\n\n\n\nIMPORTANT You have to copy all your files into the \ncbd\n working directory, what you would like to use in shell.\n For \nexample if your \ncbd\n working directory is \n~/cloudbreak-deployment\n then copy your \nblueprint JSON, public ssh key \nfile...etc.\n to here. You can refer to these files with their names from the shell.\n\n\n\n\nAutocomplete and hints\n\n\nCloudbreak Shell helps to you with \nhint messages\n from the very beginning, for example:\n\n\ncloudbreak-shell\nhint\nHint: Add a blueprint with the 'blueprint add' command or select an existing one with 'blueprint select'\ncloudbreak-shell\n\n\n\n\n\nBeyond this you can use the \nautocompletion (double-TAB)\n as well:\n\n\ncloudbreak-shell\ncredential create --\ncredential create --AWS          credential create --AZURE        credential create --EC2          credential create --GCP          credential create --OPENSTACK\n\n\n\n\nProvisioning via CLI\n\n\nSetting up GCP credential\n\n\nCloudbreak works by connecting your GCP account through so called Credentials, and then uses these credentials to \ncreate resources on your behalf. Credentials can be configured with the following command for example:\n\n\ncredential create --GCP --description \nsample description\n --name my-gcp-credential --projectId \nyour gcp projectid\n \n--serviceAccountId \nyour GCP service account mail address\n --serviceAccountPrivateKeyPath /files/mykey.p12 \n--sshKeyString \nssh-rsa AAAAB3***etc.\n\n\n\n\n\n\n\nNOTE\n that Cloudbreak \ndoes not set your cloud user details\n - we work around the concept of GCP Service \nAccount Credentials. You should have already a valid GCP service account. You can find further details \nhere\n.\n\n\n\n\nAlternatives to provide \nSSH Key\n:\n\n\n\n\nyou can upload your public key from an url: \n\u2014sshKeyUrl\n \n\n\nor you can add the path of your public key: \n\u2014sshKeyPath\n\n\n\n\nYou can check whether the credential was created successfully\n\n\ncredential list\n\n\n\n\nYou can switch between your existing credentials\n\n\ncredential select --name my-gcp-credential\n\n\n\n\nInfrastructure templates\n\n\nAfter your GCP account is linked to Cloudbreak you can start creating resource templates that describe your clusters' \ninfrastructure:\n\n\n\n\nsecurity groups\n\n\nnetworks\n\n\ntemplates\n\n\n\n\nWhen you create one of the above resource, \nCloudbreak does not make any requests to GCP. Resources are only created\n on GCP after the \ncluster create\n has applied.\n These templates are saved to Cloudbreak's database and can be \n reused with multiple clusters to describe the infrastructure.\n\n\nTemplates\n\n\nTemplates describe the \ninstances of your cluster\n - the instance type and the attached volumes. A typical setup is\n to combine multiple templates in a cluster for the different types of nodes. For example you may want to attach multiple\n large disks to the datanodes or have memory optimized instances for Spark nodes.\n\n\nA template can be used repeatedly to create identical copies of the same stack (or to use as a foundation to start a \nnew stack). Templates can be configured with the following command for example:\n\n\ntemplate create --GCP --name my-gcp-template --instanceType n1-standard-2 --volumeCount 2 --volumeSize 100\n\n\n\n\nOther available options here:\n\n\n--volumeType\n The default is \npd-standard\n (HDD), other allowed value is \npd-ssd\n \n(SSD).\n\n\n--publicInAccount\n is true, all the users belonging to your account will be able to use this template \nto create clusters, but cannot delete it.\n\n\nYou can check whether the template was created successfully\n\n\ntemplate list\n\n\n\n\nNetworks\n\n\nYour clusters can be created in their own \nnetworks\n or in one of your already existing one. If you choose an \nexisting network, it is possible to create a new subnet within the network. The subnet's IP range must be defined in \nthe \nSubnet (CIDR)\n field using the general CIDR notation. You can read more about \nGCP Networks\n and \nSubnet networks\n.\n\n\nDefault GCP Network\n\n\nIf you don't want to create or use your custom network, you can use the \ndefault-gcp-network\n for all your \nCloudbreak clusters. It will create a new network with a \n10.0.0.0/16\n subnet every time a cluster is created.\n\n\nCustom GCP Network\n\n\nIf you'd like to deploy a cluster to a custom network you'll have to apply the following command:\n\n\nnetwork create --GCP --name my-gcp-network --description \nsample description\n\n\n\n\n\nOther available options here:\n\n\n--networkId\n The Virtual Network Identifier of your network. This is an optional \nvalue and must be an ID of an existing GCP virtual network. If the identifier is provided, the subnet CIDR will be \nignored and the existing network's CIDR range will be used.\n\n\n--publicInAccount\n is true, all the users belonging to your account will be able to use this network template \nto create clusters, but cannot delete it.\n\n\n--subnet\n specified subnet which will be used by the cluster (will be created under the provisioning).\n\n\n--subnetId\n if you have an existing subnet in the network then you can specify the id here and the cluster will use that existing subnet.\n\n\n\n\nIMPORTANT\n Please make sure the defined subnet here doesn't overlap with any of your \nalready deployed subnet in the network, because of the validation only happens after the cluster creation starts.\n\n\nIn case of existing subnet make sure you have enough room within your network space for the new instances. The \nprovided subnet CIDR will be ignored, but a proper CIDR range will be used.\n\n\n\n\nYou can check whether the network was created successfully\n\n\nnetwork list\n\n\n\n\n\n\nNOTE\n The new networks are created on GCP only after the the cluster provisioning starts with the selected \nnetwork template.\n\n\n\n\nDefining cluster services\n\n\nBlueprints\n\n\nBlueprints are your declarative definition of a Hadoop cluster. These are the same blueprints that are \nused by Ambari\n.\n\n\nYou can use the 3 default blueprints pre-defined in Cloudbreak or you can create your own ones.\nBlueprints can be added from file or URL (an \nexample blueprint\n).\n\n\nThe host groups in the JSON will be mapped to a set of instances when starting the cluster. Besides this the services and\n components will also be installed on the corresponding nodes. Blueprints can be modified later from the Ambari UI.\n\n\n\n\nNOTE\n Not necessary to define all the configuration in the blueprint. If a configuration is missing, Ambari will fill that with a default value.\n\n\n\n\nblueprint add --name my-blueprint --description \nsample description\n --file \nthe path of the blueprint\n\n\n\n\n\nOther available options:\n\n\n--url\n the url of the blueprint\n\n\n--publicInAccount\n If it is true, all the users belonging to your account will be able to use this blueprint to create \nclusters, but cannot delete it.\n\n\nYou can check whether the blueprint was created successfully\n\n\nblueprint list\n\n\n\n\nA blueprint can be exported from a running Ambari cluster that can be reused in Cloudbreak with slight \nmodifications.\n\nThere is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the \nmodifications have to be done manually.\nWhen the blueprint is exported some configurations are hardcoded for example domain names, memory configurations..etc. that won't be applicable to the Cloudbreak cluster.\n\n\nCluster customization\n\n\nSometimes it can be useful to \ndefine some custom scripts so called Recipes in Cloudbreak\n that run during cluster \ncreation and add some additional functionality.\n\n\nFor example it can be a service you'd like to install but it's not supported by Ambari or some script that \nautomatically downloads some data to the necessary nodes.\nThe most \nnotable example is Ranger setup\n:\n\n\n\n\nIt has a prerequisite of a running database when Ranger Admin is installing.\n\n\nA PostgreSQL database can be easily started and configured with a recipe before the blueprint installation starts.\n\n\n\n\nTo learn more about these and check the Ranger recipe out, take a look at the \nCluster customization\n\n\nMetadata show\n\n\nYou can check the stack metadata with\n\n\nstack metadata --name myawsstack --instancegroup master\n\n\n\n\nOther available options:\n\n\n--id\n In this case you can select a stack with id.\n\n\n--outputType\n In this case you can modify the outputformat of the command (RAW or JSON). \n\n\nCluster deployment\n\n\nAfter all the cluster resources are configured you can deploy a new HDP cluster. The following sub-sections show \nyou a \nbasic flow for cluster creation with Cloudbreak Shell\n.\n\n\nSelect credential\n\n\nSelect one of your previously created GCP credential:\n\n\ncredential select --name my-gcp-credential\n\n\n\n\nSelect blueprint\n\n\nSelect one of your previously created blueprint which fits your needs:\n\n\nblueprint select --name multi-node-hdfs-yarn\n\n\n\n\nConfigure instance groups\n\n\nYou must configure instance groups before provisioning. An instance group define a group of nodes with a specified \ntemplate. Usually we create instance groups for host groups in the blueprint.\n\n\ninstancegroup configure --instanceGroup cbgateway --nodecount 1 --templateName minviable-gcp\ninstancegroup configure --instanceGroup master --nodecount 1 --templateName minviable-gcp\ninstancegroup configure --instanceGroup slave_1 --nodecount 1 --templateName minviable-gcp\n\n\n\n\nOther available option:\n\n\n--templateId\n Id of the template\n\n\nSelect network\n\n\nSelect one of your previously created network which fits your needs or a default one:\n\n\nnetwork select --name default-gcp-network\n\n\n\n\nSelect security group\n\n\nSelect one of your previously created security which fits your needs or a default one:\n\n\nsecuritygroup select --name all-services-port\n\n\n\n\nCreate stack / Create cloud infrastructure\n\n\nStack means the running cloud infrastructure that is created based on the instance groups configured earlier \n(\ncredential\n, \ninstancegroups\n, \nnetwork\n, \nsecuritygroup\n). Same as in case of the API or UI the new cluster will \nuse your templates and by using GCP will launch your cloud stack. Use the following command to create a \nstack to be used with your Hadoop cluster:\n\n\nstack create --name mygcpstack --region us-central1\n\n\n\n\nThe infrastructure is created asynchronously, the state of the stack can be checked with the stack \nshow command\n. If \nit reports AVAILABLE, it means that the virtual machines and the corresponding infrastructure is running at the cloud provider.\n\n\nOther available option is:\n\n\n--wait\n - in this case the create command will return only after the process has finished. \n\n\nCreate a Hadoop cluster / Cloud provisioning\n\n\nYou are almost done! One more command and your Hadoop cluster is starting!\n Cloud provisioning is done once the \ncluster is up and running. The new cluster will use your selected blueprint and install your custom Hadoop cluster \nwith the selected components and services.\n\n\ncluster create --description \nmy first cluster\n\n\n\n\n\nOther available option is \n--wait\n - in this case the create command will return only after the process has finished. \n\n\nYou are done!\n You have several opportunities to check the progress during the infrastructure creation then \nprovisioning:\n\n\n\n\nCloudbreak uses \nGoogle Cloud Platform\n to create the resources - you can check out the resources created by \nCloudbreak on the Compute Engine page of the Google Compute Platform..\n\n\n\n\n\n\nFull size \nhere\n.\n\n\n\n\nIf stack then cluster creation have successfully done, you can check the Ambari Web UI. However you need to know the \nAmbari IP (for example: \nhttp://130.211.163.13:8080\n): \n\n\nYou can get the IP from the CLI as a result (\nambariServerIp 130.211.163.13\n) of the following command:\n\n\n\n\n\n\n\n\n         cluster show\n\n\n\n\n\n\nFull size \nhere\n.\n\n\n\n\nBesides these you can check the entire progress and the Ambari IP as well on the Cloudbreak Web UI itself. Open the \nnew cluster's \ndetails\n and its \nEvent History\n here.\n\n\n\n\n\n\nFull size \nhere\n.\n\n\nStop cluster\n\n\nYou have the ability to \nstop your existing stack then its cluster\n if you want to suspend the work on it.\n\n\nSelect a stack for example with its name:\n\n\nstack select --name my-stack\n\n\n\n\nOther available option to define a stack is its \n--id\n.\n\n\nEvery time you should stop the \ncluster\n first then the \nstack\n. So apply following commands to stop the previously \nselected stack:\n\n\ncluster stop\nstack stop\n\n\n\n\nRestart cluster\n\n\nSelect your stack that you would like to restart\n after this you can apply:\n\n\nstack start\n\n\n\n\nAfter the stack has successfully restarted, you can \nrestart the related cluster as well\n:\n\n\ncluster start\n\n\n\n\nUpscale cluster\n\n\nIf you need more instances to your infrastructure, you can \nupscale your selected stack\n:\n\n\nstack node --ADD --instanceGroup host_group_slave_1 --adjustment 6\n\n\n\n\nOther available option is \n--withClusterUpScale\n - this indicates also a cluster upscale after the stack upscale. You\n can upscale the related cluster separately if you want to do this:\n\n\ncluster node --ADD --hostgroup host_group_slave_1 --adjustment 6\n\n\n\n\nDownscale cluster\n\n\nYou also can reduce the number of instances in your infrastructure. \nAfter you selected your stack\n:\n\n\ncluster node --REMOVE  --hostgroup host_group_slave_1 --adjustment -2\n\n\n\n\nOther available option is \n--withStackDownScale\n - this indicates also a stack downscale after the cluster downscale.\n You can downscale the related stack separately if you want to do this:\n\n\nstack node --REMOVE  --instanceGroup host_group_slave_1 --adjustment -2\n\n\n\n\nCluster termination\n\n\nYou can terminate running or stopped clusters with\n\n\nstack terminate --name myawsstack\n\n\n\n\nOther available option is \n--wait\n - in this case the terminate command will return only after the process has finished. \n\n\n\n\nIMPORTANT\n Always use Cloudbreak to terminate the cluster. If that fails for some reason, try to delete the \nCloudFormation stack first. Instances are started in an Auto Scaling Group so they may be restarted if you terminate an instance manually!\n\n\n\n\nSometimes Cloudbreak cannot synchronize it's state with the cluster state at the cloud provider and the cluster can't\n be terminated. In this case the \nForced termination\n option on the Cloudbreak Web UI can help to terminate the cluster\n  at the Cloudbreak side. \nIf it has happened:\n\n\n\n\nYou should check the related resources at the AWS CloudFormation\n\n\nIf it is needed you need to manually remove resources from ther\n\n\n\n\nSilent mode\n\n\nWith Cloudbreak Shell you can execute script files as well. A script file contains shell commands and can \nbe executed with the \nscript\n cloudbreak shell command\n\n\nscript \nyour script file\n\n\n\n\n\nor with the \ncbd util cloudbreak-shell-quiet\n command\n\n\ncbd util cloudbreak-shell-quiet \n example.sh\n\n\n\n\n\n\nIMPORTANT\n You have to copy all your files into the \ncbd\n working directory, what you would like to use in shell.\n For example if your \ncbd\n working directory is ~/cloudbreak-deployment then copy your script file to here.\n\n\n\n\nExample\n\n\nThe following example creates a hadoop cluster with \nhdp-small-default\n blueprint on M3Xlarge instances with 2X100G \nattached disks on \ndefault-gcp-network\n network using \nall-services-port\n security group. You should copy your ssh \npublic key file (with name \nid_rsa.pub\n) and your GCP service account generated private key ( with name \ngcp.p12\n) into your \ncbd\n working \ndirectory and change the \n...\n parts with your GCP credential details.\n\n\ncredential create --GCP --description \nmy credential\n --name my-gcp-credential --projectId \nyour gcp projectid\n --serviceAccountId \nyour GCP service account mail address\n --serviceAccountPrivateKeyPath gcp.p12 --sshKeyFile id_rsa.pub\ncredential select --name my-gcp-credential\ntemplate create --GCP --name gcptemplate --description gcp-template --instanceType n1-standard-4 --volumeSize 100 \n--volumeCount 2\nblueprint select --name hdp-small-default\ninstancegroup configure --instanceGroup cbgateway --nodecount 1 --templateName gcptemplate\ninstancegroup configure --instanceGroup host_group_master_1 --nodecount 1 --templateName gcptemplate\ninstancegroup configure --instanceGroup host_group_master_2 --nodecount 1 --templateName gcptemplate\ninstancegroup configure --instanceGroup host_group_master_3 --nodecount 1 --templateName gcptemplate\ninstancegroup configure --instanceGroup host_group_client_1  --nodecount 1 --templateName gcptemplate\ninstancegroup configure --instanceGroup host_group_slave_1 --nodecount 3 --templateName gcptemplate\nnetwork select --name default-gcp-network\nsecuritygroup select --name all-services-port\nstack create --name my-first-stack --region us-central1\ncluster create --description \nMy first cluster\n\n\n\n\n\nCongratulations!\n Your cluster should now be up and running on this way as well. To learn more about Cloudbreak and \nprovisioning, we have some \ninteresting insights\n for you", 
            "title": "GCP"
        }, 
        {
            "location": "/gcp/#google-cloud-images", 
            "text": "We have pre-built Cloudbreak Deployer cloud image for Google Cloud Platform (GCP). You can launch the latest \nCloudbreak Deployer image at the  Google Developers Console .   Alternatively, instead of using the pre-built cloud images for GCP, you can install Cloudbreak Deployer on your own\n VM. See  installation page  for more information.   Please make sure you added the following ports to your firewall rules:   SSH (22)  Cloudbreak API (8080)  Identity server (8089)  Cloudbreak GUI (3000)  User authentication (3001)", 
            "title": "Google Cloud Images"
        }, 
        {
            "location": "/gcp/#cloudbreak-deployer-gcp-image-details", 
            "text": "", 
            "title": "Cloudbreak Deployer GCP Image Details"
        }, 
        {
            "location": "/gcp/#import-cloudbreak-deployer-image", 
            "text": "You can import the latest Cloudbreak Deployer image on the  Google Developers Console  with the help\n of the  Google Cloud Shell .  Just click on the  Activate Google Cloud Shell  icon in the top right corner of the page:   Full size  here .  Images are global resources, so you can use these across zones and projects.   Full size  here .  You can  create your own Cloudbreak Deployer (cbd) instance from the imported image  on the Google Developers Console.   Minimum and Recommended VM requirements :  4GB RAM, 10GB disk, 2 core", 
            "title": "Import Cloudbreak Deployer Image"
        }, 
        {
            "location": "/gcp/#google-setup", 
            "text": "Cloudbreak Deployer Highlights   The default SSH username for the GCP instances is  cloudbreak .  Cloudbreak Deployer location is  /var/lib/cloudbreak-deployment  on the launched  cbd  VM. This is the \n       cbd  root folder there.  All  cbd  actions must be executed from the  cbd  root folder.", 
            "title": "Google Setup"
        }, 
        {
            "location": "/gcp/#setup-cloudbreak-deployer", 
            "text": "You should already have the Cloudbreak Deployer either by  using the GCP Cloud Images  or by  installing the \nCloudbreak Deployer  manually on your own VM. (The minimum instance type which is fit for cloudbreak is  n1-standard-2 )  If you have your own installed VM, you should check the  Initialize your Profile  \nsection here before starting the provisioning.  You have several opportunities to  connect to the previously created  cbd  VM .   Cloudbreak Deployer location is  /var/lib/cloudbreak-deployment .  All  cbd  actions must be executed from the  cbd  folder.   Open the  cloudbreak-deployment  directory:  cd /var/lib/cloudbreak-deployment  This is the directory of the configuration files and the supporting binaries for Cloudbreak Deployer.", 
            "title": "Setup Cloudbreak Deployer"
        }, 
        {
            "location": "/gcp/#initialize-your-profile", 
            "text": "First initialize  cbd  by creating a  Profile  file:  cbd init  It will create a  Profile  file in the current directory. Please open the  Profile  file then check the  PUBLIC_IP . \nThis is mandatory, because of to can access the Cloudbreak UI (called Uluwatu). In some cases the  cbd  tool tries to \nguess it. If  cbd  cannot get the IP address during the initialization, please set the appropriate value.", 
            "title": "Initialize your Profile"
        }, 
        {
            "location": "/gcp/#start-cloudbreak-deployer", 
            "text": "To start the Cloudbreak application use the following command.\nThis will start all the Docker containers and initialize the application.  cbd start   At the very first time it will take for a while, because of need to download all the necessary docker images.   The  cbd start  command includes the  cbd generate  command which applies the following steps:   creates the  docker-compose.yml  file that describes the configuration of all the Docker containers needed for the Cloudbreak deployment.  creates the  uaa.yml  file that holds the configuration of the identity server used to authenticate users to Cloudbreak.", 
            "title": "Start Cloudbreak Deployer"
        }, 
        {
            "location": "/gcp/#validate-the-started-cloudbreak-deployer", 
            "text": "After the  cbd start  command finishes followings are worthy to check:   Pre-installed Cloudbreak Deployer version and health.      cbd doctor   In case of  cbd update  is needed, please check the related documentation for  Cloudbreak Deployer Update . Most of the  cbd  commands require  root  permissions.    Started Cloudbreak Application logs.      cbd logs cloudbreak   Cloudbreak should start within a minute - you should see a line like this: `Started CloudbreakApplication in 36.823 seconds", 
            "title": "Validate the started Cloudbreak Deployer"
        }, 
        {
            "location": "/gcp/#provisioning-prerequisites", 
            "text": "", 
            "title": "Provisioning Prerequisites"
        }, 
        {
            "location": "/gcp/#creating-a-google-cloud-service-account", 
            "text": "Follow the  instructions  in Google Cloud's documentation to create a  Service account  and  Generate a new P12 key .  Make sure that at API level ( APIs and auth  menu) you have enabled:   Google Compute Engine  Google Compute Engine Instance Group Manager API  Google Compute Engine Instance Groups API  BigQuery API  Google Cloud Deployment Manager API  Google Cloud DNS API  Google Cloud SQL  Google Cloud Storage  Google Cloud Storage JSON API    If you have enabled every API then you have to wait about  10 minutes  for the provider.   When creating GCP credentials  in Cloudbreak you will have to provide the email address of your  Service Account  \n(from the Service accounts page of your Google Cloud Platform Permissions) and the  Project ID  (from the Dashboard \nof your Google Cloud Platform Home) where the service account is created.  You'll also have to  upload the \ngenerated P12 file and provide an OpenSSH formatted public key  that will be used as an SSH key.", 
            "title": "Creating a Google Cloud Service Account"
        }, 
        {
            "location": "/gcp/#generate-a-new-ssh-key", 
            "text": "All the instances created by Cloudbreak are configured to allow key-based SSH,\nso you'll need to provide an SSH public key that can be used later to SSH onto the instances in the clusters you'll create with Cloudbreak.\nYou can use one of your existing keys or you can generate a new one.  To generate a new SSH keypair:  ssh-keygen -t rsa -b 4096 -C  your_email@example.com \n# Creates a new ssh key, using the provided email as a label\n# Generating public/private rsa key pair.  # Enter file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter]\nYou'll be asked to enter a passphrase, but you can leave it empty.\n\n# Enter passphrase (empty for no passphrase): [Type a passphrase]\n# Enter same passphrase again: [Type passphrase again]  After you enter a passphrase the keypair is generated. The output should look something like below.  # Your identification has been saved in /Users/you/.ssh/id_rsa.\n# Your public key has been saved in /Users/you/.ssh/id_rsa.pub.\n# The key fingerprint is:\n# 01:0f:f4:3b:ca:85:sd:17:sd:7d:sd:68:9d:sd:a2:sd your_email@example.com  Later you'll need to pass the  .pub  file's contents to Cloudbreak and use the private part to SSH to the instances", 
            "title": "Generate a new SSH key"
        }, 
        {
            "location": "/gcp/#provisioning-via-browser", 
            "text": "You can log into the Cloudbreak application at  http:// PUBLIC_IP :3000 .  The main goal of the Cloudbreak UI is to easily create clusters on your own cloud provider account.\nThis description details the GCP setup - if you'd like to use a different cloud provider check out its manual.  This document explains the four steps that need to be followed to create Cloudbreak clusters from the UI:   connect your GCP account with Cloudbreak  create some template resources on the UI that describe the infrastructure of your clusters  create a blueprint that describes the HDP services in your clusters and add some recipes for customization  launch the cluster itself based on these template resources    IMPORTANT  Make sure that you have sufficient qouta (CPU, network, etc) for the requested cluster size.", 
            "title": "Provisioning via Browser"
        }, 
        {
            "location": "/gcp/#setting-up-gcp-credentials", 
            "text": "Cloudbreak works by connecting your GCP account through so called  Credentials , and then uses these credentials to \ncreate resources on your behalf. The credentials can be configured on the  manage credentials  panel on the \nCloudbreak Dashboard.  To create a new GCP credential follow these steps:   Fill out the new credential  Name  Only alphanumeric and lowercase characters (min 5, max 100 characters) can be applied    Copy your GCP project ID to the  Project Id  field  Copy your GCP Service Account email address to the  Service Account Email Address  field  Upload your GCP Service Account private key (generated  p12 Key ) to the  Service Account Private (p12) Key  field  Copy your SSH public key to the  SSH public key  field  The SSH public key must be in OpenSSH format and it's private keypair can be used later to  SSH onto every instance  of every cluster you'll create with this credential.  The  SSH username  for the GCP instances is  cloudbreak .      Any other parameter is optional here.  Public in account  means that all the users belonging to your account will be able to use this credential to create \nclusters, but cannot delete it.    Full size  here .", 
            "title": "Setting up GCP credentials"
        }, 
        {
            "location": "/gcp/#infrastructure-templates", 
            "text": "After your GCP account is linked to Cloudbreak you can start creating resource templates that describe your clusters' \ninfrastructure:   templates  networks  security groups   When you create one of the above resource,  Cloudbreak does not make any requests to GCP. Resources are only created \non GCP after the  create cluster  button has pushed.  These templates are saved to Cloudbreak's database and can be \nreused with multiple clusters to describe the infrastructure.  Templates  Templates describe the  instances of your cluster  - the instance type and the attached volumes. A typical setup is\n to combine multiple templates in a cluster for the different types of nodes. For example you may want to attach multiple\n large disks to the datanodes or have memory optimized instances for Spark nodes.  The instance templates can be configured on the  manage templates  panel on the Cloudbreak Dashboard.  If  Public in account  is checked all the users belonging to your account will be able to use this resource to create clusters, but cannot delete it  Networks  Your clusters can be created in their own  networks  or in one of your already existing one. If you choose an \nexisting network, it is possible to create a new subnet within the network. The subnet's IP range must be defined in \nthe  Subnet (CIDR)  field using the general CIDR notation. You can read more about  GCP Networks  and  Subnet \nnetworks .  Default GCP Network  If you don't want to create or use your custom network, you can use the  default-gcp-network  for all your \nCloudbreak clusters. It will create a new network with a  10.0.0.0/16  subnet every time a cluster is created.  Custom GCP Network  If you'd like to deploy a cluster to a custom network you'll have to  create a new network  template on the  manage \nnetworks  panel.  You have the following options:   Create a new virtual network and a new subnet : Every time a cluster is created with this kind of network setup a new virtual network and a new subnet with the specified IP range will be created for the instances on Google Cloud.  Create a new subnet in an existing virtual network : Use this kind of network setup if you already have a virtual network on Google Cloud where you'd like to put the Cloudbreak created cluster but you'd like to have a separate subnet for it.  Use an existing subnet in an existing virtual network : Use this kind of network setup if you have an existing virtual network with one or more subnets on Google Cloud and you'd like to start the instances of a cluster in one of those subnets.  Use a legacy network without subnets : Use this kind of network setup if you have a legacy virtual network on Google Cloud that doesn't have subnet support and you'd like to start instances in that virtual network directly.    IMPORTANT  Please make sure the defined subnet here doesn't overlap with any of your already deployed subnet in the\n network, because of the validation only happens after the cluster creation starts.  In case of existing subnet make sure you have enough room within your network space for the new instances. The \nprovided subnet CIDR will be ignored, but a proper CIDR range will be used.   If  Public in account  is checked all the users belonging to your account will be able to use this network template \nto create clusters, but cannot delete it.   NOTE  The new networks are created on GCP only after the the cluster provisioning starts with the selected \nnetwork template.    Full size  here .  Security groups  Security group templates are very similar to the  Firewalls on GCP .  They describe the allowed inbound traffic \nto the instances in the cluster.  Currently only one security group template can be selected for a Cloudbreak cluster \nand all the instances have a public IP address so all the instances in the cluster will belong to the same security \ngroup. This may change in a later release.  Default Security Group  You can also use the two pre-defined security groups in Cloudbreak.  only-ssh-and-ssl:  all ports are locked down except for SSH and gateway HTTPS (you can't access Hadoop services \noutside of the virtual network):   SSH (22)  HTTPS (443)   all-services-port:  all Hadoop services and SSH, gateway HTTPS are accessible by default:   SSH (22)  HTTPS (443)  Ambari (8080)  Consul (8500)  NN (50070)  RM Web (8088)  Scheduler (8030RM)  IPC (8050RM)  Job history server (19888)  HBase master (60000)  HBase master web (60010)  HBase RS (16020)  HBase RS info (60030)  Falcon (15000)  Storm (8744)  Hive metastore (9083)  Hive server (10000)  Hive server HTTP (10001)  Accumulo master (9999)  Accumulo Tserver (9997)  Atlas (21000)  KNOX (8443)  Oozie (11000)  Spark HS (18080)  NM Web (8042)  Zeppelin WebSocket (9996)  Zeppelin UI (9995)  Kibana (3080)  Elasticsearch (9200)   Custom Security Group  You can define your own security group by adding all the ports, protocols and CIDR range you'd like to use. The rules\n defined here doesn't need to contain the internal rules, those are automatically added by Cloudbreak to the security\n  group on GCP.   IMPORTANT  443 and 22 ports needs to be there in every security group otherwise Cloudbreak won't be able to communicate with the \nprovisioned cluster   If  Public in account  is checked all the users belonging to your account will be able to use this security group \ntemplate to create clusters, but cannot delete it.   NOTE  The security groups are created on GCP only after the cluster provisioning starts with the selected \nsecurity group template.    Full size  here .", 
            "title": "Infrastructure templates"
        }, 
        {
            "location": "/gcp/#defining-cluster-services", 
            "text": "Blueprints  Blueprints are your declarative definition of a Hadoop cluster. These are the same blueprints that are  used by Ambari .  You can use the 3 default blueprints pre-defined in Cloudbreak or you can create your own ones.\nBlueprints can be added from file, URL (an  example blueprint ) or the \nwhole JSON can be written in the  JSON text  box.  The host groups in the JSON will be mapped to a set of instances when starting the cluster. Besides this the services and\n components will also be installed on the corresponding nodes. Blueprints can be modified later from the Ambari UI.   NOTE  Not necessary to define all the configuration in the blueprint. If a configuration is missing, Ambari will \nfill that with a default value.   If  Public in account  is checked all the users belonging to your account will be able to use this blueprint to \ncreate clusters, but cannot delete or modify it.   Full size  here .  A blueprint can be exported from a running Ambari cluster that can be reused in Cloudbreak with slight \nmodifications. \nThere is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the \nmodifications have to be done manually.\nWhen the blueprint is exported some configurations are hardcoded for example domain names, memory configurations...etc. that won't be applicable to the Cloudbreak cluster  Cluster customization  Sometimes it can be useful to  define some custom scripts so called Recipes in Cloudbreak  that run during cluster \ncreation and add some additional functionality.  For example it can be a service you'd like to install but it's not supported by Ambari or some script that \nautomatically downloads some data to the necessary nodes.\nThe most  notable example is Ranger setup :   It has a prerequisite of a running database when Ranger Admin is installing.  A PostgreSQL database can be easily started and configured with a recipe before the blueprint installation starts.   To learn more about these and check the Ranger recipe out, take a look at the  Cluster customization", 
            "title": "Defining cluster services"
        }, 
        {
            "location": "/gcp/#cluster-deployment", 
            "text": "After all the cluster resources are configured you can deploy a new HDP cluster.  Here is a  basic flow for cluster creation on Cloudbreak Web UI :   Start by selecting a previously created GCP credential in the header.    Full size  here .   Open  create cluster   Configure Cluster  tab   Fill out the new cluster  name  Cluster name must start with a lowercase alphabetic character then you can apply lowercase alphanumeric and \n   hyphens only (min 5, max 40 characters)    Select a  Region  where you like your cluster be provisioned  Click on the  Setup Network and Security  button  If  Public in account  is checked all the users belonging to your account will be able to see the created cluster on\n the UI, but cannot delete or modify it.     Setup Network and Security  tab   Select one of the networks  Select one of the security groups  Click on the  Choose Blueprint  button  If  Enable security  is checked as well, Cloudbreak will install Key Distribution Center (KDC) and the cluster will \nbe Kerberized. See more about it in the  Kerberos  section of this documentation.     Choose Blueprint  tab   Select one of the blueprint  After you've selected a  Blueprint , you should be able to configure:  the templates  the number of nodes for all of the host groups in the blueprint  the recipes for nodes    Click on the  Add File System  button   Add File System  tab   Select one of the file system that fits your needs  After you've selected  GCS file system , you should configure:  Default Bucket Name    Click on the  Review and Launch  button  You can read more about  GCS File System  and  Bucket Naming  in GCP \nDocumentation.     Review and Launch  tab   After the  create and start cluster  button has clicked Cloudbreak will start to create the cluster's resources on \n your GCP account.   Cloudbreak uses  Google Cloud Platform  to create the resources - you can check out the resources created by Cloudbreak\n on the  Compute Engine  page of the  Google Compute Platform .  Full size  here .  Besides these you can check the progress on the Cloudbreak Web UI itself if you open the new cluster's  Event History .  Full size  here .  Advanced options  There are some advanced features when deploying a new cluster, these are the following:  Availability Zone  You can restrict the instances to a  specific availability zone . It may be useful if you're using\n reserved instances.  Minimum cluster size  The provisioning strategy in case of the cloud provider cannot allocate all the requested nodes.  Validate blueprint  This is selected by default. Cloudbreak validates the Ambari blueprint in this case.  Shipyard enabled cluster  This is selected by default. Cloudbreak will start a  Shipyard  container which helps you to manage your containers.  Config recommendation strategy  Strategy for configuration recommendations how will be applied. Recommended \nconfigurations gathered by the response of the stack advisor.    NEVER_APPLY                Configuration recommendations are ignored with this option.  ONLY_STACK_DEFAULTS_APPLY  Applies only on the default configurations for all included services.  ALWAYS_APPLY               Applies on all configuration properties.   Start LDAP and configure SSSD  Enables the  System Security Services Daemon  configuration.", 
            "title": "Cluster deployment"
        }, 
        {
            "location": "/gcp/#cluster-termination", 
            "text": "You can terminate running or stopped clusters with the  terminate  button in the cluster details.   IMPORTANT  Always use Cloudbreak to terminate the cluster. If that fails for some reason, try to delete the \nGCP instances first. Instances are started in an Auto Scaling Group so they may be restarted if you terminate an \ninstance manually!   Sometimes Cloudbreak cannot synchronize it's state with the cluster state at the cloud provider and the cluster can't\n be terminated. In this case the  Forced termination  option can help to terminate the cluster at the Cloudbreak \n side.  If it has happened:   You should check the related resources at the Google Cloud Platform  If it is needed you need to manually remove resources from there    Full size  here .", 
            "title": "Cluster termination"
        }, 
        {
            "location": "/gcp/#interactive-mode-cloudbreak-shell", 
            "text": "The goal with the Cloudbreak Shell (Cloudbreak CLI) was to provide an interactive command line tool which supports:   all functionality available through the REST API or Cloudbreak Web UI  makes possible complete automation of management task via scripts  context aware command availability  tab completion  required/optional parameter support  hint command to guide you on the usual path", 
            "title": "Interactive mode / Cloudbreak Shell"
        }, 
        {
            "location": "/gcp/#start-cloudbreak-shell", 
            "text": "To start the Cloudbreak CLI use the following commands:   Open your  cloudbreak-deployment  directory if it is needed. For example:      cd cloudbreak-deployment   Start the  cbd  from here if it is needed      cbd start   In the root of your  cloudbreak-deployment  folder apply:      cbd util cloudbreak-shell   At the very first time it will take for a while, because of need to download all the necessary docker images.   This will launch the Cloudbreak shell inside a Docker container then it is ready to use.  Full size  here .   IMPORTANT You have to copy all your files into the  cbd  working directory, what you would like to use in shell.  For \nexample if your  cbd  working directory is  ~/cloudbreak-deployment  then copy your  blueprint JSON, public ssh key \nfile...etc.  to here. You can refer to these files with their names from the shell.", 
            "title": "Start Cloudbreak Shell"
        }, 
        {
            "location": "/gcp/#autocomplete-and-hints", 
            "text": "Cloudbreak Shell helps to you with  hint messages  from the very beginning, for example:  cloudbreak-shell hint\nHint: Add a blueprint with the 'blueprint add' command or select an existing one with 'blueprint select'\ncloudbreak-shell   Beyond this you can use the  autocompletion (double-TAB)  as well:  cloudbreak-shell credential create --\ncredential create --AWS          credential create --AZURE        credential create --EC2          credential create --GCP          credential create --OPENSTACK", 
            "title": "Autocomplete and hints"
        }, 
        {
            "location": "/gcp/#provisioning-via-cli", 
            "text": "", 
            "title": "Provisioning via CLI"
        }, 
        {
            "location": "/gcp/#setting-up-gcp-credential", 
            "text": "Cloudbreak works by connecting your GCP account through so called Credentials, and then uses these credentials to \ncreate resources on your behalf. Credentials can be configured with the following command for example:  credential create --GCP --description  sample description  --name my-gcp-credential --projectId  your gcp projectid  \n--serviceAccountId  your GCP service account mail address  --serviceAccountPrivateKeyPath /files/mykey.p12 \n--sshKeyString  ssh-rsa AAAAB3***etc.    NOTE  that Cloudbreak  does not set your cloud user details  - we work around the concept of GCP Service \nAccount Credentials. You should have already a valid GCP service account. You can find further details  here .   Alternatives to provide  SSH Key :   you can upload your public key from an url:  \u2014sshKeyUrl    or you can add the path of your public key:  \u2014sshKeyPath   You can check whether the credential was created successfully  credential list  You can switch between your existing credentials  credential select --name my-gcp-credential", 
            "title": "Setting up GCP credential"
        }, 
        {
            "location": "/gcp/#infrastructure-templates_1", 
            "text": "After your GCP account is linked to Cloudbreak you can start creating resource templates that describe your clusters' \ninfrastructure:   security groups  networks  templates   When you create one of the above resource,  Cloudbreak does not make any requests to GCP. Resources are only created\n on GCP after the  cluster create  has applied.  These templates are saved to Cloudbreak's database and can be \n reused with multiple clusters to describe the infrastructure.  Templates  Templates describe the  instances of your cluster  - the instance type and the attached volumes. A typical setup is\n to combine multiple templates in a cluster for the different types of nodes. For example you may want to attach multiple\n large disks to the datanodes or have memory optimized instances for Spark nodes.  A template can be used repeatedly to create identical copies of the same stack (or to use as a foundation to start a \nnew stack). Templates can be configured with the following command for example:  template create --GCP --name my-gcp-template --instanceType n1-standard-2 --volumeCount 2 --volumeSize 100  Other available options here:  --volumeType  The default is  pd-standard  (HDD), other allowed value is  pd-ssd  \n(SSD).  --publicInAccount  is true, all the users belonging to your account will be able to use this template \nto create clusters, but cannot delete it.  You can check whether the template was created successfully  template list  Networks  Your clusters can be created in their own  networks  or in one of your already existing one. If you choose an \nexisting network, it is possible to create a new subnet within the network. The subnet's IP range must be defined in \nthe  Subnet (CIDR)  field using the general CIDR notation. You can read more about  GCP Networks  and  Subnet networks .  Default GCP Network  If you don't want to create or use your custom network, you can use the  default-gcp-network  for all your \nCloudbreak clusters. It will create a new network with a  10.0.0.0/16  subnet every time a cluster is created.  Custom GCP Network  If you'd like to deploy a cluster to a custom network you'll have to apply the following command:  network create --GCP --name my-gcp-network --description  sample description   Other available options here:  --networkId  The Virtual Network Identifier of your network. This is an optional \nvalue and must be an ID of an existing GCP virtual network. If the identifier is provided, the subnet CIDR will be \nignored and the existing network's CIDR range will be used.  --publicInAccount  is true, all the users belonging to your account will be able to use this network template \nto create clusters, but cannot delete it.  --subnet  specified subnet which will be used by the cluster (will be created under the provisioning).  --subnetId  if you have an existing subnet in the network then you can specify the id here and the cluster will use that existing subnet.   IMPORTANT  Please make sure the defined subnet here doesn't overlap with any of your \nalready deployed subnet in the network, because of the validation only happens after the cluster creation starts.  In case of existing subnet make sure you have enough room within your network space for the new instances. The \nprovided subnet CIDR will be ignored, but a proper CIDR range will be used.   You can check whether the network was created successfully  network list   NOTE  The new networks are created on GCP only after the the cluster provisioning starts with the selected \nnetwork template.", 
            "title": "Infrastructure templates"
        }, 
        {
            "location": "/gcp/#defining-cluster-services_1", 
            "text": "Blueprints  Blueprints are your declarative definition of a Hadoop cluster. These are the same blueprints that are  used by Ambari .  You can use the 3 default blueprints pre-defined in Cloudbreak or you can create your own ones.\nBlueprints can be added from file or URL (an  example blueprint ).  The host groups in the JSON will be mapped to a set of instances when starting the cluster. Besides this the services and\n components will also be installed on the corresponding nodes. Blueprints can be modified later from the Ambari UI.   NOTE  Not necessary to define all the configuration in the blueprint. If a configuration is missing, Ambari will fill that with a default value.   blueprint add --name my-blueprint --description  sample description  --file  the path of the blueprint   Other available options:  --url  the url of the blueprint  --publicInAccount  If it is true, all the users belonging to your account will be able to use this blueprint to create \nclusters, but cannot delete it.  You can check whether the blueprint was created successfully  blueprint list  A blueprint can be exported from a running Ambari cluster that can be reused in Cloudbreak with slight \nmodifications. \nThere is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the \nmodifications have to be done manually.\nWhen the blueprint is exported some configurations are hardcoded for example domain names, memory configurations..etc. that won't be applicable to the Cloudbreak cluster.  Cluster customization  Sometimes it can be useful to  define some custom scripts so called Recipes in Cloudbreak  that run during cluster \ncreation and add some additional functionality.  For example it can be a service you'd like to install but it's not supported by Ambari or some script that \nautomatically downloads some data to the necessary nodes.\nThe most  notable example is Ranger setup :   It has a prerequisite of a running database when Ranger Admin is installing.  A PostgreSQL database can be easily started and configured with a recipe before the blueprint installation starts.   To learn more about these and check the Ranger recipe out, take a look at the  Cluster customization", 
            "title": "Defining cluster services"
        }, 
        {
            "location": "/gcp/#metadata-show", 
            "text": "You can check the stack metadata with  stack metadata --name myawsstack --instancegroup master  Other available options:  --id  In this case you can select a stack with id.  --outputType  In this case you can modify the outputformat of the command (RAW or JSON).", 
            "title": "Metadata show"
        }, 
        {
            "location": "/gcp/#cluster-deployment_1", 
            "text": "After all the cluster resources are configured you can deploy a new HDP cluster. The following sub-sections show \nyou a  basic flow for cluster creation with Cloudbreak Shell .  Select credential  Select one of your previously created GCP credential:  credential select --name my-gcp-credential  Select blueprint  Select one of your previously created blueprint which fits your needs:  blueprint select --name multi-node-hdfs-yarn  Configure instance groups  You must configure instance groups before provisioning. An instance group define a group of nodes with a specified \ntemplate. Usually we create instance groups for host groups in the blueprint.  instancegroup configure --instanceGroup cbgateway --nodecount 1 --templateName minviable-gcp\ninstancegroup configure --instanceGroup master --nodecount 1 --templateName minviable-gcp\ninstancegroup configure --instanceGroup slave_1 --nodecount 1 --templateName minviable-gcp  Other available option:  --templateId  Id of the template  Select network  Select one of your previously created network which fits your needs or a default one:  network select --name default-gcp-network  Select security group  Select one of your previously created security which fits your needs or a default one:  securitygroup select --name all-services-port  Create stack / Create cloud infrastructure  Stack means the running cloud infrastructure that is created based on the instance groups configured earlier \n( credential ,  instancegroups ,  network ,  securitygroup ). Same as in case of the API or UI the new cluster will \nuse your templates and by using GCP will launch your cloud stack. Use the following command to create a \nstack to be used with your Hadoop cluster:  stack create --name mygcpstack --region us-central1  The infrastructure is created asynchronously, the state of the stack can be checked with the stack  show command . If \nit reports AVAILABLE, it means that the virtual machines and the corresponding infrastructure is running at the cloud provider.  Other available option is:  --wait  - in this case the create command will return only after the process has finished.   Create a Hadoop cluster / Cloud provisioning  You are almost done! One more command and your Hadoop cluster is starting!  Cloud provisioning is done once the \ncluster is up and running. The new cluster will use your selected blueprint and install your custom Hadoop cluster \nwith the selected components and services.  cluster create --description  my first cluster   Other available option is  --wait  - in this case the create command will return only after the process has finished.   You are done!  You have several opportunities to check the progress during the infrastructure creation then \nprovisioning:   Cloudbreak uses  Google Cloud Platform  to create the resources - you can check out the resources created by \nCloudbreak on the Compute Engine page of the Google Compute Platform..    Full size  here .   If stack then cluster creation have successfully done, you can check the Ambari Web UI. However you need to know the \nAmbari IP (for example:  http://130.211.163.13:8080 ):   You can get the IP from the CLI as a result ( ambariServerIp 130.211.163.13 ) of the following command:              cluster show   Full size  here .   Besides these you can check the entire progress and the Ambari IP as well on the Cloudbreak Web UI itself. Open the \nnew cluster's  details  and its  Event History  here.    Full size  here .  Stop cluster  You have the ability to  stop your existing stack then its cluster  if you want to suspend the work on it.  Select a stack for example with its name:  stack select --name my-stack  Other available option to define a stack is its  --id .  Every time you should stop the  cluster  first then the  stack . So apply following commands to stop the previously \nselected stack:  cluster stop\nstack stop  Restart cluster  Select your stack that you would like to restart  after this you can apply:  stack start  After the stack has successfully restarted, you can  restart the related cluster as well :  cluster start  Upscale cluster  If you need more instances to your infrastructure, you can  upscale your selected stack :  stack node --ADD --instanceGroup host_group_slave_1 --adjustment 6  Other available option is  --withClusterUpScale  - this indicates also a cluster upscale after the stack upscale. You\n can upscale the related cluster separately if you want to do this:  cluster node --ADD --hostgroup host_group_slave_1 --adjustment 6  Downscale cluster  You also can reduce the number of instances in your infrastructure.  After you selected your stack :  cluster node --REMOVE  --hostgroup host_group_slave_1 --adjustment -2  Other available option is  --withStackDownScale  - this indicates also a stack downscale after the cluster downscale.\n You can downscale the related stack separately if you want to do this:  stack node --REMOVE  --instanceGroup host_group_slave_1 --adjustment -2", 
            "title": "Cluster deployment"
        }, 
        {
            "location": "/gcp/#cluster-termination_1", 
            "text": "You can terminate running or stopped clusters with  stack terminate --name myawsstack  Other available option is  --wait  - in this case the terminate command will return only after the process has finished.    IMPORTANT  Always use Cloudbreak to terminate the cluster. If that fails for some reason, try to delete the \nCloudFormation stack first. Instances are started in an Auto Scaling Group so they may be restarted if you terminate an instance manually!   Sometimes Cloudbreak cannot synchronize it's state with the cluster state at the cloud provider and the cluster can't\n be terminated. In this case the  Forced termination  option on the Cloudbreak Web UI can help to terminate the cluster\n  at the Cloudbreak side.  If it has happened:   You should check the related resources at the AWS CloudFormation  If it is needed you need to manually remove resources from ther", 
            "title": "Cluster termination"
        }, 
        {
            "location": "/gcp/#silent-mode", 
            "text": "With Cloudbreak Shell you can execute script files as well. A script file contains shell commands and can \nbe executed with the  script  cloudbreak shell command  script  your script file   or with the  cbd util cloudbreak-shell-quiet  command  cbd util cloudbreak-shell-quiet   example.sh   IMPORTANT  You have to copy all your files into the  cbd  working directory, what you would like to use in shell.\n For example if your  cbd  working directory is ~/cloudbreak-deployment then copy your script file to here.", 
            "title": "Silent mode"
        }, 
        {
            "location": "/gcp/#example", 
            "text": "The following example creates a hadoop cluster with  hdp-small-default  blueprint on M3Xlarge instances with 2X100G \nattached disks on  default-gcp-network  network using  all-services-port  security group. You should copy your ssh \npublic key file (with name  id_rsa.pub ) and your GCP service account generated private key ( with name  gcp.p12 ) into your  cbd  working \ndirectory and change the  ...  parts with your GCP credential details.  credential create --GCP --description  my credential  --name my-gcp-credential --projectId  your gcp projectid  --serviceAccountId  your GCP service account mail address  --serviceAccountPrivateKeyPath gcp.p12 --sshKeyFile id_rsa.pub\ncredential select --name my-gcp-credential\ntemplate create --GCP --name gcptemplate --description gcp-template --instanceType n1-standard-4 --volumeSize 100 \n--volumeCount 2\nblueprint select --name hdp-small-default\ninstancegroup configure --instanceGroup cbgateway --nodecount 1 --templateName gcptemplate\ninstancegroup configure --instanceGroup host_group_master_1 --nodecount 1 --templateName gcptemplate\ninstancegroup configure --instanceGroup host_group_master_2 --nodecount 1 --templateName gcptemplate\ninstancegroup configure --instanceGroup host_group_master_3 --nodecount 1 --templateName gcptemplate\ninstancegroup configure --instanceGroup host_group_client_1  --nodecount 1 --templateName gcptemplate\ninstancegroup configure --instanceGroup host_group_slave_1 --nodecount 3 --templateName gcptemplate\nnetwork select --name default-gcp-network\nsecuritygroup select --name all-services-port\nstack create --name my-first-stack --region us-central1\ncluster create --description  My first cluster   Congratulations!  Your cluster should now be up and running on this way as well. To learn more about Cloudbreak and \nprovisioning, we have some  interesting insights  for you", 
            "title": "Example"
        }, 
        {
            "location": "/openstack/", 
            "text": "OpenStack Images\n\n\nWe have pre-built cloud images for OpenStack with the Cloudbreak Deployer pre-installed and with Cloudbreak\npre-installed. Following steps will guide you through the launch of the images then the needed configuration.\n\n\n\n\nAlternatively, instead of using the pre-built cloud image, you can install Cloudbreak Deployer on your own VM. See\n \ninstall the Cloudbreak Deployer\n for more information.\n\n\n\n\nPlease make sure you opened the following ports on your \nsecurity group\n:\n\n\n\n\nSSH (22)\n\n\nCloudbreak API (8080)\n\n\nIdentity server (8089)\n\n\nCloudbreak GUI (3000)\n\n\nUser authentication (3001)\n\n\n\n\nOpenStack Image Details\n\n\nCloudbreak Deployer image\n\n\nCloudbreak image\n\n\nImport the image into your OpenStack\n\n\nCloudbreak Deployer import\n\n\nexport OS_IMAGE_NAME=\nadd_a_name_to_your_new_image\n\nexport OS_USERNAME=\nyour_os_user_name\n\nexport OS_AUTH_URL=\nhttp://.../v2.0\n\nexport OS_TENANT_NAME=\nyour_os_tenant_name\n\n\n\n\n\nImport the new image into your OpenStack:\n\n\nglance image-create --name \n$OS_IMAGE_NAME\n --file \n$CBD_LATEST_IMAGE\n --disk-format qcow2 --container-format bare\n--progress\n\n\n\n\n\n\nMinimum and Recommended VM requirements\n:\n 8GB RAM, 10GB disk, 2 cores\n\n\n\n\n\n\nFull size \nhere\n.\n\n\nCloudbreak import\n\n\nexport CB_LATEST_IMAGE_NAME=\nfile_name_of_the_above_cloudbreak_image\n\nexport OS_USERNAME=\nyour_os_user_name\n\nexport OS_AUTH_URL=\nhttp://.../v2.0\n\nexport OS_TENANT_NAME=\nyour_os_tenant_name\n\n\n\n\n\nImport the new image into your OpenStack:\n\n\nglance image-create --name \n$CB_LATEST_IMAGE_NAME\n --file \n$CB_LATEST_IMAGE\n --disk-format qcow2\n--container-format bare --progress\n\n\n\n\nOpenStack Setup\n\n\nCloudbreak Deployer Highlights\n\n\n\n\nThe default SSH username for the OpenStack instances is \ncloudbreak\n.\n\n\nCloudbreak Deployer location is \n/var/lib/cloudbreak-deployment\n on the launched EC2 instance. This is the\n  \ncbd\n root folder.\n\n\nAll \ncbd\n actions must be executed from the \ncbd\n root folder as \ncloudbreak\n user.\n\n\n\n\nSetup Cloudbreak Deployer\n\n\nYou should already have the Cloudbreak Deployer either by \nusing the OpenStack Cloud Images\n or by\n\ninstalling the Cloudbreak Deployer\n manually on your own VM.\n\n\nIf you have your own installed VM, you should check the \nInitialize your Profile\n\nsection here before starting the provisioning.\n\n\nYou can \nconnect to the previously created \ncbd\n VM\n.\n\n\nTo open the \ncloudbreak-deployment\n directory:\n\n\ncd /var/lib/cloudbreak-deployment/\n\n\n\n\nThis is the directory of the configuration files and the supporting binaries for Cloudbreak Deployer.\n\n\nInitialize your Profile\n\n\nFirst initialize \ncbd\n by creating a \nProfile\n file:\n\n\ncbd init\n\n\n\n\nIt will create a \nProfile\n file in the current directory. Please open the \nProfile\n file then check the \nPUBLIC_IP\n.\nThis is mandatory, because of to can access the Cloudbreak UI (called Uluwatu). In some cases the \ncbd\n tool tries to\nguess it. If \ncbd\n cannot get the IP address during the initialization, please set the appropriate value.\n\n\nOpenStack specific configuration\n\n\nMake sure that the \nVM image used by Cloudbreak is imported on your OpenStack\n.\n\n\nStart Cloudbreak Deployer\n\n\nTo start the Cloudbreak application use the following command.\nThis will start all the Docker containers and initialize the application.\n\n\ncbd start\n\n\n\n\n\n\nAt the very first time it will take for a while, because of need to download all the necessary docker images.\n\n\n\n\nThe \ncbd start\n command includes the \ncbd generate\n command which applies the following steps:\n\n\n\n\ncreates the \ndocker-compose.yml\n file that describes the configuration of all the Docker containers needed for the Cloudbreak deployment.\n\n\ncreates the \nuaa.yml\n file that holds the configuration of the identity server used to authenticate users to Cloudbreak.\n\n\n\n\nValidate the started Cloudbreak Deployer\n\n\nAfter the \ncbd start\n command finishes followings are worthy to check:\n\n\n\n\nPre-installed Cloudbreak Deployer version and health.\n\n\n\n\n   cbd doctor\n\n\n\n\n\n\nIn case of \ncbd update\n is needed, please check the related documentation for \nCloudbreak Deployer Update\n.\n\n\n\n\n\n\nStarted Cloudbreak Application logs.\n\n\n\n\n   cbd logs cloudbreak\n\n\n\n\n\n\nCloudbreak should start within a minute - you should see a line like this: \nStarted CloudbreakApplication in 36.823 seconds\n\n\n\n\nProvisioning Prerequisites\n\n\nGenerate a new SSH key\n\n\nAll the instances created by Cloudbreak are configured to allow key-based SSH,\nso you'll need to provide an SSH public key that can be used later to SSH onto the instances in the clusters you'll create with Cloudbreak.\nYou can use one of your existing keys or you can generate a new one.\n\n\nTo generate a new SSH keypair:\n\n\nssh-keygen -t rsa -b 4096 -C \nyour_email@example.com\n\n# Creates a new ssh key, using the provided email as a label\n# Generating public/private rsa key pair.\n\n\n\n\n# Enter file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter]\nYou'll be asked to enter a passphrase, but you can leave it empty.\n\n# Enter passphrase (empty for no passphrase): [Type a passphrase]\n# Enter same passphrase again: [Type passphrase again]\n\n\n\n\nAfter you enter a passphrase the keypair is generated. The output should look something like below.\n\n\n# Your identification has been saved in /Users/you/.ssh/id_rsa.\n# Your public key has been saved in /Users/you/.ssh/id_rsa.pub.\n# The key fingerprint is:\n# 01:0f:f4:3b:ca:85:sd:17:sd:7d:sd:68:9d:sd:a2:sd your_email@example.com\n\n\n\n\nLater you'll need to pass the \n.pub\n file's contents to Cloudbreak and use the private part to SSH to the instances\n\n\nProvisioning via Browser\n\n\nYou can log into the Cloudbreak application at \nhttp://\nPublic_IP\n:3000/\n.\n\n\nThe main goal of the Cloudbreak UI is to easily create clusters on your own cloud provider account.\nThis description details the OpenStack setup - if you'd like to use a different cloud provider check out its manual.\n\n\nThis document explains the four steps that need to be followed to create Cloudbreak clusters from the UI:\n\n\n\n\nconnect your OpenStack account with Cloudbreak\n\n\ncreate some template resources on the UI that describe the infrastructure of your clusters\n\n\ncreate a blueprint that describes the HDP services in your clusters and add some recipes for customization\n\n\nlaunch the cluster itself based on these resources\n\n\n\n\n\n\nIMPORTANT\n Make sure that you have sufficient qouta (CPU, network, etc) for the requested cluster size\n\n\n\n\nSetting up OpenStack credentials\n\n\nCloudbreak works by connecting your OpenStack account through so called \nCredentials\n, and then uses these credentials\n to create resources on your behalf. The credentials can be configured on the \nmanage credentials\n panel on the \nCloudbreak Dashboard.\n\n\nTo create a new OpenStack credential follow these steps:\n\n\n\n\nSelect the \nKeystone Version\n. For instance, select the \nv2\n\n\nFill out the new credential \nName\n\n\nOnly alphanumeric and lowercase characters (min 5, max 100 characters) can be applied\n\n\n\n\n\n\nCopy your OpenStack user name to the \nUser\n field\n\n\nCopy your OpenStack user password to the \nPassword\n field\n\n\nCopy your OpenStack tenant name to the \nTenant Name\n field\n\n\nCopy your OpenStack identity service (Keystone) endpoint (e.g. http://PUBLIC_IP:5000/v2.0) to the \nEndpoint\n field\n\n\nCopy your SSH public key to the \nSSH public key\n field\n\n\nThe SSH public key must be in OpenSSH format and it's private keypair can be used later to \nSSH onto every \ninstance\n of every cluster you'll create with this credential.\n\n\nThe \nSSH username\n for the OpenStack instances is \ncloudbreak\n.\n\n\n\n\n\n\n\n\n\n\nAny other parameter is optional here. You can read more about Keystone v3 \nhere\n.\n\n\nPublic in account\n means that all the users belonging to your account will be able to use this credential to create \nclusters, but cannot delete it.\n\n\n\n\n\n\nFull size \nhere\n.\n/sub\n\n\nInfrastructure templates\n\n\nAfter your OpenStack account is linked to Cloudbreak you can start creating resource templates that describe your \nclusters' infrastructure:\n\n\n\n\ntemplates\n\n\nnetworks\n\n\nsecurity groups\n\n\n\n\nWhen you create one of the above resource, \nCloudbreak does not make any requests to OpenStack. Resources are only \ncreated on OpenStack after the \ncreate cluster\n button has pushed.\n These templates are saved to Cloudbreak's \ndatabase and can be reused with multiple clusters to describe the infrastructure.\n\n\nTemplates\n\n\nTemplates describe the \ninstances of your cluster\n - the instance type and the attached volumes. A typical setup is\n to combine multiple templates in a cluster for the different types of nodes. For example you may want to attach multiple\n large disks to the datanodes or have memory optimized instances for Spark nodes.\n\n\nThe instance templates can be configured on the \nmanage templates\n panel on the Cloudbreak Dashboard.\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to use this resource to create clusters, but cannot delete it.\n\n\nNetworks\n\n\nYour clusters can be created in their own \nnetworks\n or in one of your already existing one. If you choose an \nexisting network, it is possible to create a new subnet within the network. The subnet's IP range must be defined in \nthe \nSubnet (CIDR)\n field using the general CIDR notation. Here you can read more about \nOpenStack networking\n.\n\n\nCustom OpenStack Network\n\n\nIf you'd like to deploy a cluster to your OpenStack network you'll have to \ncreate a new network\n template on the \n\nmanage networks\n panel on the Cloudbreak Dashboard.\n\n\n\n\n\"Before launching an instance, you must create the necessary virtual network infrastructure...an instance uses a \npublic provider virtual network that connects to the physical network infrastructure...This network includes a DHCP \nserver that provides IP addresses to instances...The admin or other privileged user must create this network because \nit connects directly to the physical network infrastructure.\"\n\n\nHere you can read more about OpenStack \nvirtual network\n and \npublic provider network\n.\n\n\n\n\nYou have the following options to create a new network:\n\n\n\n\nCreate a new network and a new subnet\n: Every time a cluster is created with this kind of network setup a new network and a new subnet with the specified IP range will be created for the instances on OpenStack.\n\n\nCreate a new subnet in an existing network\n: Use this kind of network setup if you already have a network on OpenStack where you'd like to put the Cloudbreak created cluster but you'd like to have a separate subnet for it.\n\n\nUse an existing subnet in an existing network\n: Use this kind of network setup if you have an existing network with one or more subnets on OpenStack and you'd like to start the instances of a cluster in one of those subnets.\n\n\n\n\nExplanation of the parameters:\n\n\n\n\nName\n the name of the new network\n\n\nit must be between 5 and 100 characters long\n\n\nStarts with a lowercase alphabetic character\n\n\nCan contain lowercase alphanumeric and hyphens only\n\n\n\n\n\n\nSubnet (CIDR)\n Copy your OpenStack public network's subnet with CIDR block to the \nSubnet (CIDR)\n field\n\n\nPublic Network ID\n Copy your OpenStack public network ID to the \nPublic Network ID\n field\n\n\nVirtual Network Identifier\n This must be an ID of an existing OpenStack virtual network.\n\n\nRouter Identifier\n Your virtual network router ID (must be provided in case of existing virtual network).\n\n\nSubnet Identifier\n Your subnet ID within your virtual network. If the identifier is provided, the \nSubnet (CIDR)\n will be ignored.\n\n\n\n\n\n\nIMPORTANT\n Please make sure the defined subnet here doesn't overlap with any of your already deployed subnet in the\n network, because of the validation only happens after the cluster creation starts.\n\n\nIn case of existing subnet make sure you have enough room within your network space for the new instances. The \nprovided subnet CIDR will be ignored, but a proper CIDR range will be used.\n\n\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to use this network template \nto create clusters, but cannot delete it.\n\n\n\n\nNOTE\n The new networks are created on OpenStack only after the the cluster provisioning starts with the selected \nnetwork template.\n\n\n\n\n\n\nFull size \nhere\n.\n\n\nSecurity groups\n\n\nSecurity group templates are very similar to the \nSecurity Groups on OpenStack\n. \nThey describe the allowed inbound traffic \nto the instances in the cluster.\n Currently only one security group template can be selected for a Cloudbreak cluster \nand all the instances have a public IP address so all the instances in the cluster will belong to the same security \ngroup. This may change in a later release.\n\n\nDefault Security Group\n\n\nYou can also use the two pre-defined security groups in Cloudbreak.\n\n\nonly-ssh-and-ssl:\n all ports are locked down except for SSH and gateway HTTPS (you can't access Hadoop services \noutside of the network):\n\n\n\n\nSSH (22)\n\n\nHTTPS (443)\n\n\n\n\nall-services-port:\n all Hadoop services and SSH, gateway HTTPS are accessible by default:\n\n\n\n\nSSH (22)\n\n\nHTTPS (443)\n\n\nAmbari (8080)\n\n\nConsul (8500)\n\n\nNN (50070)\n\n\nRM Web (8088)\n\n\nScheduler (8030RM)\n\n\nIPC (8050RM)\n\n\nJob history server (19888)\n\n\nHBase master (60000)\n\n\nHBase master web (60010)\n\n\nHBase RS (16020)\n\n\nHBase RS info (60030)\n\n\nFalcon (15000)\n\n\nStorm (8744)\n\n\nHive metastore (9083)\n\n\nHive server (10000)\n\n\nHive server HTTP (10001)\n\n\nAccumulo master (9999)\n\n\nAccumulo Tserver (9997)\n\n\nAtlas (21000)\n\n\nKNOX (8443)\n\n\nOozie (11000)\n\n\nSpark HS (18080)\n\n\nNM Web (8042)\n\n\nZeppelin WebSocket (9996)\n\n\nZeppelin UI (9995)\n\n\nKibana (3080)\n\n\nElasticsearch (9200)\n\n\n\n\nCustom Security Group\n\n\nYou can define your own security group by adding all the ports, protocols and CIDR range you'd like to use. The rules\n defined here doesn't need to contain the internal rules, those are automatically added by Cloudbreak to the security\n  group on OpenStack.\n\n\n\n\nIMPORTANT\n 443 and 22 ports needs to be there in every security group otherwise Cloudbreak won't be able to communicate with the \nprovisioned cluster\n\n\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to use this security group \ntemplate to create clusters, but cannot delete it.\n\n\n\n\nNOTE\n The security groups are created on OpenStack only after the cluster provisioning starts with the selected \nsecurity group template.\n\n\n\n\n\n\nFull size \nhere\n.\n/sub\n\n\nDefining cluster services\n\n\nBlueprints\n\n\nBlueprints are your declarative definition of a Hadoop cluster. These are the same blueprints that are \nused by Ambari\n.\n\n\nYou can use the 3 default blueprints pre-defined in Cloudbreak or you can create your own ones.\nBlueprints can be added from file, URL (an \nexample blueprint\n) or the \nwhole JSON can be written in the \nJSON text\n box.\n\n\nThe host groups in the JSON will be mapped to a set of instances when starting the cluster. Besides this the services and\n components will also be installed on the corresponding nodes. Blueprints can be modified later from the Ambari UI.\n\n\n\n\nNOTE\n Not necessary to define all the configuration in the blueprint. If a configuration is missing, Ambari will \nfill that with a default value.\n\n\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to use this blueprint to \ncreate clusters, but cannot delete or modify it.\n\n\n\n\nFull size \nhere\n.\n\n\nA blueprint can be exported from a running Ambari cluster that can be reused in Cloudbreak with slight \nmodifications.\n\nThere is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the \nmodifications have to be done manually.\nWhen the blueprint is exported some configurations are hardcoded for example domain names, memory configurations...etc. that won't be applicable to the Cloudbreak cluster\n\n\nCluster customization\n\n\nSometimes it can be useful to \ndefine some custom scripts so called Recipes in Cloudbreak\n that run during cluster \ncreation and add some additional functionality.\n\n\nFor example it can be a service you'd like to install but it's not supported by Ambari or some script that \nautomatically downloads some data to the necessary nodes.\nThe most \nnotable example is Ranger setup\n:\n\n\n\n\nIt has a prerequisite of a running database when Ranger Admin is installing.\n\n\nA PostgreSQL database can be easily started and configured with a recipe before the blueprint installation starts.\n\n\n\n\nTo learn more about these and check the Ranger recipe out, take a look at the \nCluster customization\n\n\nCluster deployment\n\n\nAfter all the cluster resources are configured you can deploy a new HDP cluster.\n\n\nHere is a \nbasic flow for cluster creation on Cloudbreak Web UI\n:\n\n\n\n\nStart by selecting a previously created OpenStack credential in the header.\n\n\n\n\n\n\nFull size \nhere\n.\n\n\n\n\nOpen \ncreate cluster\n\n\n\n\nConfigure Cluster\n tab\n\n\n\n\nFill out the new cluster \nname\n\n\nThe name must be between 5 and 40 characters long and must satisfy the followings:\n\n\nStarts with a lowercase alphabetic character\n\n\nCan contain lowercase alphanumeric and hyphens only\n\n\n\n\n\n\n\n\n\n\nSelect one of your \nRegion\n where you like your cluster be provisioned\n\n\nClick on the \nSetup Network and Security\n button\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to see the created cluster on\n the UI, but cannot delete or modify it.\n\n\n\n\n\n\n\n\nSetup Network and Security\n tab\n\n\n\n\nSelect one of your previously created networks\n\n\nSelect one of the security groups\n\n\nClick on the \nChoose Blueprint\n button\n\n\nIf \nEnable security\n is checked as well, Cloudbreak will install Key Distribution Center (KDC) and the cluster will \nbe Kerberized. See more about it in the \nKerberos\n section of this documentation.\n\n\n\n\n\n\n\n\nChoose Blueprint\n tab\n\n\n\n\nSelect one of the blueprints\n\n\nAfter you've selected a \nBlueprint\n, you should be able to configure:\n\n\nthe templates\n\n\nthe number of nodes for all of the host groups in the blueprint\n\n\nthe recipes for nodes\n\n\n\n\n\n\nClick on the \nReview and Launch\n button\n\n\n\n\nReview and Launch\n tab\n\n\n\n\nAfter the \ncreate and start cluster\n button has clicked Cloudbreak will start to create the cluster's resources on \n your OpenStack account.\n\n\n\n\nCloudbreak uses \nOpenStack\n to create the resources - you can check out the resources created by Cloudbreak\n on the \nInstances\n page of your OpenStack \nProject\n.\n\n\n\nFull size \nhere\n.\n\n\nBesides these you can check the progress on the Cloudbreak Web UI itself if you open the new cluster's \nEvent History\n.\n\n\n\nFull size \nhere\n.\n\n\nAdvanced options\n\n\nConsul server count\n the number of Consul servers (add number), by default is 3. It varies with the cluster size.\n\n\nConnector Variant\n Cloudbreak provides two implementation for creating OpenStack cluster\n\n\n\n\nHEAT\n using \nHEAT\n template to create the resources\n\n\nNATIVE\n using API calls to create the resources\n\n\n\n\n\n\nThe HEAT variant utilizes the Heat templating to launch a stack, but the NATIVE variant starts the cluster\n  by using a sequence of API calls without Heat to achieve the same result, although both of them are using the same \n  authentication and credential management.\n\n\n\n\nMinimum cluster size\n The provisioning strategy in case of the cloud provider cannot allocate all the requested nodes.\n\n\nValidate blueprint\n This is selected by default. Cloudbreak validates the Ambari blueprint in this case.\n\n\nShipyard enabled cluster\n This is selected by default. Cloudbreak will start a \nShipyard\n container which helps you to manage your containers.\n\n\nConfig recommendation strategy\n Strategy for configuration recommendations how will be applied. Recommended \nconfigurations gathered by the response of the stack advisor. \n\n\n\n\nNEVER_APPLY\n               Configuration recommendations are ignored with this option.\n\n\nONLY_STACK_DEFAULTS_APPLY\n Applies only on the default configurations for all included services.\n\n\nALWAYS_APPLY\n              Applies on all configuration properties.\n\n\n\n\nStart LDAP and configure SSSD\n Enables the \nSystem Security Services Daemon\n configuration.\n\n\nCluster termination\n\n\nYou can terminate running or stopped clusters with the \nterminate\n button in the cluster details.\n\n\n\n\nIMPORTANT\n Always use Cloudbreak to terminate the cluster. If that fails for some reason, try to delete the \nOpenStack instances first. Instances are started in an Auto Scaling Group so they may be restarted if you terminate an \ninstance manually!\n\n\n\n\nSometimes Cloudbreak cannot synchronize it's state with the cluster state at the cloud provider and the cluster can't\n be terminated. In this case the \nForced termination\n option can help to terminate the cluster at the Cloudbreak \n side. \nIf it has happened:\n\n\n\n\nYou should check the related resources at the OpenStack\n\n\nIf it is needed you need to manually remove resources from there\n\n\n\n\n\n\nFull size \nhere\n.\n/sub\n\n\nInteractive mode / Cloudbreak Shell\n\n\nThe goal with the Cloudbreak Shell (Cloudbreak CLI) was to provide an interactive command line tool which supports:\n\n\n\n\nall functionality available through the REST API or Cloudbreak Web UI\n\n\nmakes possible complete automation of management task via scripts\n\n\ncontext aware command availability\n\n\ntab completion\n\n\nrequired/optional parameter support\n\n\nhint command to guide you on the usual path\n\n\n\n\nStart Cloudbreak Shell\n\n\nTo start the Cloudbreak CLI use the following commands:\n\n\n\n\nOpen your \ncloudbreak-deployment\n directory if it is needed. For example:\n\n\n\n\n   cd cloudbreak-deployment\n\n\n\n\n\n\nStart the \ncbd\n from here if it is needed\n\n\n\n\n   cbd start\n\n\n\n\n\n\nIn the root of your \ncloudbreak-deployment\n folder apply:\n\n\n\n\n   cbd util cloudbreak-shell\n\n\n\n\n\n\nAt the very first time it will take for a while, because of need to download all the necessary docker images.\n\n\n\n\nThis will launch the Cloudbreak shell inside a Docker container then it is ready to use.\n\n\n\nFull size \nhere\n.\n\n\n\n\nIMPORTANT You have to copy all your files into the \ncbd\n working directory, what you would like to use in shell.\n For \nexample if your \ncbd\n working directory is \n~/cloudbreak-deployment\n then copy your \nblueprint JSON, public ssh key \nfile...etc.\n to here. You can refer to these files with their names from the shell.\n\n\n\n\nAutocomplete and hints\n\n\nCloudbreak Shell helps to you with \nhint messages\n from the very beginning, for example:\n\n\ncloudbreak-shell\nhint\nHint: Add a blueprint with the 'blueprint add' command or select an existing one with 'blueprint select'\ncloudbreak-shell\n\n\n\n\n\nBeyond this you can use the \nautocompletion (double-TAB)\n as well:\n\n\ncloudbreak-shell\ncredential create --\ncredential create --AWS          credential create --AZURE        credential create --EC2          credential create --GCP          credential create --OPENSTACK\n\n\n\n\nProvisioning via CLI\n\n\nSetting up OpenStack credential\n\n\nCloudbreak works by connecting your OpenStack account through so called Credentials, and then uses these credentials to \ncreate resources on your behalf. Credentials can be configured with the following command for example:\n\n\ncredential create --OPENSTACK --name my-os-credential --description \nsample description\n --userName \nOpenStack username\n --password \nOpenStack password\n --tenantName \nOpenStack tenant name\n --endPoint \nOpenStack Identity Service (Keystone) endpoint\n --sshKeyString \nssh-rsa AAAAB****etc\n\n\n\n\n\n\n\nNOTE\n that Cloudbreak \ndoes not set your cloud user details\n - we work around the concept of \nOpenStack's \nauthentication\n. You should have already valid OpenStack credentials. You can \nfind further details \nhere\n.\n\n\n\n\nAlternatives to provide \nSSH Key\n:\n\n\n\n\nyou can upload your public key from an url: \n\u2014sshKeyUrl\n \n\n\nor you can add the path of your public key: \n\u2014sshKeyPath\n\n\n\n\nYou can check whether the credential was created successfully\n\n\ncredential list\n\n\n\n\nYou can switch between your existing credentials\n\n\ncredential select --name my-os-credential\n\n\n\n\nInfrastructure templates\n\n\nAfter your OpenStack account is linked to Cloudbreak you can start creating resource templates that describe your \nclusters' infrastructure:\n\n\n\n\nsecurity groups\n\n\nnetworks\n\n\ntemplates\n\n\n\n\nWhen you create one of the above resource, \nCloudbreak does not make any requests to OpenStack. Resources are only \ncreated on OpenStack after the \ncluster create\n has applied.\n These templates are saved to Cloudbreak's database and\n can be reused with multiple clusters to describe the infrastructure.\n\n\nTemplates\n\n\nTemplates describe the \ninstances of your cluster\n - the instance type and the attached volumes. A typical setup is\n to combine multiple templates in a cluster for the different types of nodes. For example you may want to attach multiple\n large disks to the datanodes or have memory optimized instances for Spark nodes.\n\n\nA template can be used repeatedly to create identical copies of the same stack (or to use as a foundation to start a \nnew stack). Templates can be configured with the following command for example:\n\n\ntemplate create --OPENSTACK --name my-os-template --description \nsample description\n --instanceType m1.medium \n--volumeSize 100 --volumeCount 1\n\n\n\n\nOther available option here is \n--publicInAccount\n. If it is true, all the users belonging to your account will be able\n to use this template to create clusters, but cannot delete it.\n\n\nYou can check whether the template was created successfully\n\n\ntemplate list\n\n\n\n\nNetworks\n\n\nYour clusters can be created in their own \nnetworks\n or in one of your already existing one. If you choose an \nexisting network, it is possible to create a new subnet within the network. The subnet's IP range must be defined in \nthe \nSubnet (CIDR)\n field using the general CIDR notation. Here you can read more about \nOpenStack networking\n.\n\n\nCustom OpenStack Network\n\n\nIf you'd like to deploy a cluster to your OpenStack network you'll have to \ncreate a new network\n template.\n\n\nA network also can be used repeatedly to create identical copies of the same stack (or to use as a foundation to \nstart a new stack).\n\n\n\n\n\"Before launching an instance, you must create the necessary virtual network infrastructure...an instance uses a \npublic provider virtual network that connects to the physical network infrastructure...This network includes a DHCP \nserver that provides IP addresses to instances...The admin or other privileged user must create this network because \nit connects directly to the physical network infrastructure.\"\n\n\nHere you can read more about OpenStack \nvirtual network\n and \npublic provider network\n.\n\n\n\n\nnetwork create --OPENSTACK --name my-os-network --description openstack-network --publicNetID \nid of an OpenStack \npublic network\n --subnet 10.0.0.0/16\n\n\n\n\n\n\nIMPORTANT\n\n\n\n\nIn case of existing subnet all three parameters must be provided, with new subnet only two are required.\n\n\nPlease make sure the defined subnet here doesn't overlap with any of your already deployed subnet in the\n network, because of the validation only happens after the cluster creation starts.\n\n\nIn case of existing subnet make sure you have enough room within your network space for the new instances. The \nprovided subnet CIDR will be ignored, but a proper CIDR range will be used.\n\n\n\n\nNOTE\n The new networks are created on OpenStack only after the the cluster provisioning starts with the selected \nnetwork template.\n\n\n\n\nOther available options here:\n\n\n--networkId\n This must be an ID of an existing OpenStack virtual network.\n\n\n--routerId\n Your virtual network router ID (must be provided in case of existing virtual network).\n\n\n--subnetId\n Your subnet ID within your virtual network. If the identifier is provided, the \nSubnet \n(CIDR)\n will be ignored. Leave it blank if you'd like to create a new subnet within the virtual network with the \nprovided \nSubnet (CIDR)\n range.\n\n\n--publicInAccount\n If it is true, all the users belonging to your account will be able to use this template to create clusters, but cannot delete it.\n\n\nYou can check whether the network was created successfully\n\n\nnetwork list\n\n\n\n\nDefining cluster services\n\n\nBlueprints\n\n\nBlueprints are your declarative definition of a Hadoop cluster. These are the same blueprints that are \nused by Ambari\n.\n\n\nYou can use the 3 default blueprints pre-defined in Cloudbreak or you can create your own ones.\nBlueprints can be added from file or URL (an \nexample blueprint\n).\n\n\nThe host groups in the JSON will be mapped to a set of instances when starting the cluster. Besides this the services and\n components will also be installed on the corresponding nodes. Blueprints can be modified later from the Ambari UI.\n\n\n\n\nNOTE\n Not necessary to define all the configuration in the blueprint. If a configuration is missing, Ambari will fill that with a default value.\n\n\n\n\nblueprint add --name my-blueprint --description \nsample description\n --file \nthe path of the blueprint\n\n\n\n\n\nOther available options:\n\n\n--url\n the url of the blueprint\n\n\n--publicInAccount\n If it is true, all the users belonging to your account will be able to use this blueprint to create \nclusters, but cannot delete it.\n\n\nYou can check whether the blueprint was created successfully\n\n\nblueprint list\n\n\n\n\nA blueprint can be exported from a running Ambari cluster that can be reused in Cloudbreak with slight \nmodifications.\n\nThere is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the \nmodifications have to be done manually.\nWhen the blueprint is exported some configurations are hardcoded for example domain names, memory configurations..etc. that won't be applicable to the Cloudbreak cluster.\n\n\nCluster customization\n\n\nSometimes it can be useful to \ndefine some custom scripts so called Recipes in Cloudbreak\n that run during cluster \ncreation and add some additional functionality.\n\n\nFor example it can be a service you'd like to install but it's not supported by Ambari or some script that \nautomatically downloads some data to the necessary nodes.\nThe most \nnotable example is Ranger setup\n:\n\n\n\n\nIt has a prerequisite of a running database when Ranger Admin is installing.\n\n\nA PostgreSQL database can be easily started and configured with a recipe before the blueprint installation starts.\n\n\n\n\nTo learn more about these and check the Ranger recipe out, take a look at the \nCluster customization\n\n\nMetadata show\n\n\nYou can check the stack metadata with\n\n\nstack metadata --name myawsstack --instancegroup master\n\n\n\n\nOther available options:\n\n\n--id\n In this case you can select a stack with id.\n\n\n--outputType\n In this case you can modify the outputformat of the command (RAW or JSON). \n\n\nCluster deployment\n\n\nAfter all the cluster resources are configured you can deploy a new HDP cluster. The following sub-sections show \nyou a \nbasic flow for cluster creation with Cloudbreak Shell\n.\n\n\nSelect credential\n\n\nSelect one of your previously created OpenStack credential:\n\n\ncredential select --name my-os-credential\n\n\n\n\nSelect blueprint\n\n\nSelect one of your previously created blueprint which fits your needs:\n\n\nblueprint select --name multi-node-hdfs-yarn\n\n\n\n\nConfigure instance groups\n\n\nYou must configure instance groups before provisioning. An instance group define a group of nodes with a specified \ntemplate. Usually we create instance groups for host groups in the blueprint.\n\n\ninstancegroup configure --instanceGroup cbgateway --nodecount 1 --templateName my-os-template\ninstancegroup configure --instanceGroup master --nodecount 1 --templateName my-os-template\ninstancegroup configure --instanceGroup slave_1 --nodecount 1 --templateName my-os-template\n\n\n\n\nOther available option:\n\n\n--templateId\n Id of the template\n\n\nSelect network\n\n\nSelect one of your previously created network which fits your needs or a default one:\n\n\nnetwork select --name my-os-network\n\n\n\n\nSelect security group\n\n\nSelect one of your previously created security which fits your needs or a default one:\n\n\nsecuritygroup select --name all-services-port\n\n\n\n\nCreate stack / Create cloud infrastructure\n\n\nStack means the running cloud infrastructure that is created based on the instance groups configured earlier \n(\ncredential\n, \ninstancegroups\n, \nnetwork\n, \nsecuritygroup\n). Same as in case of the API or UI the new cluster will \nuse your templates and by using OpenStack will launch your cloud stack. Use the following command to create a \nstack to be used with your Hadoop cluster:\n\n\nstack create --name myosstack --region local\n\n\n\n\nThe infrastructure is created asynchronously, the state of the stack can be checked with the stack \nshow command\n. If \nit reports AVAILABLE, it means that the virtual machines and the corresponding infrastructure is running at the cloud provider.\n\n\nOther available option is:\n\n\n--wait\n - in this case the create command will return only after the process has finished. \n\n\nCreate a Hadoop cluster / Cloud provisioning\n\n\nYou are almost done! One more command and your Hadoop cluster is starting!\n Cloud provisioning is done once the \ncluster is up and running. The new cluster will use your selected blueprint and install your custom Hadoop cluster \nwith the selected components and services.\n\n\ncluster create --description \nmy first cluster\n\n\n\n\n\nOther available option is \n--wait\n - in this case the create command will return only after the process has finished. \n\n\nYou are done!\n You have several opportunities to check the progress during the infrastructure creation then \nprovisioning:\n\n\n\n\nCloudbreak uses \nOpenStack\n to create the resources - you can check out the resources created by Cloudbreak on\n the OpenStack Console Instances page.\n\n\n\n\nFor example:\n\n\n\nFull size \nhere\n.\n\n\n\n\nIf stack then cluster creation have successfully done, you can check the Ambari Web UI. However you need to know the \nAmbari IP (for example: \nhttp://172.16.252.59:8080\n): \n\n\nYou can get the IP from the CLI as a result (\nambariServerIp 172.16.252.59\n) of the following command:\n\n\n\n\n\n\n\n\n         cluster show\n\n\n\n\nFor example:\n\n\n\nFull size \nhere\n.\n\n\n\n\nBesides these you can check the entire progress and the Ambari IP as well on the Cloudbreak Web UI itself. Open the \nnew cluster's \ndetails\n and its \nEvent History\n here.\n\n\n\n\nFor example:\n\n\n\nFull size \nhere\n.\n\n\nStop cluster\n\n\nYou have the ability to \nstop your existing stack then its cluster\n if you want to suspend the work on it.\n\n\nSelect a stack for example with its name:\n\n\nstack select --name my-stack\n\n\n\n\nOther available option to define a stack is its \n--id\n.\n\n\nEvery time you should stop the \ncluster\n first then the \nstack\n. So apply following commands to stop the previously \nselected stack:\n\n\ncluster stop\nstack stop\n\n\n\n\nRestart cluster\n\n\nSelect your stack that you would like to restart\n after this you can apply:\n\n\nstack start\n\n\n\n\nAfter the stack has successfully restarted, you can \nrestart the related cluster as well\n:\n\n\ncluster start\n\n\n\n\nUpscale cluster\n\n\nIf you need more instances to your infrastructure, you can \nupscale your selected stack\n:\n\n\nstack node --ADD --instanceGroup host_group_slave_1 --adjustment 6\n\n\n\n\nOther available option is \n--withClusterUpScale\n - this indicates also a cluster upscale after the stack upscale. You\n can upscale the related cluster separately if you want to do this:\n\n\ncluster node --ADD --hostgroup host_group_slave_1 --adjustment 6\n\n\n\n\nDownscale cluster\n\n\nYou also can reduce the number of instances in your infrastructure. \nAfter you selected your stack\n:\n\n\ncluster node --REMOVE  --hostgroup host_group_slave_1 --adjustment -2\n\n\n\n\nOther available option is \n--withStackDownScale\n - this indicates also a stack downscale after the cluster downscale.\n You can downscale the related stack separately if you want to do this:\n\n\nstack node --REMOVE  --instanceGroup host_group_slave_1 --adjustment -2\n\n\n\n\nCluster termination\n\n\nYou can terminate running or stopped clusters with\n\n\nstack terminate --name myawsstack\n\n\n\n\nOther available option is \n--wait\n - in this case the terminate command will return only after the process has finished. \n\n\n\n\nIMPORTANT\n Always use Cloudbreak to terminate the cluster. If that fails for some reason, try to delete the \nCloudFormation stack first. Instances are started in an Auto Scaling Group so they may be restarted if you terminate an instance manually!\n\n\n\n\nSometimes Cloudbreak cannot synchronize it's state with the cluster state at the cloud provider and the cluster can't\n be terminated. In this case the \nForced termination\n option on the Cloudbreak Web UI can help to terminate the cluster\n  at the Cloudbreak side. \nIf it has happened:\n\n\n\n\nYou should check the related resources at the AWS CloudFormation\n\n\nIf it is needed you need to manually remove resources from ther\n\n\n\n\nSilent mode\n\n\nWith Cloudbreak Shell you can execute script files as well. A script file contains shell commands and can \nbe executed with the \nscript\n cloudbreak shell command\n\n\nscript \nyour script file\n\n\n\n\n\nor with the \ncbd util cloudbreak-shell-quiet\n command\n\n\ncbd util cloudbreak-shell-quiet \n example.sh\n\n\n\n\n\n\nIMPORTANT\n You have to copy all your files into the \ncbd\n working directory, what you would like to use in shell.\n For example if your \ncbd\n working directory is ~/cloudbreak-deployment then copy your script file to here.\n\n\n\n\nExample\n\n\nThe following example creates a hadoop cluster with \nhdp-small-default\n blueprint on \nm1.large\n instances with 2X100G\n attached disks on \nosnetwork\n network using \nall-services-port\n security group. You should copy your ssh public key \n file into your \ncbd\n working directory with name \nid_rsa.pub\n and change the \n...\n parts with your OpenStack \n credential and network details.\n\n\ncredential create --OPENSTACK --name my-os-credential --description \ncredentail description\n --userName \nOpenStack username\n --password \nOpenStack password\n --tenantName \nOpenStack tenant name\n --endPoint \nOpenStack Identity Service (Keystone) endpoint\n --sshKeyPath \npath of your public SSH key file\n\ncredential select --name my-os-credential\ntemplate create --OPENSTACK --name ostemplate --description openstack-template --instanceType m1.large --volumeSize 100 --volumeCount 2\nblueprint select --name hdp-small-default\ninstancegroup configure --instanceGroup cbgateway --nodecount 1 --templateName ostemplate\ninstancegroup configure --instanceGroup host_group_master_1 --nodecount 1 --templateName ostemplate\ninstancegroup configure --instanceGroup host_group_master_2 --nodecount 1 --templateName ostemplate\ninstancegroup configure --instanceGroup host_group_master_3 --nodecount 1 --templateName ostemplate\ninstancegroup configure --instanceGroup host_group_client_1  --nodecount 1 --templateName ostemplate\ninstancegroup configure --instanceGroup host_group_slave_1 --nodecount 3 --templateName ostemplate\nnetwork create --OPENSTACK --name osnetwork --description openstack-network --publicNetID \nid of an OpenStack public network\n --subnet 10.0.0.0/16\nnetwork select --name osnetwork\nsecuritygroup select --name all-services-port\nstack create --name my-first-stack --region local\ncluster create --description \nMy first cluster\n\n\n\n\n\nCongratulations!\n Your cluster should now be up and running on this way as well. To learn more about Cloudbreak and \nprovisioning, we have some \ninteresting insights\n for you.", 
            "title": "OpenStack"
        }, 
        {
            "location": "/openstack/#openstack-images", 
            "text": "We have pre-built cloud images for OpenStack with the Cloudbreak Deployer pre-installed and with Cloudbreak\npre-installed. Following steps will guide you through the launch of the images then the needed configuration.   Alternatively, instead of using the pre-built cloud image, you can install Cloudbreak Deployer on your own VM. See\n  install the Cloudbreak Deployer  for more information.   Please make sure you opened the following ports on your  security group :   SSH (22)  Cloudbreak API (8080)  Identity server (8089)  Cloudbreak GUI (3000)  User authentication (3001)", 
            "title": "OpenStack Images"
        }, 
        {
            "location": "/openstack/#openstack-image-details", 
            "text": "", 
            "title": "OpenStack Image Details"
        }, 
        {
            "location": "/openstack/#cloudbreak-deployer-image", 
            "text": "", 
            "title": "Cloudbreak Deployer image"
        }, 
        {
            "location": "/openstack/#cloudbreak-image", 
            "text": "", 
            "title": "Cloudbreak image"
        }, 
        {
            "location": "/openstack/#import-the-image-into-your-openstack", 
            "text": "", 
            "title": "Import the image into your OpenStack"
        }, 
        {
            "location": "/openstack/#cloudbreak-deployer-import", 
            "text": "export OS_IMAGE_NAME= add_a_name_to_your_new_image \nexport OS_USERNAME= your_os_user_name \nexport OS_AUTH_URL= http://.../v2.0 \nexport OS_TENANT_NAME= your_os_tenant_name   Import the new image into your OpenStack:  glance image-create --name  $OS_IMAGE_NAME  --file  $CBD_LATEST_IMAGE  --disk-format qcow2 --container-format bare\n--progress   Minimum and Recommended VM requirements :  8GB RAM, 10GB disk, 2 cores    Full size  here .", 
            "title": "Cloudbreak Deployer import"
        }, 
        {
            "location": "/openstack/#cloudbreak-import", 
            "text": "export CB_LATEST_IMAGE_NAME= file_name_of_the_above_cloudbreak_image \nexport OS_USERNAME= your_os_user_name \nexport OS_AUTH_URL= http://.../v2.0 \nexport OS_TENANT_NAME= your_os_tenant_name   Import the new image into your OpenStack:  glance image-create --name  $CB_LATEST_IMAGE_NAME  --file  $CB_LATEST_IMAGE  --disk-format qcow2\n--container-format bare --progress", 
            "title": "Cloudbreak import"
        }, 
        {
            "location": "/openstack/#openstack-setup", 
            "text": "Cloudbreak Deployer Highlights   The default SSH username for the OpenStack instances is  cloudbreak .  Cloudbreak Deployer location is  /var/lib/cloudbreak-deployment  on the launched EC2 instance. This is the\n   cbd  root folder.  All  cbd  actions must be executed from the  cbd  root folder as  cloudbreak  user.", 
            "title": "OpenStack Setup"
        }, 
        {
            "location": "/openstack/#setup-cloudbreak-deployer", 
            "text": "You should already have the Cloudbreak Deployer either by  using the OpenStack Cloud Images  or by installing the Cloudbreak Deployer  manually on your own VM.  If you have your own installed VM, you should check the  Initialize your Profile \nsection here before starting the provisioning.  You can  connect to the previously created  cbd  VM .  To open the  cloudbreak-deployment  directory:  cd /var/lib/cloudbreak-deployment/  This is the directory of the configuration files and the supporting binaries for Cloudbreak Deployer.", 
            "title": "Setup Cloudbreak Deployer"
        }, 
        {
            "location": "/openstack/#initialize-your-profile", 
            "text": "First initialize  cbd  by creating a  Profile  file:  cbd init  It will create a  Profile  file in the current directory. Please open the  Profile  file then check the  PUBLIC_IP .\nThis is mandatory, because of to can access the Cloudbreak UI (called Uluwatu). In some cases the  cbd  tool tries to\nguess it. If  cbd  cannot get the IP address during the initialization, please set the appropriate value.", 
            "title": "Initialize your Profile"
        }, 
        {
            "location": "/openstack/#openstack-specific-configuration", 
            "text": "Make sure that the  VM image used by Cloudbreak is imported on your OpenStack .", 
            "title": "OpenStack specific configuration"
        }, 
        {
            "location": "/openstack/#start-cloudbreak-deployer", 
            "text": "To start the Cloudbreak application use the following command.\nThis will start all the Docker containers and initialize the application.  cbd start   At the very first time it will take for a while, because of need to download all the necessary docker images.   The  cbd start  command includes the  cbd generate  command which applies the following steps:   creates the  docker-compose.yml  file that describes the configuration of all the Docker containers needed for the Cloudbreak deployment.  creates the  uaa.yml  file that holds the configuration of the identity server used to authenticate users to Cloudbreak.", 
            "title": "Start Cloudbreak Deployer"
        }, 
        {
            "location": "/openstack/#validate-the-started-cloudbreak-deployer", 
            "text": "After the  cbd start  command finishes followings are worthy to check:   Pre-installed Cloudbreak Deployer version and health.      cbd doctor   In case of  cbd update  is needed, please check the related documentation for  Cloudbreak Deployer Update .    Started Cloudbreak Application logs.      cbd logs cloudbreak   Cloudbreak should start within a minute - you should see a line like this:  Started CloudbreakApplication in 36.823 seconds", 
            "title": "Validate the started Cloudbreak Deployer"
        }, 
        {
            "location": "/openstack/#provisioning-prerequisites", 
            "text": "", 
            "title": "Provisioning Prerequisites"
        }, 
        {
            "location": "/openstack/#generate-a-new-ssh-key", 
            "text": "All the instances created by Cloudbreak are configured to allow key-based SSH,\nso you'll need to provide an SSH public key that can be used later to SSH onto the instances in the clusters you'll create with Cloudbreak.\nYou can use one of your existing keys or you can generate a new one.  To generate a new SSH keypair:  ssh-keygen -t rsa -b 4096 -C  your_email@example.com \n# Creates a new ssh key, using the provided email as a label\n# Generating public/private rsa key pair.  # Enter file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter]\nYou'll be asked to enter a passphrase, but you can leave it empty.\n\n# Enter passphrase (empty for no passphrase): [Type a passphrase]\n# Enter same passphrase again: [Type passphrase again]  After you enter a passphrase the keypair is generated. The output should look something like below.  # Your identification has been saved in /Users/you/.ssh/id_rsa.\n# Your public key has been saved in /Users/you/.ssh/id_rsa.pub.\n# The key fingerprint is:\n# 01:0f:f4:3b:ca:85:sd:17:sd:7d:sd:68:9d:sd:a2:sd your_email@example.com  Later you'll need to pass the  .pub  file's contents to Cloudbreak and use the private part to SSH to the instances", 
            "title": "Generate a new SSH key"
        }, 
        {
            "location": "/openstack/#provisioning-via-browser", 
            "text": "You can log into the Cloudbreak application at  http:// Public_IP :3000/ .  The main goal of the Cloudbreak UI is to easily create clusters on your own cloud provider account.\nThis description details the OpenStack setup - if you'd like to use a different cloud provider check out its manual.  This document explains the four steps that need to be followed to create Cloudbreak clusters from the UI:   connect your OpenStack account with Cloudbreak  create some template resources on the UI that describe the infrastructure of your clusters  create a blueprint that describes the HDP services in your clusters and add some recipes for customization  launch the cluster itself based on these resources    IMPORTANT  Make sure that you have sufficient qouta (CPU, network, etc) for the requested cluster size", 
            "title": "Provisioning via Browser"
        }, 
        {
            "location": "/openstack/#setting-up-openstack-credentials", 
            "text": "Cloudbreak works by connecting your OpenStack account through so called  Credentials , and then uses these credentials\n to create resources on your behalf. The credentials can be configured on the  manage credentials  panel on the \nCloudbreak Dashboard.  To create a new OpenStack credential follow these steps:   Select the  Keystone Version . For instance, select the  v2  Fill out the new credential  Name  Only alphanumeric and lowercase characters (min 5, max 100 characters) can be applied    Copy your OpenStack user name to the  User  field  Copy your OpenStack user password to the  Password  field  Copy your OpenStack tenant name to the  Tenant Name  field  Copy your OpenStack identity service (Keystone) endpoint (e.g. http://PUBLIC_IP:5000/v2.0) to the  Endpoint  field  Copy your SSH public key to the  SSH public key  field  The SSH public key must be in OpenSSH format and it's private keypair can be used later to  SSH onto every \ninstance  of every cluster you'll create with this credential.  The  SSH username  for the OpenStack instances is  cloudbreak .      Any other parameter is optional here. You can read more about Keystone v3  here .  Public in account  means that all the users belonging to your account will be able to use this credential to create \nclusters, but cannot delete it.    Full size  here . /sub", 
            "title": "Setting up OpenStack credentials"
        }, 
        {
            "location": "/openstack/#infrastructure-templates", 
            "text": "After your OpenStack account is linked to Cloudbreak you can start creating resource templates that describe your \nclusters' infrastructure:   templates  networks  security groups   When you create one of the above resource,  Cloudbreak does not make any requests to OpenStack. Resources are only \ncreated on OpenStack after the  create cluster  button has pushed.  These templates are saved to Cloudbreak's \ndatabase and can be reused with multiple clusters to describe the infrastructure.  Templates  Templates describe the  instances of your cluster  - the instance type and the attached volumes. A typical setup is\n to combine multiple templates in a cluster for the different types of nodes. For example you may want to attach multiple\n large disks to the datanodes or have memory optimized instances for Spark nodes.  The instance templates can be configured on the  manage templates  panel on the Cloudbreak Dashboard.  If  Public in account  is checked all the users belonging to your account will be able to use this resource to create clusters, but cannot delete it.  Networks  Your clusters can be created in their own  networks  or in one of your already existing one. If you choose an \nexisting network, it is possible to create a new subnet within the network. The subnet's IP range must be defined in \nthe  Subnet (CIDR)  field using the general CIDR notation. Here you can read more about  OpenStack networking .  Custom OpenStack Network  If you'd like to deploy a cluster to your OpenStack network you'll have to  create a new network  template on the  manage networks  panel on the Cloudbreak Dashboard.   \"Before launching an instance, you must create the necessary virtual network infrastructure...an instance uses a \npublic provider virtual network that connects to the physical network infrastructure...This network includes a DHCP \nserver that provides IP addresses to instances...The admin or other privileged user must create this network because \nit connects directly to the physical network infrastructure.\"  Here you can read more about OpenStack  virtual network  and  public provider network .   You have the following options to create a new network:   Create a new network and a new subnet : Every time a cluster is created with this kind of network setup a new network and a new subnet with the specified IP range will be created for the instances on OpenStack.  Create a new subnet in an existing network : Use this kind of network setup if you already have a network on OpenStack where you'd like to put the Cloudbreak created cluster but you'd like to have a separate subnet for it.  Use an existing subnet in an existing network : Use this kind of network setup if you have an existing network with one or more subnets on OpenStack and you'd like to start the instances of a cluster in one of those subnets.   Explanation of the parameters:   Name  the name of the new network  it must be between 5 and 100 characters long  Starts with a lowercase alphabetic character  Can contain lowercase alphanumeric and hyphens only    Subnet (CIDR)  Copy your OpenStack public network's subnet with CIDR block to the  Subnet (CIDR)  field  Public Network ID  Copy your OpenStack public network ID to the  Public Network ID  field  Virtual Network Identifier  This must be an ID of an existing OpenStack virtual network.  Router Identifier  Your virtual network router ID (must be provided in case of existing virtual network).  Subnet Identifier  Your subnet ID within your virtual network. If the identifier is provided, the  Subnet (CIDR)  will be ignored.    IMPORTANT  Please make sure the defined subnet here doesn't overlap with any of your already deployed subnet in the\n network, because of the validation only happens after the cluster creation starts.  In case of existing subnet make sure you have enough room within your network space for the new instances. The \nprovided subnet CIDR will be ignored, but a proper CIDR range will be used.   If  Public in account  is checked all the users belonging to your account will be able to use this network template \nto create clusters, but cannot delete it.   NOTE  The new networks are created on OpenStack only after the the cluster provisioning starts with the selected \nnetwork template.    Full size  here .  Security groups  Security group templates are very similar to the  Security Groups on OpenStack .  They describe the allowed inbound traffic \nto the instances in the cluster.  Currently only one security group template can be selected for a Cloudbreak cluster \nand all the instances have a public IP address so all the instances in the cluster will belong to the same security \ngroup. This may change in a later release.  Default Security Group  You can also use the two pre-defined security groups in Cloudbreak.  only-ssh-and-ssl:  all ports are locked down except for SSH and gateway HTTPS (you can't access Hadoop services \noutside of the network):   SSH (22)  HTTPS (443)   all-services-port:  all Hadoop services and SSH, gateway HTTPS are accessible by default:   SSH (22)  HTTPS (443)  Ambari (8080)  Consul (8500)  NN (50070)  RM Web (8088)  Scheduler (8030RM)  IPC (8050RM)  Job history server (19888)  HBase master (60000)  HBase master web (60010)  HBase RS (16020)  HBase RS info (60030)  Falcon (15000)  Storm (8744)  Hive metastore (9083)  Hive server (10000)  Hive server HTTP (10001)  Accumulo master (9999)  Accumulo Tserver (9997)  Atlas (21000)  KNOX (8443)  Oozie (11000)  Spark HS (18080)  NM Web (8042)  Zeppelin WebSocket (9996)  Zeppelin UI (9995)  Kibana (3080)  Elasticsearch (9200)   Custom Security Group  You can define your own security group by adding all the ports, protocols and CIDR range you'd like to use. The rules\n defined here doesn't need to contain the internal rules, those are automatically added by Cloudbreak to the security\n  group on OpenStack.   IMPORTANT  443 and 22 ports needs to be there in every security group otherwise Cloudbreak won't be able to communicate with the \nprovisioned cluster   If  Public in account  is checked all the users belonging to your account will be able to use this security group \ntemplate to create clusters, but cannot delete it.   NOTE  The security groups are created on OpenStack only after the cluster provisioning starts with the selected \nsecurity group template.    Full size  here . /sub", 
            "title": "Infrastructure templates"
        }, 
        {
            "location": "/openstack/#defining-cluster-services", 
            "text": "Blueprints  Blueprints are your declarative definition of a Hadoop cluster. These are the same blueprints that are  used by Ambari .  You can use the 3 default blueprints pre-defined in Cloudbreak or you can create your own ones.\nBlueprints can be added from file, URL (an  example blueprint ) or the \nwhole JSON can be written in the  JSON text  box.  The host groups in the JSON will be mapped to a set of instances when starting the cluster. Besides this the services and\n components will also be installed on the corresponding nodes. Blueprints can be modified later from the Ambari UI.   NOTE  Not necessary to define all the configuration in the blueprint. If a configuration is missing, Ambari will \nfill that with a default value.   If  Public in account  is checked all the users belonging to your account will be able to use this blueprint to \ncreate clusters, but cannot delete or modify it.   Full size  here .  A blueprint can be exported from a running Ambari cluster that can be reused in Cloudbreak with slight \nmodifications. \nThere is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the \nmodifications have to be done manually.\nWhen the blueprint is exported some configurations are hardcoded for example domain names, memory configurations...etc. that won't be applicable to the Cloudbreak cluster  Cluster customization  Sometimes it can be useful to  define some custom scripts so called Recipes in Cloudbreak  that run during cluster \ncreation and add some additional functionality.  For example it can be a service you'd like to install but it's not supported by Ambari or some script that \nautomatically downloads some data to the necessary nodes.\nThe most  notable example is Ranger setup :   It has a prerequisite of a running database when Ranger Admin is installing.  A PostgreSQL database can be easily started and configured with a recipe before the blueprint installation starts.   To learn more about these and check the Ranger recipe out, take a look at the  Cluster customization", 
            "title": "Defining cluster services"
        }, 
        {
            "location": "/openstack/#cluster-deployment", 
            "text": "After all the cluster resources are configured you can deploy a new HDP cluster.  Here is a  basic flow for cluster creation on Cloudbreak Web UI :   Start by selecting a previously created OpenStack credential in the header.    Full size  here .   Open  create cluster   Configure Cluster  tab   Fill out the new cluster  name  The name must be between 5 and 40 characters long and must satisfy the followings:  Starts with a lowercase alphabetic character  Can contain lowercase alphanumeric and hyphens only      Select one of your  Region  where you like your cluster be provisioned  Click on the  Setup Network and Security  button  If  Public in account  is checked all the users belonging to your account will be able to see the created cluster on\n the UI, but cannot delete or modify it.     Setup Network and Security  tab   Select one of your previously created networks  Select one of the security groups  Click on the  Choose Blueprint  button  If  Enable security  is checked as well, Cloudbreak will install Key Distribution Center (KDC) and the cluster will \nbe Kerberized. See more about it in the  Kerberos  section of this documentation.     Choose Blueprint  tab   Select one of the blueprints  After you've selected a  Blueprint , you should be able to configure:  the templates  the number of nodes for all of the host groups in the blueprint  the recipes for nodes    Click on the  Review and Launch  button   Review and Launch  tab   After the  create and start cluster  button has clicked Cloudbreak will start to create the cluster's resources on \n your OpenStack account.   Cloudbreak uses  OpenStack  to create the resources - you can check out the resources created by Cloudbreak\n on the  Instances  page of your OpenStack  Project .  Full size  here .  Besides these you can check the progress on the Cloudbreak Web UI itself if you open the new cluster's  Event History .  Full size  here .  Advanced options  Consul server count  the number of Consul servers (add number), by default is 3. It varies with the cluster size.  Connector Variant  Cloudbreak provides two implementation for creating OpenStack cluster   HEAT  using  HEAT  template to create the resources  NATIVE  using API calls to create the resources    The HEAT variant utilizes the Heat templating to launch a stack, but the NATIVE variant starts the cluster\n  by using a sequence of API calls without Heat to achieve the same result, although both of them are using the same \n  authentication and credential management.   Minimum cluster size  The provisioning strategy in case of the cloud provider cannot allocate all the requested nodes.  Validate blueprint  This is selected by default. Cloudbreak validates the Ambari blueprint in this case.  Shipyard enabled cluster  This is selected by default. Cloudbreak will start a  Shipyard  container which helps you to manage your containers.  Config recommendation strategy  Strategy for configuration recommendations how will be applied. Recommended \nconfigurations gathered by the response of the stack advisor.    NEVER_APPLY                Configuration recommendations are ignored with this option.  ONLY_STACK_DEFAULTS_APPLY  Applies only on the default configurations for all included services.  ALWAYS_APPLY               Applies on all configuration properties.   Start LDAP and configure SSSD  Enables the  System Security Services Daemon  configuration.", 
            "title": "Cluster deployment"
        }, 
        {
            "location": "/openstack/#cluster-termination", 
            "text": "You can terminate running or stopped clusters with the  terminate  button in the cluster details.   IMPORTANT  Always use Cloudbreak to terminate the cluster. If that fails for some reason, try to delete the \nOpenStack instances first. Instances are started in an Auto Scaling Group so they may be restarted if you terminate an \ninstance manually!   Sometimes Cloudbreak cannot synchronize it's state with the cluster state at the cloud provider and the cluster can't\n be terminated. In this case the  Forced termination  option can help to terminate the cluster at the Cloudbreak \n side.  If it has happened:   You should check the related resources at the OpenStack  If it is needed you need to manually remove resources from there    Full size  here . /sub", 
            "title": "Cluster termination"
        }, 
        {
            "location": "/openstack/#interactive-mode-cloudbreak-shell", 
            "text": "The goal with the Cloudbreak Shell (Cloudbreak CLI) was to provide an interactive command line tool which supports:   all functionality available through the REST API or Cloudbreak Web UI  makes possible complete automation of management task via scripts  context aware command availability  tab completion  required/optional parameter support  hint command to guide you on the usual path", 
            "title": "Interactive mode / Cloudbreak Shell"
        }, 
        {
            "location": "/openstack/#start-cloudbreak-shell", 
            "text": "To start the Cloudbreak CLI use the following commands:   Open your  cloudbreak-deployment  directory if it is needed. For example:      cd cloudbreak-deployment   Start the  cbd  from here if it is needed      cbd start   In the root of your  cloudbreak-deployment  folder apply:      cbd util cloudbreak-shell   At the very first time it will take for a while, because of need to download all the necessary docker images.   This will launch the Cloudbreak shell inside a Docker container then it is ready to use.  Full size  here .   IMPORTANT You have to copy all your files into the  cbd  working directory, what you would like to use in shell.  For \nexample if your  cbd  working directory is  ~/cloudbreak-deployment  then copy your  blueprint JSON, public ssh key \nfile...etc.  to here. You can refer to these files with their names from the shell.", 
            "title": "Start Cloudbreak Shell"
        }, 
        {
            "location": "/openstack/#autocomplete-and-hints", 
            "text": "Cloudbreak Shell helps to you with  hint messages  from the very beginning, for example:  cloudbreak-shell hint\nHint: Add a blueprint with the 'blueprint add' command or select an existing one with 'blueprint select'\ncloudbreak-shell   Beyond this you can use the  autocompletion (double-TAB)  as well:  cloudbreak-shell credential create --\ncredential create --AWS          credential create --AZURE        credential create --EC2          credential create --GCP          credential create --OPENSTACK", 
            "title": "Autocomplete and hints"
        }, 
        {
            "location": "/openstack/#provisioning-via-cli", 
            "text": "", 
            "title": "Provisioning via CLI"
        }, 
        {
            "location": "/openstack/#setting-up-openstack-credential", 
            "text": "Cloudbreak works by connecting your OpenStack account through so called Credentials, and then uses these credentials to \ncreate resources on your behalf. Credentials can be configured with the following command for example:  credential create --OPENSTACK --name my-os-credential --description  sample description  --userName  OpenStack username  --password  OpenStack password  --tenantName  OpenStack tenant name  --endPoint  OpenStack Identity Service (Keystone) endpoint  --sshKeyString  ssh-rsa AAAAB****etc    NOTE  that Cloudbreak  does not set your cloud user details  - we work around the concept of  OpenStack's \nauthentication . You should have already valid OpenStack credentials. You can \nfind further details  here .   Alternatives to provide  SSH Key :   you can upload your public key from an url:  \u2014sshKeyUrl    or you can add the path of your public key:  \u2014sshKeyPath   You can check whether the credential was created successfully  credential list  You can switch between your existing credentials  credential select --name my-os-credential", 
            "title": "Setting up OpenStack credential"
        }, 
        {
            "location": "/openstack/#infrastructure-templates_1", 
            "text": "After your OpenStack account is linked to Cloudbreak you can start creating resource templates that describe your \nclusters' infrastructure:   security groups  networks  templates   When you create one of the above resource,  Cloudbreak does not make any requests to OpenStack. Resources are only \ncreated on OpenStack after the  cluster create  has applied.  These templates are saved to Cloudbreak's database and\n can be reused with multiple clusters to describe the infrastructure.  Templates  Templates describe the  instances of your cluster  - the instance type and the attached volumes. A typical setup is\n to combine multiple templates in a cluster for the different types of nodes. For example you may want to attach multiple\n large disks to the datanodes or have memory optimized instances for Spark nodes.  A template can be used repeatedly to create identical copies of the same stack (or to use as a foundation to start a \nnew stack). Templates can be configured with the following command for example:  template create --OPENSTACK --name my-os-template --description  sample description  --instanceType m1.medium \n--volumeSize 100 --volumeCount 1  Other available option here is  --publicInAccount . If it is true, all the users belonging to your account will be able\n to use this template to create clusters, but cannot delete it.  You can check whether the template was created successfully  template list  Networks  Your clusters can be created in their own  networks  or in one of your already existing one. If you choose an \nexisting network, it is possible to create a new subnet within the network. The subnet's IP range must be defined in \nthe  Subnet (CIDR)  field using the general CIDR notation. Here you can read more about  OpenStack networking .  Custom OpenStack Network  If you'd like to deploy a cluster to your OpenStack network you'll have to  create a new network  template.  A network also can be used repeatedly to create identical copies of the same stack (or to use as a foundation to \nstart a new stack).   \"Before launching an instance, you must create the necessary virtual network infrastructure...an instance uses a \npublic provider virtual network that connects to the physical network infrastructure...This network includes a DHCP \nserver that provides IP addresses to instances...The admin or other privileged user must create this network because \nit connects directly to the physical network infrastructure.\"  Here you can read more about OpenStack  virtual network  and  public provider network .   network create --OPENSTACK --name my-os-network --description openstack-network --publicNetID  id of an OpenStack \npublic network  --subnet 10.0.0.0/16   IMPORTANT   In case of existing subnet all three parameters must be provided, with new subnet only two are required.  Please make sure the defined subnet here doesn't overlap with any of your already deployed subnet in the\n network, because of the validation only happens after the cluster creation starts.  In case of existing subnet make sure you have enough room within your network space for the new instances. The \nprovided subnet CIDR will be ignored, but a proper CIDR range will be used.   NOTE  The new networks are created on OpenStack only after the the cluster provisioning starts with the selected \nnetwork template.   Other available options here:  --networkId  This must be an ID of an existing OpenStack virtual network.  --routerId  Your virtual network router ID (must be provided in case of existing virtual network).  --subnetId  Your subnet ID within your virtual network. If the identifier is provided, the  Subnet \n(CIDR)  will be ignored. Leave it blank if you'd like to create a new subnet within the virtual network with the \nprovided  Subnet (CIDR)  range.  --publicInAccount  If it is true, all the users belonging to your account will be able to use this template to create clusters, but cannot delete it.  You can check whether the network was created successfully  network list", 
            "title": "Infrastructure templates"
        }, 
        {
            "location": "/openstack/#defining-cluster-services_1", 
            "text": "Blueprints  Blueprints are your declarative definition of a Hadoop cluster. These are the same blueprints that are  used by Ambari .  You can use the 3 default blueprints pre-defined in Cloudbreak or you can create your own ones.\nBlueprints can be added from file or URL (an  example blueprint ).  The host groups in the JSON will be mapped to a set of instances when starting the cluster. Besides this the services and\n components will also be installed on the corresponding nodes. Blueprints can be modified later from the Ambari UI.   NOTE  Not necessary to define all the configuration in the blueprint. If a configuration is missing, Ambari will fill that with a default value.   blueprint add --name my-blueprint --description  sample description  --file  the path of the blueprint   Other available options:  --url  the url of the blueprint  --publicInAccount  If it is true, all the users belonging to your account will be able to use this blueprint to create \nclusters, but cannot delete it.  You can check whether the blueprint was created successfully  blueprint list  A blueprint can be exported from a running Ambari cluster that can be reused in Cloudbreak with slight \nmodifications. \nThere is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the \nmodifications have to be done manually.\nWhen the blueprint is exported some configurations are hardcoded for example domain names, memory configurations..etc. that won't be applicable to the Cloudbreak cluster.  Cluster customization  Sometimes it can be useful to  define some custom scripts so called Recipes in Cloudbreak  that run during cluster \ncreation and add some additional functionality.  For example it can be a service you'd like to install but it's not supported by Ambari or some script that \nautomatically downloads some data to the necessary nodes.\nThe most  notable example is Ranger setup :   It has a prerequisite of a running database when Ranger Admin is installing.  A PostgreSQL database can be easily started and configured with a recipe before the blueprint installation starts.   To learn more about these and check the Ranger recipe out, take a look at the  Cluster customization", 
            "title": "Defining cluster services"
        }, 
        {
            "location": "/openstack/#metadata-show", 
            "text": "You can check the stack metadata with  stack metadata --name myawsstack --instancegroup master  Other available options:  --id  In this case you can select a stack with id.  --outputType  In this case you can modify the outputformat of the command (RAW or JSON).", 
            "title": "Metadata show"
        }, 
        {
            "location": "/openstack/#cluster-deployment_1", 
            "text": "After all the cluster resources are configured you can deploy a new HDP cluster. The following sub-sections show \nyou a  basic flow for cluster creation with Cloudbreak Shell .  Select credential  Select one of your previously created OpenStack credential:  credential select --name my-os-credential  Select blueprint  Select one of your previously created blueprint which fits your needs:  blueprint select --name multi-node-hdfs-yarn  Configure instance groups  You must configure instance groups before provisioning. An instance group define a group of nodes with a specified \ntemplate. Usually we create instance groups for host groups in the blueprint.  instancegroup configure --instanceGroup cbgateway --nodecount 1 --templateName my-os-template\ninstancegroup configure --instanceGroup master --nodecount 1 --templateName my-os-template\ninstancegroup configure --instanceGroup slave_1 --nodecount 1 --templateName my-os-template  Other available option:  --templateId  Id of the template  Select network  Select one of your previously created network which fits your needs or a default one:  network select --name my-os-network  Select security group  Select one of your previously created security which fits your needs or a default one:  securitygroup select --name all-services-port  Create stack / Create cloud infrastructure  Stack means the running cloud infrastructure that is created based on the instance groups configured earlier \n( credential ,  instancegroups ,  network ,  securitygroup ). Same as in case of the API or UI the new cluster will \nuse your templates and by using OpenStack will launch your cloud stack. Use the following command to create a \nstack to be used with your Hadoop cluster:  stack create --name myosstack --region local  The infrastructure is created asynchronously, the state of the stack can be checked with the stack  show command . If \nit reports AVAILABLE, it means that the virtual machines and the corresponding infrastructure is running at the cloud provider.  Other available option is:  --wait  - in this case the create command will return only after the process has finished.   Create a Hadoop cluster / Cloud provisioning  You are almost done! One more command and your Hadoop cluster is starting!  Cloud provisioning is done once the \ncluster is up and running. The new cluster will use your selected blueprint and install your custom Hadoop cluster \nwith the selected components and services.  cluster create --description  my first cluster   Other available option is  --wait  - in this case the create command will return only after the process has finished.   You are done!  You have several opportunities to check the progress during the infrastructure creation then \nprovisioning:   Cloudbreak uses  OpenStack  to create the resources - you can check out the resources created by Cloudbreak on\n the OpenStack Console Instances page.   For example:  Full size  here .   If stack then cluster creation have successfully done, you can check the Ambari Web UI. However you need to know the \nAmbari IP (for example:  http://172.16.252.59:8080 ):   You can get the IP from the CLI as a result ( ambariServerIp 172.16.252.59 ) of the following command:              cluster show  For example:  Full size  here .   Besides these you can check the entire progress and the Ambari IP as well on the Cloudbreak Web UI itself. Open the \nnew cluster's  details  and its  Event History  here.   For example:  Full size  here .  Stop cluster  You have the ability to  stop your existing stack then its cluster  if you want to suspend the work on it.  Select a stack for example with its name:  stack select --name my-stack  Other available option to define a stack is its  --id .  Every time you should stop the  cluster  first then the  stack . So apply following commands to stop the previously \nselected stack:  cluster stop\nstack stop  Restart cluster  Select your stack that you would like to restart  after this you can apply:  stack start  After the stack has successfully restarted, you can  restart the related cluster as well :  cluster start  Upscale cluster  If you need more instances to your infrastructure, you can  upscale your selected stack :  stack node --ADD --instanceGroup host_group_slave_1 --adjustment 6  Other available option is  --withClusterUpScale  - this indicates also a cluster upscale after the stack upscale. You\n can upscale the related cluster separately if you want to do this:  cluster node --ADD --hostgroup host_group_slave_1 --adjustment 6  Downscale cluster  You also can reduce the number of instances in your infrastructure.  After you selected your stack :  cluster node --REMOVE  --hostgroup host_group_slave_1 --adjustment -2  Other available option is  --withStackDownScale  - this indicates also a stack downscale after the cluster downscale.\n You can downscale the related stack separately if you want to do this:  stack node --REMOVE  --instanceGroup host_group_slave_1 --adjustment -2", 
            "title": "Cluster deployment"
        }, 
        {
            "location": "/openstack/#cluster-termination_1", 
            "text": "You can terminate running or stopped clusters with  stack terminate --name myawsstack  Other available option is  --wait  - in this case the terminate command will return only after the process has finished.    IMPORTANT  Always use Cloudbreak to terminate the cluster. If that fails for some reason, try to delete the \nCloudFormation stack first. Instances are started in an Auto Scaling Group so they may be restarted if you terminate an instance manually!   Sometimes Cloudbreak cannot synchronize it's state with the cluster state at the cloud provider and the cluster can't\n be terminated. In this case the  Forced termination  option on the Cloudbreak Web UI can help to terminate the cluster\n  at the Cloudbreak side.  If it has happened:   You should check the related resources at the AWS CloudFormation  If it is needed you need to manually remove resources from ther", 
            "title": "Cluster termination"
        }, 
        {
            "location": "/openstack/#silent-mode", 
            "text": "With Cloudbreak Shell you can execute script files as well. A script file contains shell commands and can \nbe executed with the  script  cloudbreak shell command  script  your script file   or with the  cbd util cloudbreak-shell-quiet  command  cbd util cloudbreak-shell-quiet   example.sh   IMPORTANT  You have to copy all your files into the  cbd  working directory, what you would like to use in shell.\n For example if your  cbd  working directory is ~/cloudbreak-deployment then copy your script file to here.", 
            "title": "Silent mode"
        }, 
        {
            "location": "/openstack/#example", 
            "text": "The following example creates a hadoop cluster with  hdp-small-default  blueprint on  m1.large  instances with 2X100G\n attached disks on  osnetwork  network using  all-services-port  security group. You should copy your ssh public key \n file into your  cbd  working directory with name  id_rsa.pub  and change the  ...  parts with your OpenStack \n credential and network details.  credential create --OPENSTACK --name my-os-credential --description  credentail description  --userName  OpenStack username  --password  OpenStack password  --tenantName  OpenStack tenant name  --endPoint  OpenStack Identity Service (Keystone) endpoint  --sshKeyPath  path of your public SSH key file \ncredential select --name my-os-credential\ntemplate create --OPENSTACK --name ostemplate --description openstack-template --instanceType m1.large --volumeSize 100 --volumeCount 2\nblueprint select --name hdp-small-default\ninstancegroup configure --instanceGroup cbgateway --nodecount 1 --templateName ostemplate\ninstancegroup configure --instanceGroup host_group_master_1 --nodecount 1 --templateName ostemplate\ninstancegroup configure --instanceGroup host_group_master_2 --nodecount 1 --templateName ostemplate\ninstancegroup configure --instanceGroup host_group_master_3 --nodecount 1 --templateName ostemplate\ninstancegroup configure --instanceGroup host_group_client_1  --nodecount 1 --templateName ostemplate\ninstancegroup configure --instanceGroup host_group_slave_1 --nodecount 3 --templateName ostemplate\nnetwork create --OPENSTACK --name osnetwork --description openstack-network --publicNetID  id of an OpenStack public network  --subnet 10.0.0.0/16\nnetwork select --name osnetwork\nsecuritygroup select --name all-services-port\nstack create --name my-first-stack --region local\ncluster create --description  My first cluster   Congratulations!  Your cluster should now be up and running on this way as well. To learn more about Cloudbreak and \nprovisioning, we have some  interesting insights  for you.", 
            "title": "Example"
        }, 
        {
            "location": "/api/", 
            "text": "API documentation\n\n\nCloudbreak is a RESTful application development platform with the goal of helping developers to build solutions for deploying HDP clusters in different environments. Once it is deployed in your favourite servlet container it exposes a REST API allowing to span up Hadoop clusters of arbitary sizes and cloud providers.\n\n\nThe \nAPI documentation\n is generated from the code using \nSwagger\n.", 
            "title": "API"
        }, 
        {
            "location": "/api/#api-documentation", 
            "text": "Cloudbreak is a RESTful application development platform with the goal of helping developers to build solutions for deploying HDP clusters in different environments. Once it is deployed in your favourite servlet container it exposes a REST API allowing to span up Hadoop clusters of arbitary sizes and cloud providers.  The  API documentation  is generated from the code using  Swagger .", 
            "title": "API documentation"
        }, 
        {
            "location": "/shell/", 
            "text": "Cloudbreak Shell\n\n\nThe goal with the CLI was to provide an interactive command line tool which supports:\n\n\n\n\nall functionality available through the REST API or Cloudbreak web UI\n\n\nmakes possible complete automation of management task via \nscripts\n\n\ncontext aware command availability\n\n\ntab completion\n\n\nrequired/optional parameter support\n\n\nhint\n command to guide you on the usual path\n\n\n\n\nInstall and start Cloudbreak shell\n\n\nYou have a few options to give it a try:\n\n\n\n\nuse Cloudreak deployer - \nrecommended\n\n\nuse our prepared docker image\n\n\nbuild it from source\n\n\n\n\n\n\nStarting cloudbreak shell using cloudbreak deployer\n\n\nStart the shell with \ncbd util cloudbreak-shell\n. This will launch the Cloudbreak shell inside a Docker container and you are ready to start using it.\n\n\n\n\nStarting Cloudbreak shell with our prepared docker image\n\n\nYou can find the docker image and its documentation \nhere\n.\n\n\n\n\nBuild from source\n\n\nIf want to use the code or extend it with new commands follow the steps below. You will need:\n- jdk 1.7\n\n\ngit clone https://github.com/sequenceiq/cloudbreak.git\ncd cloudbreak/shell\n../gradlew clean build\n\n\n\n\n\n\nNote\n\nIn case you use the hosted version of Cloudbreak you should use the \nlatest-release.sh\n to get the right version of the CLI.\n\n\n\n\nStart Cloudbreak-shell from the built source\n\n\nUsage:\n  java -jar cloudbreak-shell-0.5-SNAPSHOT.jar                  : Starts Cloudbreak Shell in interactive mode.\n  java -jar cloudbreak-shell-0.5-SNAPSHOT.jar --cmdfile=\nFILE\n : Cloudbreak executes commands read from the file.\n\nOptions:\n  --cloudbreak.address=\nhttp[s]://HOSTNAME:PORT\n  Address of the Cloudbreak Server [default: https://cloudbreak-api.sequenceiq.com].\n  --identity.address=\nhttp[s]://HOSTNAME:PORT\n    Address of the SequenceIQ identity server [default: https://identity.sequenceiq.com].\n  --sequenceiq.user=\nUSER\n                        Username of the SequenceIQ user [default: user@sequenceiq.com].\n  --sequenceiq.password=\nPASSWORD\n                Password of the SequenceIQ user [default: password].\n\nNote:\n  You should specify at least your username and password.\n\n\n\n\nOnce you are connected you can start to create a cluster. If you are lost and need guidance through the process you can use \nhint\n. You can always use \nTAB\n for completion.\n\n\n\n\nNote\n\nAll commands are \ncontext aware\n - they are available only when it makes sense - this way you are never confused and guided by the system on the right path.\n\n\n\n\nProvider specific documentations\n\n\n\n\nAWS\n\n\nAzure\n\n\nGCP\n\n\nOpenStack\n\n\n\n\nor you can find a more detailed documentation about Cloudbreak-shell in its \nGithub repositiry\n.", 
            "title": "CLI/Shell"
        }, 
        {
            "location": "/shell/#cloudbreak-shell", 
            "text": "The goal with the CLI was to provide an interactive command line tool which supports:   all functionality available through the REST API or Cloudbreak web UI  makes possible complete automation of management task via  scripts  context aware command availability  tab completion  required/optional parameter support  hint  command to guide you on the usual path", 
            "title": "Cloudbreak Shell"
        }, 
        {
            "location": "/shell/#install-and-start-cloudbreak-shell", 
            "text": "You have a few options to give it a try:   use Cloudreak deployer -  recommended  use our prepared docker image  build it from source", 
            "title": "Install and start Cloudbreak shell"
        }, 
        {
            "location": "/shell/#starting-cloudbreak-shell-using-cloudbreak-deployer", 
            "text": "Start the shell with  cbd util cloudbreak-shell . This will launch the Cloudbreak shell inside a Docker container and you are ready to start using it.", 
            "title": "Starting cloudbreak shell using cloudbreak deployer"
        }, 
        {
            "location": "/shell/#starting-cloudbreak-shell-with-our-prepared-docker-image", 
            "text": "You can find the docker image and its documentation  here .", 
            "title": "Starting Cloudbreak shell with our prepared docker image"
        }, 
        {
            "location": "/shell/#build-from-source", 
            "text": "If want to use the code or extend it with new commands follow the steps below. You will need:\n- jdk 1.7  git clone https://github.com/sequenceiq/cloudbreak.git\ncd cloudbreak/shell\n../gradlew clean build   Note \nIn case you use the hosted version of Cloudbreak you should use the  latest-release.sh  to get the right version of the CLI.   Start Cloudbreak-shell from the built source  Usage:\n  java -jar cloudbreak-shell-0.5-SNAPSHOT.jar                  : Starts Cloudbreak Shell in interactive mode.\n  java -jar cloudbreak-shell-0.5-SNAPSHOT.jar --cmdfile= FILE  : Cloudbreak executes commands read from the file.\n\nOptions:\n  --cloudbreak.address= http[s]://HOSTNAME:PORT   Address of the Cloudbreak Server [default: https://cloudbreak-api.sequenceiq.com].\n  --identity.address= http[s]://HOSTNAME:PORT     Address of the SequenceIQ identity server [default: https://identity.sequenceiq.com].\n  --sequenceiq.user= USER                         Username of the SequenceIQ user [default: user@sequenceiq.com].\n  --sequenceiq.password= PASSWORD                 Password of the SequenceIQ user [default: password].\n\nNote:\n  You should specify at least your username and password.  Once you are connected you can start to create a cluster. If you are lost and need guidance through the process you can use  hint . You can always use  TAB  for completion.   Note \nAll commands are  context aware  - they are available only when it makes sense - this way you are never confused and guided by the system on the right path.   Provider specific documentations   AWS  Azure  GCP  OpenStack   or you can find a more detailed documentation about Cloudbreak-shell in its  Github repositiry .", 
            "title": "Build from source"
        }, 
        {
            "location": "/ui_account/", 
            "text": "Account management on UI\n\n\nOn the Cloudbreak UI (Uluwatu) there are opportunity to review the user's entitlements and manage some settings.\n\n\nAccount Details page\n\n\nYou can view the \naccount details\n on the \naccount\n page (by clicking on the related header menu).\n\n\nSecurity scopes\n\n\nOn the expanded \naccount details\n you can check your \nsecurity scopes\n.\n\n\n\n\nNote:\n Even the administrator users cannot modify the list of scopes.\n\n\n\n\nCloudbreak has distinct security scope for the following resources:\n\n\n\n\nBlueprints\n\n\nRecipes\n\n\nTemplates\n\n\nCredentials\n\n\nStacks\n\n\nNetworks\n\n\nSecurity Groups\n\n\n\n\n\n\nIn the future the list of security scopes could be extended with new resources.\n\n\n\n\nCloud platforms\n\n\nCloud platforms\n table lists the supported cloud platforms by Cloudbreak.\n\n\nAdministrator users can set the used (what will be available) cloud platforms for the group.\n\n\nFor example:\n If the AWS cloud platform is the selected, only the AWS networks, resources, credentials and \nplatforms to be displayed and can be created for every users in the account (also for the managed users).\n\n\nSupported Cloud platforms:\n\n\n\n\nAWS\n\n\nAzure RM\n\n\nGCP\n\n\nOpenStack", 
            "title": "Account Management"
        }, 
        {
            "location": "/ui_account/#account-management-on-ui", 
            "text": "On the Cloudbreak UI (Uluwatu) there are opportunity to review the user's entitlements and manage some settings.", 
            "title": "Account management on UI"
        }, 
        {
            "location": "/ui_account/#account-details-page", 
            "text": "You can view the  account details  on the  account  page (by clicking on the related header menu).", 
            "title": "Account Details page"
        }, 
        {
            "location": "/ui_account/#security-scopes", 
            "text": "On the expanded  account details  you can check your  security scopes .   Note:  Even the administrator users cannot modify the list of scopes.   Cloudbreak has distinct security scope for the following resources:   Blueprints  Recipes  Templates  Credentials  Stacks  Networks  Security Groups    In the future the list of security scopes could be extended with new resources.", 
            "title": "Security scopes"
        }, 
        {
            "location": "/ui_account/#cloud-platforms", 
            "text": "Cloud platforms  table lists the supported cloud platforms by Cloudbreak.  Administrator users can set the used (what will be available) cloud platforms for the group.  For example:  If the AWS cloud platform is the selected, only the AWS networks, resources, credentials and \nplatforms to be displayed and can be created for every users in the account (also for the managed users).", 
            "title": "Cloud platforms"
        }, 
        {
            "location": "/ui_account/#supported-cloud-platforms", 
            "text": "AWS  Azure RM  GCP  OpenStack", 
            "title": "Supported Cloud platforms:"
        }, 
        {
            "location": "/periscope/", 
            "text": "Auto-Scaling\n\n\nThe purpose of \nauto-scaling\n is to apply SLA scaling policies to a Cloudbreak-managed Hadoop cluster.\n\n\nHow It Works\n\n\nThe auto-scaling capabilities is based on \nAmbari Metrics\n - and \nAmbari Alerts\n. Based on the Blueprint\nused and the running services, Cloudbreak can access all the available metrics from the subsystem and define \nalerts\n based on this information.\n\n\nBeside the default Ambari Metrics, Cloudbreak includes two custom metrics: \nPending YARN containers\n and \nPending applications\n. These two custom metrics works with the YARN subsystem in order to bring \napplication\n level QoS to the cluster.\n\n\n\n\nIn order to use the \nautoscaling\n feature with Cloudbreak you will have to enable from the UI or shell.\n\n\n\n\n\n\nAlerts\n\n\nAuto-scaling supports two \nAlert\n types: \nmetric\n and \ntime\n based.\n\n\nMetric-based Alerts\n\n\nMetric based alerts are using the default (or custom) Ambari metrics. These metrics have a default \nThreshold\n value configured in Ambari - nevertheless these thresholds can be configured, changed or altered in Ambari. In order to change the default threshold for a metric please go to Ambari UI and select the \nAlerts\n tab and the metric. The values can be changed in the \nThreshold\n section.\n\n\n\n\nMetric alerts have a few configurable fields.\n\n\n\n\nalert name\n - name of the alert\n\n\ndescription\n - description of the alert\n\n\nmetric - desired state\n - the Ambari metrics based on the installed services and their \nstate\n (OK, WARN, CRITICAL), based on the \nthreshold\n value\n\n\nperiod\n - for how many \nminutes\n the metric state has to be sustained in order for an alert to be triggered\n\n\n\n\n\n\nTime-based Alerts\n\n\nTime based alerts are based on \ncron\n expressions and allow alerts to be triggered based on time.\n\n\nTime alerts have a few configurable fields.\n\n\n\n\nalert name\n - name of the alert\n\n\ndescription\n - description of the alert\n\n\ntime zone\n - the time zone\n\n\ncrom expression\n - the \ncron\n expression to be used for the alert\n\n\n\n\n\n\nScaling Policies\n\n\nScaling is the ability to increase or decrease the capacity of the Hadoop cluster or application based on an alert.\nWhen scaling policies are used, the capacity is automatically increased or decreased according to the conditions defined.\nCloudbreak will do the heavy lifting and based on the alerts and the scaling policy linked to them it executes the associated policy. We scaling granularity is at the \nhostgroup\n level - thus you have the option to scale services or components only, not the whole cluster.\n\n\nScaling policies have a few configurable fields.\n\n\n\n\npolicy name\n - name of the scaling policy\n\n\nscaling adjustment\n - the number of added or removed noded based on \nnode count\n (the number of nodes), \npercentage\n (computed percentage adjustment based on the cluster size) and \nexact\n (a given exact size of the cluster)\n\n\nhost group\n - the Ambari hostgroup to be scaled\n\n\nalert\n - the triggered alert based on that the scaling policy applies\n\n\n\n\n\n\nCluster Scaling Configuration\n\n\nAn SLA scaling policy can contain multiple alerts. When an alert is triggered a \nscaling adjustment\n is applied, however to keep the cluster size within boundaries a \ncluster size min.\n and \ncluster size max.\n is attached to the cluster - thus a scaling policy can never over or undersize a cluster. Also in order to avoid stressing the cluster we have introduced a \ncooldown time\n period (minutes) - though an alert is raised and there is an associated scaling policy, the system will not apply the policy within the configured timeframe. In an SLA scaling policy the triggered rules are applied in order.\n\n\n\n\ncooldown time\n - period (minutes) between two scaling events while the cluster is locked from adjustments\n\n\ncluster size min.\n - size will never go under the minimum value, despite scaling adjustments\n\n\ncluster size max.\n - size will never go above the maximum value, despite scaling adjustments\n\n\n\n\n\n\nDownscale Scaling Considerations\n\n\nCloudbreak auto-scaling will try to keep a healthy cluster, thus does several background checks during \ndownscale\n.\n\n\n\n\nWe never remove \nApplication master nodes\n from a cluster. In order to make sure that a node running AM is not removed, Cloudbreak has to be able to access the YARN Resource Manager - when creating a cluster using the \ndefault\n secure network template please make sure that the RM's port is open on the node\n\n\nIn order to keep a healthy HDFS during downscale we always keep the configured \nreplication\n factor and make sure there is enough \nspace\n on HDFS to rebalance data. Also during downscale in order to minimize the rebalancing, replication and HDFS storms we check block locations and compute the least costly operations.", 
            "title": "Auto-Scaling"
        }, 
        {
            "location": "/periscope/#auto-scaling", 
            "text": "The purpose of  auto-scaling  is to apply SLA scaling policies to a Cloudbreak-managed Hadoop cluster.", 
            "title": "Auto-Scaling"
        }, 
        {
            "location": "/periscope/#how-it-works", 
            "text": "The auto-scaling capabilities is based on  Ambari Metrics  - and  Ambari Alerts . Based on the Blueprint\nused and the running services, Cloudbreak can access all the available metrics from the subsystem and define  alerts  based on this information.  Beside the default Ambari Metrics, Cloudbreak includes two custom metrics:  Pending YARN containers  and  Pending applications . These two custom metrics works with the YARN subsystem in order to bring  application  level QoS to the cluster.   In order to use the  autoscaling  feature with Cloudbreak you will have to enable from the UI or shell.", 
            "title": "How It Works"
        }, 
        {
            "location": "/periscope/#alerts", 
            "text": "Auto-scaling supports two  Alert  types:  metric  and  time  based.  Metric-based Alerts  Metric based alerts are using the default (or custom) Ambari metrics. These metrics have a default  Threshold  value configured in Ambari - nevertheless these thresholds can be configured, changed or altered in Ambari. In order to change the default threshold for a metric please go to Ambari UI and select the  Alerts  tab and the metric. The values can be changed in the  Threshold  section.   Metric alerts have a few configurable fields.   alert name  - name of the alert  description  - description of the alert  metric - desired state  - the Ambari metrics based on the installed services and their  state  (OK, WARN, CRITICAL), based on the  threshold  value  period  - for how many  minutes  the metric state has to be sustained in order for an alert to be triggered    Time-based Alerts  Time based alerts are based on  cron  expressions and allow alerts to be triggered based on time.  Time alerts have a few configurable fields.   alert name  - name of the alert  description  - description of the alert  time zone  - the time zone  crom expression  - the  cron  expression to be used for the alert", 
            "title": "Alerts"
        }, 
        {
            "location": "/periscope/#scaling-policies", 
            "text": "Scaling is the ability to increase or decrease the capacity of the Hadoop cluster or application based on an alert.\nWhen scaling policies are used, the capacity is automatically increased or decreased according to the conditions defined.\nCloudbreak will do the heavy lifting and based on the alerts and the scaling policy linked to them it executes the associated policy. We scaling granularity is at the  hostgroup  level - thus you have the option to scale services or components only, not the whole cluster.  Scaling policies have a few configurable fields.   policy name  - name of the scaling policy  scaling adjustment  - the number of added or removed noded based on  node count  (the number of nodes),  percentage  (computed percentage adjustment based on the cluster size) and  exact  (a given exact size of the cluster)  host group  - the Ambari hostgroup to be scaled  alert  - the triggered alert based on that the scaling policy applies", 
            "title": "Scaling Policies"
        }, 
        {
            "location": "/periscope/#cluster-scaling-configuration", 
            "text": "An SLA scaling policy can contain multiple alerts. When an alert is triggered a  scaling adjustment  is applied, however to keep the cluster size within boundaries a  cluster size min.  and  cluster size max.  is attached to the cluster - thus a scaling policy can never over or undersize a cluster. Also in order to avoid stressing the cluster we have introduced a  cooldown time  period (minutes) - though an alert is raised and there is an associated scaling policy, the system will not apply the policy within the configured timeframe. In an SLA scaling policy the triggered rules are applied in order.   cooldown time  - period (minutes) between two scaling events while the cluster is locked from adjustments  cluster size min.  - size will never go under the minimum value, despite scaling adjustments  cluster size max.  - size will never go above the maximum value, despite scaling adjustments    Downscale Scaling Considerations  Cloudbreak auto-scaling will try to keep a healthy cluster, thus does several background checks during  downscale .   We never remove  Application master nodes  from a cluster. In order to make sure that a node running AM is not removed, Cloudbreak has to be able to access the YARN Resource Manager - when creating a cluster using the  default  secure network template please make sure that the RM's port is open on the node  In order to keep a healthy HDFS during downscale we always keep the configured  replication  factor and make sure there is enough  space  on HDFS to rebalance data. Also during downscale in order to minimize the rebalancing, replication and HDFS storms we check block locations and compute the least costly operations.", 
            "title": "Cluster Scaling Configuration"
        }, 
        {
            "location": "/sssd/", 
            "text": "System Security Services Daemon\n\n\n\n\nThis feature is currently \nTECHNICAL PREVIEW\n.\n\n\n\n\nBy default Cloudbreak installs Hadoop with a few default users. Most of the time, default users do not give great freedom, to manage permissions in the ecosystem.\nOther big problem around the user management is that created users must be consistent on the whole cluster, otherwise operations will fail.\n\n\nThat's where \nSystem Security Services Daemon\n (SSSD) comes in picture. SSSD is a system daemon. Its primary function is to provide access to remote identity and authentication resources through a common framework that can provide caching and offline support to the system. Cloudbreak helps to install and configure SSSD, and sets up an SSH server on Ambari hosts, to allow log in with external users on the host's 2022 port.\n\n\nssh [external-user]@[ambari-host] -p 2022\n\n\n\n\nManage SSSD configurations\n\n\nYou can add new configuration on the web UI. Go to \nmanage security configurations\n section and select \ncreate configuration\n. There are 3 types of configuration:\n\n\n\n\nPARAMETERS, Cloudbreak will generate configuration by the given parameters\n\n\nFILE, upload your configuration\n\n\nTEXT, simply type or paste your configuration\n\n\n\n\nIn the shell you have two options \nadd\n and \nupload\n.\n\n\nsssdconfig add --name [config-name] --providerType [LDAP|ACTIVE_DIRECTORY|IPA] --url [provider-url] --schema [AD|IPA|RFC2307|RFC2307BIS] --baseSearch [search-base-of-posix-entities] --tlsReqcert [NEVER|ALLOW|TRY|DEMAND|HARD]\n\n\n\n\nThis command has optional parameters:\n\n\n--description\n \"string\" description of the configuration\n\n\n--adServer\n \"string\" comma-separated list of IP addresses or hostnames of the AD servers\n\n\n--kerberosServer\n \"string\" comma-separated list of IP addresses or hostnames of the Kerberos servers\n\n\n--kerberosRealm\n \"string\" name of the Kerberos realm\n\n\n--publicInAccount\n \"flag\" flags if the configuration is public in the account\n\n\nTo upload configuration please use \nupload\n command:\n\n\nsssdconfig upload --name [config-name] --file [configuration-path]\n\n\n\n\nThis command also has optional parameters:\n\n\n--description\n \"string\" description of the configuration\n\n\n--publicInAccount\n \"flag\" flags if the configuration is public in the account\n\n\nEnable SSSD configuration\n\n\nThere are two ways to enable SSSD configuration on a Cloudbreak cluster.\n\n\n\n\nOn UI in the \nCreate cluster\n wizard, on the \nSetup Network and Security\n tab, check the \nUse pre configured SSSD\n option and select an existing configuration at \nSelect configuration\n.\n\n\nIn CLoudbreak shell before \ncluster create\n command select a configuration by name \nsssdconfig select --name [config-name]\n or by id \nsssdconfig select --id [config-id]\n\n\n\n\nEmbedded LDAP support\n\n\nIn Cloudbreak there is a built in but optionally LDAP service,\n\n\n\n\nOn UI in the \nCreate cluster\n wizard, on the \nSetup Network and Security\n tab, check the \nStart LDAP and configure SSSD\n option.\n\n\n\n\nor\n\n\n\n\nIn Cloudbreak shell place \nldapRequired\n flag in \ncluster create\n command: \ncluster create --ldapRequired\n.\n\n\n\n\nThat's it. In the background Cloudbreak creates a default SSSD configuration named \ncloudbreak-default-ldap\n if there is no other selected configuration, starts LDAP service and integrate them together. If you want to connect to an Ambari node (except Ambari server), just use \nambari-qa\n user with \ncloudbreak\n password to login: \nssh ambari-qa@[ambari-host] -p 2022\n.\n\n\nTest configuration\n\n\nUser and group ids are starting at 10000 in the LDAP database, so after login you have to see this results:\n\n\n[ambari-qa@host ~]$ id ambari-qa\nuid=10000(ambari-qa) gid=10000(hadoop) groups=10000(hadoop),100(users)\n[ambari-qa@host ~]$ getent passwd ambari-qa\nambari-qa:*:10000:10000:ambari-qa:/home/ambari-qa:/bin/bash\n\n\n\n\nThroubleshooting\n\n\nIn Cloudbreak the SSSD service starts in very verbose mode. To read log files first login to the host as \ncentos\n user.\n\n\nssh centos@[ambari-host] -i [cluster-credential]\n\n\n\n\nNext you have jump into Ambari agent Docker container.\n\n\nsudo docker exec -it $(docker ps --format=\n{{.Names}}\n | grep ambari-agent) bash\n\n\n\n\nLogs are found at \n/var/log/sssd\n directory in file per service format.\n\n\n[root@docker-ambari /]# ls /var/log/sssd\nldap_child.log  sssd_LDAP.log  sssd.log  sssd_nss.log  sssd_pam.log", 
            "title": "SSSD Configuration"
        }, 
        {
            "location": "/sssd/#system-security-services-daemon", 
            "text": "This feature is currently  TECHNICAL PREVIEW .   By default Cloudbreak installs Hadoop with a few default users. Most of the time, default users do not give great freedom, to manage permissions in the ecosystem.\nOther big problem around the user management is that created users must be consistent on the whole cluster, otherwise operations will fail.  That's where  System Security Services Daemon  (SSSD) comes in picture. SSSD is a system daemon. Its primary function is to provide access to remote identity and authentication resources through a common framework that can provide caching and offline support to the system. Cloudbreak helps to install and configure SSSD, and sets up an SSH server on Ambari hosts, to allow log in with external users on the host's 2022 port.  ssh [external-user]@[ambari-host] -p 2022", 
            "title": "System Security Services Daemon"
        }, 
        {
            "location": "/sssd/#manage-sssd-configurations", 
            "text": "You can add new configuration on the web UI. Go to  manage security configurations  section and select  create configuration . There are 3 types of configuration:   PARAMETERS, Cloudbreak will generate configuration by the given parameters  FILE, upload your configuration  TEXT, simply type or paste your configuration   In the shell you have two options  add  and  upload .  sssdconfig add --name [config-name] --providerType [LDAP|ACTIVE_DIRECTORY|IPA] --url [provider-url] --schema [AD|IPA|RFC2307|RFC2307BIS] --baseSearch [search-base-of-posix-entities] --tlsReqcert [NEVER|ALLOW|TRY|DEMAND|HARD]  This command has optional parameters:  --description  \"string\" description of the configuration  --adServer  \"string\" comma-separated list of IP addresses or hostnames of the AD servers  --kerberosServer  \"string\" comma-separated list of IP addresses or hostnames of the Kerberos servers  --kerberosRealm  \"string\" name of the Kerberos realm  --publicInAccount  \"flag\" flags if the configuration is public in the account  To upload configuration please use  upload  command:  sssdconfig upload --name [config-name] --file [configuration-path]  This command also has optional parameters:  --description  \"string\" description of the configuration  --publicInAccount  \"flag\" flags if the configuration is public in the account", 
            "title": "Manage SSSD configurations"
        }, 
        {
            "location": "/sssd/#enable-sssd-configuration", 
            "text": "There are two ways to enable SSSD configuration on a Cloudbreak cluster.   On UI in the  Create cluster  wizard, on the  Setup Network and Security  tab, check the  Use pre configured SSSD  option and select an existing configuration at  Select configuration .  In CLoudbreak shell before  cluster create  command select a configuration by name  sssdconfig select --name [config-name]  or by id  sssdconfig select --id [config-id]", 
            "title": "Enable SSSD configuration"
        }, 
        {
            "location": "/sssd/#embedded-ldap-support", 
            "text": "In Cloudbreak there is a built in but optionally LDAP service,   On UI in the  Create cluster  wizard, on the  Setup Network and Security  tab, check the  Start LDAP and configure SSSD  option.   or   In Cloudbreak shell place  ldapRequired  flag in  cluster create  command:  cluster create --ldapRequired .   That's it. In the background Cloudbreak creates a default SSSD configuration named  cloudbreak-default-ldap  if there is no other selected configuration, starts LDAP service and integrate them together. If you want to connect to an Ambari node (except Ambari server), just use  ambari-qa  user with  cloudbreak  password to login:  ssh ambari-qa@[ambari-host] -p 2022 .", 
            "title": "Embedded LDAP support"
        }, 
        {
            "location": "/sssd/#test-configuration", 
            "text": "User and group ids are starting at 10000 in the LDAP database, so after login you have to see this results:  [ambari-qa@host ~]$ id ambari-qa\nuid=10000(ambari-qa) gid=10000(hadoop) groups=10000(hadoop),100(users)\n[ambari-qa@host ~]$ getent passwd ambari-qa\nambari-qa:*:10000:10000:ambari-qa:/home/ambari-qa:/bin/bash", 
            "title": "Test configuration"
        }, 
        {
            "location": "/sssd/#throubleshooting", 
            "text": "In Cloudbreak the SSSD service starts in very verbose mode. To read log files first login to the host as  centos  user.  ssh centos@[ambari-host] -i [cluster-credential]  Next you have jump into Ambari agent Docker container.  sudo docker exec -it $(docker ps --format= {{.Names}}  | grep ambari-agent) bash  Logs are found at  /var/log/sssd  directory in file per service format.  [root@docker-ambari /]# ls /var/log/sssd\nldap_child.log  sssd_LDAP.log  sssd.log  sssd_nss.log  sssd_pam.log", 
            "title": "Throubleshooting"
        }, 
        {
            "location": "/recipes/", 
            "text": "Recipes\n\n\nWith the help of Cloudbreak it is very easy to provision Hadoop clusters in the cloud from an Apache Ambari blueprint. Cloudbreak built in provisioning doesn't contain every use case, so we are introducing the concept of recipes.\n\n\nRecipes are basically script extensions to a cluster that run on a set of nodes before or after the Ambari cluster installation. With recipes it's quite easy for example to put a JAR file on the Hadoop classpath or run some custom scripts.\n\n\nIn Cloudbreak we supports two ways to configure recipe, we have downloadable and stored recipes.\n\n\nStored recipes\n\n\nAs the name mentions stored recipes are uploaded and stored in Cloudbreak via web interface or shell.\n\n\nThe easiest way to create a custom recipe:\n\n\n\n\ncreate your own pre and/or post scripts\n\n\nupload them on shell or web interface\n\n\n\n\nAdd recipe\n\n\nOn the web interface under \nmanage recipes\n section you should \ncreate new recipe\n. Please choose between SCRIPT, FILE or URL type plugin, and fill required fields.\n\n\nTo add recipe via shell use the following command:\n\n\nrecipe store --name [recipe-name] --executionType [ONE_NODE|ALL_NODES] --preInstallScriptFile /path/of/the/pre-install-script --postInstallScriptFile /path/of/the/post-install-script\n\n\n\n\nThis command has optional parameters:\n\n\n--description\n \"string\" description of the recipe\n\n\n--timeout\n \"integer\" timeout of the script execution\n\n\n--publicInAccount\n \"flag\" flags if the recipe is public in the account\n\n\nIn the background Cloudbreak pushes recipe to Consul key/value store during cluster creation.\n\n\nDownloadable recipes\n\n\nA downloadable recipe should be available on HTTP, HTTPS protocols optionally with basic authentication, or any kind of public Git repository.\n\n\nThis kind of recipe must contain a plugin.toml file, with some basic information about the recipe. Besides this at least a recipe-pre-install or a recipe-post-install script.\n\n\nContent of plugin.toml:\n\n\n[plugin]\nname = \n[recipe-name]\n\ndescription = \n[description-of-the-recipe]\n\nversion = \n1.0\n\nmaintainer_name = \n[maintainer-name]\n\nmaintainer_email = \n[maintainer-email]\n\nwebsite_url = \n[website-url]\n\n\n\n\n\nPre- and post scripts are regular shell scripts, and must be executable.\n\n\nTo configure recipe or recipe groups in Cloudbreak you have to create a descriptive JSON file and send it to Cloudbreak via our shell. On web interface you don't need to take care of this file.\n\n\n{\n  \nname\n: \n[recipe-name]\n,\n  \ndescription\n: \n[description-of-the-recipe]\n,\n  \nproperties\n: {\n    \n[key]\n: \n[value]\n\n  },\n  \nplugins\n: {\n      \ngit://github.com/account/recipe.git\n: \nONE_NODE\n\n      \nhttp://user:password@mydomain.com/my-recipe.tar\n: \nALL_NODES\n\n      \nhttps://mydomain.com/my-recipe.zip\n: \nALL_NODES\n\n  }\n}\n\n\n\n\nAt this point we need to understand some element of the JSON above.\n\n\nFirst of all \nproperties\n. Properties are saved to Consul key/value store, and they are available from the pre or post script by fetching http://localhost:8500/v1/kv/[key]?raw. This option is a good choice if you want to write reusable recipes.\n\n\nThe next one is \nplugins\n. As you read before we support a few kind of protocols, and each of them has their own limitations:\n\n\n\n\n\n\nGit\n\n\n\n\ngit repository must be public (or available from the cluster)\n\n\nthe recipe files must be on the root\n\n\nonly repository default branch supported, there is no opportunity to check out different branch\n\n\n\n\n\n\n\n\nHTTP(S)\n\n\n\n\non this kind of protocols you have to bundle your recipe into a tar or zip file\n\n\nbasic authentication is the only way to protect recipe from public\n\n\n\n\n\n\n\n\nLast one is the execution type of the recipe. We supports two options:\n\n\n\n\nONE_NODE means the recipe will execute only one node in the hostgroup\n\n\nAll_NODES runs every single instance in the hostgroup.\n\n\n\n\nAdd recipe\n\n\nOn the web interface please select URL type plugin, and fill other required fields.\n\n\nTo add recipe via shell use the command(s) below:\n\n\nrecipe add --file /path/of/the/recipe/json\n\n\n\n\nor\n\n\nrecipe add --url http(s)://mydomain.com/my-recipe.json\n\n\n\n\nAdd command has an optional parameter\n\n\n--publicInAccount\n is checked all the users belonging to your account will be able to use this recipe for create clusters, but cannot delete it.\n\n\nSample recipe for Ranger\n\n\nTo be able to install Ranger from a blueprint, a database must be running when Ambari starts to install Ranger Admin. With Cloudbreak a database can be configured and started from a recipe. We've created a sample recipe that can be used to initialize and start a PostgreSQL database that will be able to accept connections from Ranger and store its data. Add the \nONE_NODE\n recipe from \nthis URL\n on the Cloudbreak UI:\n\n\n\n\nAnd add this recipe to the same hostgroup where Ranger Admin is installed on the 'Choose Blueprint' when creating a new cluster:\n\n\n\n\nRanger installation also has some required properties that must be added to the blueprint. We've created a sample one-node blueprint with the necessary configurations to install Ranger Admin and Ranger Usersync. The configuration values in this blueprint match the sample recipe above - they are set to use a PostgreSQL database on the same host where Ranger Admin is installed. Usersync is configured to use UNIX as the authentication method and it should also be installed on the same host where Ranger Admin is installed.\n\n\n{\n  \nconfigurations\n: [\n    {\n      \nranger-site\n: {\n        \nproperties_attributes\n: {},\n        \nproperties\n: {}\n      }\n    },\n    {\n      \nranger-hdfs-policymgr-ssl\n: {\n        \nproperties_attributes\n: {},\n        \nproperties\n: {\n          \nxasecure.policymgr.clientssl.keystore\n: \n/etc/hadoop/conf/ranger-plugin-keystore.jks\n,\n          \nxasecure.policymgr.clientssl.keystore.credential.file\n: \njceks://file{{credential_file}}\n,\n          \nxasecure.policymgr.clientssl.truststore\n: \n/etc/hadoop/conf/ranger-plugin-truststore.jks\n,\n          \nxasecure.policymgr.clientssl.truststore.credential.file\n: \njceks://file{{credential_file}}\n\n        }\n      }\n    },\n    {\n      \nranger-ugsync-site\n: {\n        \nproperties_attributes\n: {},\n        \nproperties\n: {\n          \nranger.usersync.enabled\n: \ntrue\n,\n          \nranger.usersync.filesource.file\n: \n/tmp/usergroup.txt\n,\n          \nranger.usersync.filesource.text.delimiter\n: \n,\n,\n          \nranger.usersync.group.memberattributename\n: \nmember\n,\n          \nranger.usersync.group.nameattribute\n: \ncn\n,\n          \nranger.usersync.group.objectclass\n: \ngroupofnames\n,\n          \nranger.usersync.group.searchbase\n: \nou=groups,dc=hadoop,dc=apache,dc=org\n,\n          \nranger.usersync.group.searchenabled\n: \nfalse\n,\n          \nranger.usersync.group.searchfilter\n: \nempty\n,\n          \nranger.usersync.group.searchscope\n: \nsub\n,\n          \nranger.usersync.group.usermapsyncenabled\n: \nfalse\n,\n          \nranger.usersync.ldap.bindalias\n: \ntestldapalias\n,\n          \nranger.usersync.ldap.binddn\n: \ncn=admin,dc=xasecure,dc=net\n,\n          \nranger.usersync.ldap.bindkeystore\n: \n-\n,\n          \nranger.usersync.ldap.groupname.caseconversion\n: \nlower\n,\n          \nranger.usersync.ldap.searchBase\n: \ndc=hadoop,dc=apache,dc=org\n,\n          \nranger.usersync.ldap.url\n: \nldap://localhost:389\n,\n          \nranger.usersync.ldap.user.groupnameattribute\n: \nmemberof, ismemberof\n,\n          \nranger.usersync.ldap.user.nameattribute\n: \ncn\n,\n          \nranger.usersync.ldap.user.objectclass\n: \nperson\n,\n          \nranger.usersync.ldap.user.searchbase\n: \nou=users,dc=xasecure,dc=net\n,\n          \nranger.usersync.ldap.user.searchfilter\n: \nempty\n,\n          \nranger.usersync.ldap.user.searchscope\n: \nsub\n,\n          \nranger.usersync.ldap.username.caseconversion\n: \nlower\n,\n          \nranger.usersync.logdir\n: \n/var/log/ranger/usersync\n,\n          \nranger.usersync.pagedresultsenabled\n: \ntrue\n,\n          \nranger.usersync.pagedresultssize\n: \n500\n,\n          \nranger.usersync.policymanager.baseURL\n: \n{{ranger_external_url}}\n,\n          \nranger.usersync.policymanager.maxrecordsperapicall\n: \n1000\n,\n          \nranger.usersync.policymanager.mockrun\n: \nfalse\n,\n          \nranger.usersync.port\n: \n5151\n,\n          \nranger.usersync.sink.impl.class\n: \norg.apache.ranger.unixusersync.process.PolicyMgrUserGroupBuilder\n,\n          \nranger.usersync.sleeptimeinmillisbetweensynccycle\n: \n5\n,\n          \nranger.usersync.source.impl.class\n: \norg.apache.ranger.unixusersync.process.UnixUserGroupBuilder\n,\n          \nranger.usersync.ssl\n: \ntrue\n,\n          \nranger.usersync.unix.minUserId\n: \n500\n\n        }\n      }\n    },\n    {\n      \nadmin-properties\n: {\n        \nproperties_attributes\n: {},\n        \nproperties\n: {\n          \nDB_FLAVOR\n: \nPOSTGRES\n,\n          \nSQL_COMMAND_INVOKER\n: \npsql\n,\n          \nSQL_CONNECTOR_JAR\n: \n/var/lib/ambari-agent/tmp/postgres-jdbc-driver.jar\n,\n          \naudit_db_name\n: \nranger_audit\n,\n          \naudit_db_user\n: \nrangerlogger\n,\n          \ndb_host\n: \nlocalhost:5432\n,\n          \ndb_name\n: \nranger\n,\n          \ndb_root_user\n: \npostgres\n,\n          \ndb_root_password\n: \nadmin\n,\n          \ndb_user\n: \nrangeradmin\n,\n          \npolicymgr_external_url\n: \nhttp://localhost:6080\n,\n          \nranger_jdbc_connection_url\n: \njdbc:postgresql://{db_host}/ranger\n,\n          \nranger_jdbc_driver\n: \norg.postgresql.Driver\n\n        }\n      }\n    },\n    {\n      \nranger-admin-site\n: {\n        \nproperties_attributes\n: {},\n        \nproperties\n: {\n          \nranger.audit.source.type\n: \ndb\n,\n          \nranger.authentication.method\n: \nUNIX\n,\n          \nranger.credential.provider.path\n: \n/etc/ranger/admin/rangeradmin.jceks\n,\n          \nranger.externalurl\n: \n{{ranger_external_url}}\n,\n          \nranger.https.attrib.keystore.file\n: \n/etc/ranger/admin/keys/server.jks\n,\n          \nranger.jpa.audit.jdbc.credential.alias\n: \nrangeraudit\n,\n          \nranger.jpa.audit.jdbc.dialect\n: \n{{jdbc_dialect}}\n,\n          \nranger.jpa.audit.jdbc.driver\n: \n{{jdbc_driver}}\n,\n          \nranger.jpa.audit.jdbc.url\n: \n{{audit_jdbc_url}}\n,\n          \nranger.jpa.audit.jdbc.user\n: \n{{ranger_audit_db_user}}\n,\n          \nranger.jpa.jdbc.credential.alias\n: \nrangeradmin\n,\n          \nranger.jpa.jdbc.dialect\n: \n{{jdbc_dialect}}\n,\n          \nranger.jpa.jdbc.driver\n: \norg.postgresql.Driver\n,\n          \nranger.jpa.jdbc.url\n: \njdbc:postgresql://localhost:5432/ranger\n,\n          \nranger.jpa.jdbc.user\n: \n{{ranger_db_user}}\n,\n          \nranger.jpa.jdbc.password\n: \n{{ranger_db_password}}\n,\n          \nranger.ldap.ad.domain\n: \nlocalhost\n,\n          \nranger.ldap.ad.url\n: \nldap://ad.xasecure.net:389\n,\n          \nranger.ldap.group.roleattribute\n: \ncn\n,\n          \nranger.ldap.group.searchbase\n: \nou=groups,dc=xasecure,dc=net\n,\n          \nranger.ldap.group.searchfilter\n: \n(member=uid={0},ou=users,dc=xasecure,dc=net)\n,\n          \nranger.ldap.url\n: \nldap://localhost:389\n,\n          \nranger.ldap.user.dnpattern\n: \nuid={0},ou=users,dc=xasecure,dc=net\n,\n          \nranger.service.host\n: \n{{ranger_host}}\n,\n          \nranger.service.http.enabled\n: \ntrue\n,\n          \nranger.service.http.port\n: \n6080\n,\n          \nranger.service.https.attrib.clientAuth\n: \nfalse\n,\n          \nranger.service.https.attrib.keystore.keyalias\n: \nmkey\n,\n          \nranger.service.https.attrib.keystore.pass\n: \nranger\n,\n          \nranger.service.https.attrib.ssl.enabled\n: \nfalse\n,\n          \nranger.service.https.port\n: \n6182\n,\n          \nranger.unixauth.remote.login.enabled\n: \ntrue\n,\n          \nranger.unixauth.service.hostname\n: \nlocalhost\n,\n          \nranger.unixauth.service.port\n: \n5151\n\n        }\n      }\n    },\n    {\n      \nranger-env\n: {\n        \nproperties_attributes\n: {},\n        \nproperties\n: {\n          \nadmin_username\n: \nadmin\n,\n          \ncreate_db_dbuser\n: \ntrue\n,\n          \nranger_admin_log_dir\n: \n/var/log/ranger/admin\n,\n          \nranger_admin_username\n: \namb_ranger_admin\n,\n          \nranger_admin_password\n: \namb_ranger_pw\n,\n          \nranger_group\n: \nranger\n,\n          \nranger_jdbc_connection_url\n: \n{{ranger_jdbc_connection_url}}\n,\n          \nranger_jdbc_driver\n: \norg.postgresql.Driver\n,\n          \nranger_pid_dir\n: \n/var/run/ranger\n,\n          \nranger_user\n: \nranger\n,\n          \nranger_usersync_log_dir\n: \n/var/log/ranger/usersync\n,\n          \nxml_configurations_supported\n: \ntrue\n\n        }\n      }\n    },\n    {\n      \nranger-yarn-security\n: {\n        \nproperties_attributes\n: {},\n        \nproperties\n: {\n          \nranger.plugin.yarn.policy.cache.dir\n: \n/etc/ranger/{{repo_name}}/policycache\n,\n          \nranger.plugin.yarn.policy.pollIntervalMs\n: \n30000\n,\n          \nranger.plugin.yarn.policy.rest.ssl.config.file\n: \n/etc/yarn/conf/ranger-policymgr-ssl.xml\n,\n          \nranger.plugin.yarn.policy.rest.url\n: \n{{policymgr_mgr_url}}\n,\n          \nranger.plugin.yarn.policy.source.impl\n: \norg.apache.ranger.admin.client.RangerAdminRESTClient\n,\n          \nranger.plugin.yarn.service.name\n: \n{{repo_name}}\n\n        }\n      }\n    },\n    {\n      \nranger-yarn-audit\n: {\n        \nproperties_attributes\n: {},\n        \nproperties\n: {\n          \nxasecure.audit.credential.provider.file\n: \njceks://file{{credential_file}}\n,\n          \nxasecure.audit.db.async.max.flush.interval.ms\n: \n30000\n,\n          \nxasecure.audit.db.async.max.queue.size\n: \n10240\n,\n          \nxasecure.audit.db.batch.size\n: \n100\n,\n          \nxasecure.audit.db.is.async\n: \ntrue\n,\n          \nxasecure.audit.destination.db\n: \ntrue\n,\n          \nxasecure.audit.hdfs.async.max.flush.interval.ms\n: \n30000\n,\n          \nxasecure.audit.hdfs.async.max.queue.size\n: \n1048576\n,\n          \nxasecure.audit.destination.hdfs.dir\n: \n/ranger/audit/%app-type%/%time:yyyyMMdd%\n,\n          \nxasecure.audit.hdfs.config.destination.file\n: \n%hostname%-audit.log\n,\n          \nxasecure.audit.hdfs.config.destination.flush.interval.seconds\n: \n900\n,\n          \nxasecure.audit.hdfs.config.destination.open.retry.interval.seconds\n: \n60\n,\n          \nxasecure.audit.hdfs.config.destination.rollover.interval.seconds\n: \n86400\n,\n          \nxasecure.audit.hdfs.config.encoding\n: \n,\n          \nxasecure.audit.hdfs.config.local.archive.directory\n: \n/var/log/yarn/audit/archive\n,\n          \nxasecure.audit.hdfs.config.local.archive.max.file.count\n: \n10\n,\n          \nxasecure.audit.hdfs.config.local.buffer.directory\n: \n/var/log/yarn/audit\n,\n          \nxasecure.audit.hdfs.config.local.buffer.file\n: \n%time:yyyyMMdd-HHmm.ss%.log\n,\n          \nxasecure.audit.hdfs.config.local.buffer.file.buffer.size.bytes\n: \n8192\n,\n          \nxasecure.audit.hdfs.config.local.buffer.flush.interval.seconds\n: \n60\n,\n          \nxasecure.audit.hdfs.config.local.buffer.rollover.interval.seconds\n: \n600\n,\n          \nxasecure.audit.hdfs.is.async\n: \ntrue\n,\n          \nxasecure.audit.is.enabled\n: \ntrue\n,\n          \nxasecure.audit.jpa.javax.persistence.jdbc.driver\n: \n{{jdbc_driver}}\n,\n          \nxasecure.audit.jpa.javax.persistence.jdbc.url\n: \n{{audit_jdbc_url}}\n,\n          \nxasecure.audit.jpa.javax.persistence.jdbc.user\n: \n{{xa_audit_db_user}}\n,\n          \nxasecure.audit.kafka.async.max.flush.interval.ms\n: \n1000\n,\n          \nxasecure.audit.kafka.async.max.queue.size\n: \n1\n,\n          \nxasecure.audit.kafka.broker_list\n: \nlocalhost:9092\n,\n          \nxasecure.audit.kafka.is.enabled\n: \nfalse\n,\n          \nxasecure.audit.kafka.topic_name\n: \nranger_audits\n,\n          \nxasecure.audit.log4j.async.max.flush.interval.ms\n: \n30000\n,\n          \nxasecure.audit.log4j.async.max.queue.size\n: \n10240\n,\n          \nxasecure.audit.log4j.is.async\n: \nfalse\n,\n          \nxasecure.audit.log4j.is.enabled\n: \nfalse\n\n        }\n      }\n    },\n    {\n      \nranger-hdfs-security\n: {\n        \nproperties_attributes\n: {},\n        \nproperties\n: {\n          \nranger.plugin.hdfs.policy.cache.dir\n: \n/etc/ranger/{{repo_name}}/policycache\n,\n          \nranger.plugin.hdfs.policy.pollIntervalMs\n: \n30000\n,\n          \nranger.plugin.hdfs.policy.rest.ssl.config.file\n: \n/etc/hadoop/conf/ranger-policymgr-ssl.xml\n,\n          \nranger.plugin.hdfs.policy.rest.url\n: \n{{policymgr_mgr_url}}\n,\n          \nranger.plugin.hdfs.policy.source.impl\n: \norg.apache.ranger.admin.client.RangerAdminRESTClient\n,\n          \nranger.plugin.hdfs.service.name\n: \n{{repo_name}}\n,\n          \nxasecure.add-hadoop-authorization\n: \ntrue\n\n        }\n      }\n    },\n    {\n      \nranger-yarn-plugin-properties\n: {\n        \nproperties_attributes\n: {},\n        \nproperties\n: {\n          \nREPOSITORY_CONFIG_USERNAME\n: \nyarn\n,\n          \ncommon.name.for.certificate\n: \n-\n,\n          \nhadoop.rpc.protection\n: \n-\n,\n          \npolicy_user\n: \nambari-qa\n,\n          \nranger-yarn-plugin-enabled\n: \nNo\n\n        }\n      }\n    },\n    {\n      \nranger-hdfs-audit\n: {\n        \nproperties_attributes\n: {},\n        \nproperties\n: {\n          \nxasecure.audit.credential.provider.file\n: \njceks://file{{credential_file}}\n,\n          \nxasecure.audit.db.async.max.flush.interval.ms\n: \n30000\n,\n          \nxasecure.audit.db.async.max.queue.size\n: \n10240\n,\n          \nxasecure.audit.db.batch.size\n: \n100\n,\n          \nxasecure.audit.db.is.async\n: \ntrue\n,\n          \nxasecure.audit.destination.db\n: \ntrue\n,\n          \nxasecure.audit.destination.hdfs.dir\n: \n/ranger/audit/%app-type%/%time:yyyyMMdd%\n,\n          \nxasecure.audit.hdfs.async.max.flush.interval.ms\n: \n30000\n,\n          \nxasecure.audit.hdfs.async.max.queue.size\n: \n1048576\n,\n          \nxasecure.audit.hdfs.config.destination.file\n: \n%hostname%-audit.log\n,\n          \nxasecure.audit.hdfs.config.destination.flush.interval.seconds\n: \n900\n,\n          \nxasecure.audit.hdfs.config.destination.open.retry.interval.seconds\n: \n60\n,\n          \nxasecure.audit.hdfs.config.destination.rollover.interval.seconds\n: \n86400\n,\n          \nxasecure.audit.hdfs.config.encoding\n: \n,\n          \nxasecure.audit.hdfs.config.local.archive.directory\n: \n/var/log/hadoop/audit/archive/%app-type%\n,\n          \nxasecure.audit.hdfs.config.local.archive.max.file.count\n: \n10\n,\n          \nxasecure.audit.hdfs.config.local.buffer.directory\n: \n/var/log/hadoop/audit/%app-type%\n,\n          \nxasecure.audit.hdfs.config.local.buffer.file\n: \n%time:yyyyMMdd-HHmm.ss%.log\n,\n          \nxasecure.audit.hdfs.config.local.buffer.file.buffer.size.bytes\n: \n8192\n,\n          \nxasecure.audit.hdfs.config.local.buffer.flush.interval.seconds\n: \n60\n,\n          \nxasecure.audit.hdfs.config.local.buffer.rollover.interval.seconds\n: \n600\n,\n          \nxasecure.audit.hdfs.is.async\n: \ntrue\n,\n          \nxasecure.audit.is.enabled\n: \ntrue\n,\n          \nxasecure.audit.jpa.javax.persistence.jdbc.driver\n: \n{{jdbc_driver}}\n,\n          \nxasecure.audit.jpa.javax.persistence.jdbc.url\n: \n{{audit_jdbc_url}}\n,\n          \nxasecure.audit.jpa.javax.persistence.jdbc.user\n: \n{{xa_audit_db_user}}\n,\n          \nxasecure.audit.kafka.async.max.flush.interval.ms\n: \n1000\n,\n          \nxasecure.audit.kafka.async.max.queue.size\n: \n1\n,\n          \nxasecure.audit.kafka.broker_list\n: \nlocalhost:9092\n,\n          \nxasecure.audit.kafka.is.enabled\n: \nfalse\n,\n          \nxasecure.audit.kafka.topic_name\n: \nranger_audits\n,\n          \nxasecure.audit.log4j.async.max.flush.interval.ms\n: \n30000\n,\n          \nxasecure.audit.log4j.async.max.queue.size\n: \n10240\n,\n          \nxasecure.audit.log4j.is.async\n: \nfalse\n,\n          \nxasecure.audit.log4j.is.enabled\n: \nfalse\n\n        }\n      }\n    },\n    {\n      \nranger-hdfs-plugin-properties\n: {\n        \nproperties_attributes\n: {},\n        \nproperties\n: {\n          \nREPOSITORY_CONFIG_USERNAME\n: \nhadoop\n,\n          \ncommon.name.for.certificate\n: \n-\n,\n          \nhadoop.rpc.protection\n: \n-\n,\n          \npolicy_user\n: \nambari-qa\n,\n          \nranger-hdfs-plugin-enabled\n: \nNo\n\n        }\n      }\n    },\n    {\n      \nusersync-properties\n: {\n        \nproperties_attributes\n: {},\n        \nproperties\n: {}\n      }\n    }\n  ],\n  \nhost_groups\n: [\n    {\n      \ncomponents\n: [\n        {\n          \nname\n: \nNODEMANAGER\n\n        },\n        {\n          \nname\n: \nYARN_CLIENT\n\n        },\n        {\n          \nname\n: \nHDFS_CLIENT\n\n        },\n        {\n          \nname\n: \nHISTORYSERVER\n\n        },\n        {\n          \nname\n: \nMETRICS_MONITOR\n\n        },\n        {\n          \nname\n: \nNAMENODE\n\n        },\n        {\n          \nname\n: \nZOOKEEPER_CLIENT\n\n        },\n        {\n          \nname\n: \nRANGER_ADMIN\n\n        },\n        {\n          \nname\n: \nSECONDARY_NAMENODE\n\n        },\n        {\n          \nname\n: \nMAPREDUCE2_CLIENT\n\n        },\n        {\n          \nname\n: \nZOOKEEPER_SERVER\n\n        },\n        {\n          \nname\n: \nAMBARI_SERVER\n\n        },\n        {\n          \nname\n: \nDATANODE\n\n        },\n        {\n          \nname\n: \nRANGER_USERSYNC\n\n        },\n        {\n          \nname\n: \nAPP_TIMELINE_SERVER\n\n        },\n        {\n          \nname\n: \nMETRICS_COLLECTOR\n\n        },\n        {\n          \nname\n: \nRESOURCEMANAGER\n\n        }\n      ],\n      \nconfigurations\n: [],\n      \nname\n: \nhost_group_1\n,\n      \ncardinality\n: \n1\n\n    }\n  ],\n  \nBlueprints\n: {\n    \nstack_name\n: \nHDP\n,\n    \nstack_version\n: \n2.3\n,\n    \nblueprint_name\n: \nranger-psql-onenode-sample\n\n  }\n}\n\n\n\n\nNotes\n\n\n\n\nRanger plugins cannot be enabled by default in a blueprint due to some Ambari restrictions, so properties like \nranger-hdfs-plugin-enabled\n must be set to \nNo\n and the plugins must be enabled from the Ambari UI with the checkboxes and by restarting the necessary services.\n\n\nIf using the UNIX user sync, it may be necessary in some cases to restart the Ranger Usersync Services after the blueprint installation finished if the UNIX users cannot be seen on the Ranger Admin UI.", 
            "title": "Recipes"
        }, 
        {
            "location": "/recipes/#recipes", 
            "text": "With the help of Cloudbreak it is very easy to provision Hadoop clusters in the cloud from an Apache Ambari blueprint. Cloudbreak built in provisioning doesn't contain every use case, so we are introducing the concept of recipes.  Recipes are basically script extensions to a cluster that run on a set of nodes before or after the Ambari cluster installation. With recipes it's quite easy for example to put a JAR file on the Hadoop classpath or run some custom scripts.  In Cloudbreak we supports two ways to configure recipe, we have downloadable and stored recipes.", 
            "title": "Recipes"
        }, 
        {
            "location": "/recipes/#stored-recipes", 
            "text": "As the name mentions stored recipes are uploaded and stored in Cloudbreak via web interface or shell.  The easiest way to create a custom recipe:   create your own pre and/or post scripts  upload them on shell or web interface", 
            "title": "Stored recipes"
        }, 
        {
            "location": "/recipes/#add-recipe", 
            "text": "On the web interface under  manage recipes  section you should  create new recipe . Please choose between SCRIPT, FILE or URL type plugin, and fill required fields.  To add recipe via shell use the following command:  recipe store --name [recipe-name] --executionType [ONE_NODE|ALL_NODES] --preInstallScriptFile /path/of/the/pre-install-script --postInstallScriptFile /path/of/the/post-install-script  This command has optional parameters:  --description  \"string\" description of the recipe  --timeout  \"integer\" timeout of the script execution  --publicInAccount  \"flag\" flags if the recipe is public in the account  In the background Cloudbreak pushes recipe to Consul key/value store during cluster creation.", 
            "title": "Add recipe"
        }, 
        {
            "location": "/recipes/#downloadable-recipes", 
            "text": "A downloadable recipe should be available on HTTP, HTTPS protocols optionally with basic authentication, or any kind of public Git repository.  This kind of recipe must contain a plugin.toml file, with some basic information about the recipe. Besides this at least a recipe-pre-install or a recipe-post-install script.  Content of plugin.toml:  [plugin]\nname =  [recipe-name] \ndescription =  [description-of-the-recipe] \nversion =  1.0 \nmaintainer_name =  [maintainer-name] \nmaintainer_email =  [maintainer-email] \nwebsite_url =  [website-url]   Pre- and post scripts are regular shell scripts, and must be executable.  To configure recipe or recipe groups in Cloudbreak you have to create a descriptive JSON file and send it to Cloudbreak via our shell. On web interface you don't need to take care of this file.  {\n   name :  [recipe-name] ,\n   description :  [description-of-the-recipe] ,\n   properties : {\n     [key] :  [value] \n  },\n   plugins : {\n       git://github.com/account/recipe.git :  ONE_NODE \n       http://user:password@mydomain.com/my-recipe.tar :  ALL_NODES \n       https://mydomain.com/my-recipe.zip :  ALL_NODES \n  }\n}  At this point we need to understand some element of the JSON above.  First of all  properties . Properties are saved to Consul key/value store, and they are available from the pre or post script by fetching http://localhost:8500/v1/kv/[key]?raw. This option is a good choice if you want to write reusable recipes.  The next one is  plugins . As you read before we support a few kind of protocols, and each of them has their own limitations:    Git   git repository must be public (or available from the cluster)  the recipe files must be on the root  only repository default branch supported, there is no opportunity to check out different branch     HTTP(S)   on this kind of protocols you have to bundle your recipe into a tar or zip file  basic authentication is the only way to protect recipe from public     Last one is the execution type of the recipe. We supports two options:   ONE_NODE means the recipe will execute only one node in the hostgroup  All_NODES runs every single instance in the hostgroup.", 
            "title": "Downloadable recipes"
        }, 
        {
            "location": "/recipes/#add-recipe_1", 
            "text": "On the web interface please select URL type plugin, and fill other required fields.  To add recipe via shell use the command(s) below:  recipe add --file /path/of/the/recipe/json  or  recipe add --url http(s)://mydomain.com/my-recipe.json  Add command has an optional parameter  --publicInAccount  is checked all the users belonging to your account will be able to use this recipe for create clusters, but cannot delete it.", 
            "title": "Add recipe"
        }, 
        {
            "location": "/recipes/#sample-recipe-for-ranger", 
            "text": "To be able to install Ranger from a blueprint, a database must be running when Ambari starts to install Ranger Admin. With Cloudbreak a database can be configured and started from a recipe. We've created a sample recipe that can be used to initialize and start a PostgreSQL database that will be able to accept connections from Ranger and store its data. Add the  ONE_NODE  recipe from  this URL  on the Cloudbreak UI:   And add this recipe to the same hostgroup where Ranger Admin is installed on the 'Choose Blueprint' when creating a new cluster:   Ranger installation also has some required properties that must be added to the blueprint. We've created a sample one-node blueprint with the necessary configurations to install Ranger Admin and Ranger Usersync. The configuration values in this blueprint match the sample recipe above - they are set to use a PostgreSQL database on the same host where Ranger Admin is installed. Usersync is configured to use UNIX as the authentication method and it should also be installed on the same host where Ranger Admin is installed.  {\n   configurations : [\n    {\n       ranger-site : {\n         properties_attributes : {},\n         properties : {}\n      }\n    },\n    {\n       ranger-hdfs-policymgr-ssl : {\n         properties_attributes : {},\n         properties : {\n           xasecure.policymgr.clientssl.keystore :  /etc/hadoop/conf/ranger-plugin-keystore.jks ,\n           xasecure.policymgr.clientssl.keystore.credential.file :  jceks://file{{credential_file}} ,\n           xasecure.policymgr.clientssl.truststore :  /etc/hadoop/conf/ranger-plugin-truststore.jks ,\n           xasecure.policymgr.clientssl.truststore.credential.file :  jceks://file{{credential_file}} \n        }\n      }\n    },\n    {\n       ranger-ugsync-site : {\n         properties_attributes : {},\n         properties : {\n           ranger.usersync.enabled :  true ,\n           ranger.usersync.filesource.file :  /tmp/usergroup.txt ,\n           ranger.usersync.filesource.text.delimiter :  , ,\n           ranger.usersync.group.memberattributename :  member ,\n           ranger.usersync.group.nameattribute :  cn ,\n           ranger.usersync.group.objectclass :  groupofnames ,\n           ranger.usersync.group.searchbase :  ou=groups,dc=hadoop,dc=apache,dc=org ,\n           ranger.usersync.group.searchenabled :  false ,\n           ranger.usersync.group.searchfilter :  empty ,\n           ranger.usersync.group.searchscope :  sub ,\n           ranger.usersync.group.usermapsyncenabled :  false ,\n           ranger.usersync.ldap.bindalias :  testldapalias ,\n           ranger.usersync.ldap.binddn :  cn=admin,dc=xasecure,dc=net ,\n           ranger.usersync.ldap.bindkeystore :  - ,\n           ranger.usersync.ldap.groupname.caseconversion :  lower ,\n           ranger.usersync.ldap.searchBase :  dc=hadoop,dc=apache,dc=org ,\n           ranger.usersync.ldap.url :  ldap://localhost:389 ,\n           ranger.usersync.ldap.user.groupnameattribute :  memberof, ismemberof ,\n           ranger.usersync.ldap.user.nameattribute :  cn ,\n           ranger.usersync.ldap.user.objectclass :  person ,\n           ranger.usersync.ldap.user.searchbase :  ou=users,dc=xasecure,dc=net ,\n           ranger.usersync.ldap.user.searchfilter :  empty ,\n           ranger.usersync.ldap.user.searchscope :  sub ,\n           ranger.usersync.ldap.username.caseconversion :  lower ,\n           ranger.usersync.logdir :  /var/log/ranger/usersync ,\n           ranger.usersync.pagedresultsenabled :  true ,\n           ranger.usersync.pagedresultssize :  500 ,\n           ranger.usersync.policymanager.baseURL :  {{ranger_external_url}} ,\n           ranger.usersync.policymanager.maxrecordsperapicall :  1000 ,\n           ranger.usersync.policymanager.mockrun :  false ,\n           ranger.usersync.port :  5151 ,\n           ranger.usersync.sink.impl.class :  org.apache.ranger.unixusersync.process.PolicyMgrUserGroupBuilder ,\n           ranger.usersync.sleeptimeinmillisbetweensynccycle :  5 ,\n           ranger.usersync.source.impl.class :  org.apache.ranger.unixusersync.process.UnixUserGroupBuilder ,\n           ranger.usersync.ssl :  true ,\n           ranger.usersync.unix.minUserId :  500 \n        }\n      }\n    },\n    {\n       admin-properties : {\n         properties_attributes : {},\n         properties : {\n           DB_FLAVOR :  POSTGRES ,\n           SQL_COMMAND_INVOKER :  psql ,\n           SQL_CONNECTOR_JAR :  /var/lib/ambari-agent/tmp/postgres-jdbc-driver.jar ,\n           audit_db_name :  ranger_audit ,\n           audit_db_user :  rangerlogger ,\n           db_host :  localhost:5432 ,\n           db_name :  ranger ,\n           db_root_user :  postgres ,\n           db_root_password :  admin ,\n           db_user :  rangeradmin ,\n           policymgr_external_url :  http://localhost:6080 ,\n           ranger_jdbc_connection_url :  jdbc:postgresql://{db_host}/ranger ,\n           ranger_jdbc_driver :  org.postgresql.Driver \n        }\n      }\n    },\n    {\n       ranger-admin-site : {\n         properties_attributes : {},\n         properties : {\n           ranger.audit.source.type :  db ,\n           ranger.authentication.method :  UNIX ,\n           ranger.credential.provider.path :  /etc/ranger/admin/rangeradmin.jceks ,\n           ranger.externalurl :  {{ranger_external_url}} ,\n           ranger.https.attrib.keystore.file :  /etc/ranger/admin/keys/server.jks ,\n           ranger.jpa.audit.jdbc.credential.alias :  rangeraudit ,\n           ranger.jpa.audit.jdbc.dialect :  {{jdbc_dialect}} ,\n           ranger.jpa.audit.jdbc.driver :  {{jdbc_driver}} ,\n           ranger.jpa.audit.jdbc.url :  {{audit_jdbc_url}} ,\n           ranger.jpa.audit.jdbc.user :  {{ranger_audit_db_user}} ,\n           ranger.jpa.jdbc.credential.alias :  rangeradmin ,\n           ranger.jpa.jdbc.dialect :  {{jdbc_dialect}} ,\n           ranger.jpa.jdbc.driver :  org.postgresql.Driver ,\n           ranger.jpa.jdbc.url :  jdbc:postgresql://localhost:5432/ranger ,\n           ranger.jpa.jdbc.user :  {{ranger_db_user}} ,\n           ranger.jpa.jdbc.password :  {{ranger_db_password}} ,\n           ranger.ldap.ad.domain :  localhost ,\n           ranger.ldap.ad.url :  ldap://ad.xasecure.net:389 ,\n           ranger.ldap.group.roleattribute :  cn ,\n           ranger.ldap.group.searchbase :  ou=groups,dc=xasecure,dc=net ,\n           ranger.ldap.group.searchfilter :  (member=uid={0},ou=users,dc=xasecure,dc=net) ,\n           ranger.ldap.url :  ldap://localhost:389 ,\n           ranger.ldap.user.dnpattern :  uid={0},ou=users,dc=xasecure,dc=net ,\n           ranger.service.host :  {{ranger_host}} ,\n           ranger.service.http.enabled :  true ,\n           ranger.service.http.port :  6080 ,\n           ranger.service.https.attrib.clientAuth :  false ,\n           ranger.service.https.attrib.keystore.keyalias :  mkey ,\n           ranger.service.https.attrib.keystore.pass :  ranger ,\n           ranger.service.https.attrib.ssl.enabled :  false ,\n           ranger.service.https.port :  6182 ,\n           ranger.unixauth.remote.login.enabled :  true ,\n           ranger.unixauth.service.hostname :  localhost ,\n           ranger.unixauth.service.port :  5151 \n        }\n      }\n    },\n    {\n       ranger-env : {\n         properties_attributes : {},\n         properties : {\n           admin_username :  admin ,\n           create_db_dbuser :  true ,\n           ranger_admin_log_dir :  /var/log/ranger/admin ,\n           ranger_admin_username :  amb_ranger_admin ,\n           ranger_admin_password :  amb_ranger_pw ,\n           ranger_group :  ranger ,\n           ranger_jdbc_connection_url :  {{ranger_jdbc_connection_url}} ,\n           ranger_jdbc_driver :  org.postgresql.Driver ,\n           ranger_pid_dir :  /var/run/ranger ,\n           ranger_user :  ranger ,\n           ranger_usersync_log_dir :  /var/log/ranger/usersync ,\n           xml_configurations_supported :  true \n        }\n      }\n    },\n    {\n       ranger-yarn-security : {\n         properties_attributes : {},\n         properties : {\n           ranger.plugin.yarn.policy.cache.dir :  /etc/ranger/{{repo_name}}/policycache ,\n           ranger.plugin.yarn.policy.pollIntervalMs :  30000 ,\n           ranger.plugin.yarn.policy.rest.ssl.config.file :  /etc/yarn/conf/ranger-policymgr-ssl.xml ,\n           ranger.plugin.yarn.policy.rest.url :  {{policymgr_mgr_url}} ,\n           ranger.plugin.yarn.policy.source.impl :  org.apache.ranger.admin.client.RangerAdminRESTClient ,\n           ranger.plugin.yarn.service.name :  {{repo_name}} \n        }\n      }\n    },\n    {\n       ranger-yarn-audit : {\n         properties_attributes : {},\n         properties : {\n           xasecure.audit.credential.provider.file :  jceks://file{{credential_file}} ,\n           xasecure.audit.db.async.max.flush.interval.ms :  30000 ,\n           xasecure.audit.db.async.max.queue.size :  10240 ,\n           xasecure.audit.db.batch.size :  100 ,\n           xasecure.audit.db.is.async :  true ,\n           xasecure.audit.destination.db :  true ,\n           xasecure.audit.hdfs.async.max.flush.interval.ms :  30000 ,\n           xasecure.audit.hdfs.async.max.queue.size :  1048576 ,\n           xasecure.audit.destination.hdfs.dir :  /ranger/audit/%app-type%/%time:yyyyMMdd% ,\n           xasecure.audit.hdfs.config.destination.file :  %hostname%-audit.log ,\n           xasecure.audit.hdfs.config.destination.flush.interval.seconds :  900 ,\n           xasecure.audit.hdfs.config.destination.open.retry.interval.seconds :  60 ,\n           xasecure.audit.hdfs.config.destination.rollover.interval.seconds :  86400 ,\n           xasecure.audit.hdfs.config.encoding :  ,\n           xasecure.audit.hdfs.config.local.archive.directory :  /var/log/yarn/audit/archive ,\n           xasecure.audit.hdfs.config.local.archive.max.file.count :  10 ,\n           xasecure.audit.hdfs.config.local.buffer.directory :  /var/log/yarn/audit ,\n           xasecure.audit.hdfs.config.local.buffer.file :  %time:yyyyMMdd-HHmm.ss%.log ,\n           xasecure.audit.hdfs.config.local.buffer.file.buffer.size.bytes :  8192 ,\n           xasecure.audit.hdfs.config.local.buffer.flush.interval.seconds :  60 ,\n           xasecure.audit.hdfs.config.local.buffer.rollover.interval.seconds :  600 ,\n           xasecure.audit.hdfs.is.async :  true ,\n           xasecure.audit.is.enabled :  true ,\n           xasecure.audit.jpa.javax.persistence.jdbc.driver :  {{jdbc_driver}} ,\n           xasecure.audit.jpa.javax.persistence.jdbc.url :  {{audit_jdbc_url}} ,\n           xasecure.audit.jpa.javax.persistence.jdbc.user :  {{xa_audit_db_user}} ,\n           xasecure.audit.kafka.async.max.flush.interval.ms :  1000 ,\n           xasecure.audit.kafka.async.max.queue.size :  1 ,\n           xasecure.audit.kafka.broker_list :  localhost:9092 ,\n           xasecure.audit.kafka.is.enabled :  false ,\n           xasecure.audit.kafka.topic_name :  ranger_audits ,\n           xasecure.audit.log4j.async.max.flush.interval.ms :  30000 ,\n           xasecure.audit.log4j.async.max.queue.size :  10240 ,\n           xasecure.audit.log4j.is.async :  false ,\n           xasecure.audit.log4j.is.enabled :  false \n        }\n      }\n    },\n    {\n       ranger-hdfs-security : {\n         properties_attributes : {},\n         properties : {\n           ranger.plugin.hdfs.policy.cache.dir :  /etc/ranger/{{repo_name}}/policycache ,\n           ranger.plugin.hdfs.policy.pollIntervalMs :  30000 ,\n           ranger.plugin.hdfs.policy.rest.ssl.config.file :  /etc/hadoop/conf/ranger-policymgr-ssl.xml ,\n           ranger.plugin.hdfs.policy.rest.url :  {{policymgr_mgr_url}} ,\n           ranger.plugin.hdfs.policy.source.impl :  org.apache.ranger.admin.client.RangerAdminRESTClient ,\n           ranger.plugin.hdfs.service.name :  {{repo_name}} ,\n           xasecure.add-hadoop-authorization :  true \n        }\n      }\n    },\n    {\n       ranger-yarn-plugin-properties : {\n         properties_attributes : {},\n         properties : {\n           REPOSITORY_CONFIG_USERNAME :  yarn ,\n           common.name.for.certificate :  - ,\n           hadoop.rpc.protection :  - ,\n           policy_user :  ambari-qa ,\n           ranger-yarn-plugin-enabled :  No \n        }\n      }\n    },\n    {\n       ranger-hdfs-audit : {\n         properties_attributes : {},\n         properties : {\n           xasecure.audit.credential.provider.file :  jceks://file{{credential_file}} ,\n           xasecure.audit.db.async.max.flush.interval.ms :  30000 ,\n           xasecure.audit.db.async.max.queue.size :  10240 ,\n           xasecure.audit.db.batch.size :  100 ,\n           xasecure.audit.db.is.async :  true ,\n           xasecure.audit.destination.db :  true ,\n           xasecure.audit.destination.hdfs.dir :  /ranger/audit/%app-type%/%time:yyyyMMdd% ,\n           xasecure.audit.hdfs.async.max.flush.interval.ms :  30000 ,\n           xasecure.audit.hdfs.async.max.queue.size :  1048576 ,\n           xasecure.audit.hdfs.config.destination.file :  %hostname%-audit.log ,\n           xasecure.audit.hdfs.config.destination.flush.interval.seconds :  900 ,\n           xasecure.audit.hdfs.config.destination.open.retry.interval.seconds :  60 ,\n           xasecure.audit.hdfs.config.destination.rollover.interval.seconds :  86400 ,\n           xasecure.audit.hdfs.config.encoding :  ,\n           xasecure.audit.hdfs.config.local.archive.directory :  /var/log/hadoop/audit/archive/%app-type% ,\n           xasecure.audit.hdfs.config.local.archive.max.file.count :  10 ,\n           xasecure.audit.hdfs.config.local.buffer.directory :  /var/log/hadoop/audit/%app-type% ,\n           xasecure.audit.hdfs.config.local.buffer.file :  %time:yyyyMMdd-HHmm.ss%.log ,\n           xasecure.audit.hdfs.config.local.buffer.file.buffer.size.bytes :  8192 ,\n           xasecure.audit.hdfs.config.local.buffer.flush.interval.seconds :  60 ,\n           xasecure.audit.hdfs.config.local.buffer.rollover.interval.seconds :  600 ,\n           xasecure.audit.hdfs.is.async :  true ,\n           xasecure.audit.is.enabled :  true ,\n           xasecure.audit.jpa.javax.persistence.jdbc.driver :  {{jdbc_driver}} ,\n           xasecure.audit.jpa.javax.persistence.jdbc.url :  {{audit_jdbc_url}} ,\n           xasecure.audit.jpa.javax.persistence.jdbc.user :  {{xa_audit_db_user}} ,\n           xasecure.audit.kafka.async.max.flush.interval.ms :  1000 ,\n           xasecure.audit.kafka.async.max.queue.size :  1 ,\n           xasecure.audit.kafka.broker_list :  localhost:9092 ,\n           xasecure.audit.kafka.is.enabled :  false ,\n           xasecure.audit.kafka.topic_name :  ranger_audits ,\n           xasecure.audit.log4j.async.max.flush.interval.ms :  30000 ,\n           xasecure.audit.log4j.async.max.queue.size :  10240 ,\n           xasecure.audit.log4j.is.async :  false ,\n           xasecure.audit.log4j.is.enabled :  false \n        }\n      }\n    },\n    {\n       ranger-hdfs-plugin-properties : {\n         properties_attributes : {},\n         properties : {\n           REPOSITORY_CONFIG_USERNAME :  hadoop ,\n           common.name.for.certificate :  - ,\n           hadoop.rpc.protection :  - ,\n           policy_user :  ambari-qa ,\n           ranger-hdfs-plugin-enabled :  No \n        }\n      }\n    },\n    {\n       usersync-properties : {\n         properties_attributes : {},\n         properties : {}\n      }\n    }\n  ],\n   host_groups : [\n    {\n       components : [\n        {\n           name :  NODEMANAGER \n        },\n        {\n           name :  YARN_CLIENT \n        },\n        {\n           name :  HDFS_CLIENT \n        },\n        {\n           name :  HISTORYSERVER \n        },\n        {\n           name :  METRICS_MONITOR \n        },\n        {\n           name :  NAMENODE \n        },\n        {\n           name :  ZOOKEEPER_CLIENT \n        },\n        {\n           name :  RANGER_ADMIN \n        },\n        {\n           name :  SECONDARY_NAMENODE \n        },\n        {\n           name :  MAPREDUCE2_CLIENT \n        },\n        {\n           name :  ZOOKEEPER_SERVER \n        },\n        {\n           name :  AMBARI_SERVER \n        },\n        {\n           name :  DATANODE \n        },\n        {\n           name :  RANGER_USERSYNC \n        },\n        {\n           name :  APP_TIMELINE_SERVER \n        },\n        {\n           name :  METRICS_COLLECTOR \n        },\n        {\n           name :  RESOURCEMANAGER \n        }\n      ],\n       configurations : [],\n       name :  host_group_1 ,\n       cardinality :  1 \n    }\n  ],\n   Blueprints : {\n     stack_name :  HDP ,\n     stack_version :  2.3 ,\n     blueprint_name :  ranger-psql-onenode-sample \n  }\n}  Notes   Ranger plugins cannot be enabled by default in a blueprint due to some Ambari restrictions, so properties like  ranger-hdfs-plugin-enabled  must be set to  No  and the plugins must be enabled from the Ambari UI with the checkboxes and by restarting the necessary services.  If using the UNIX user sync, it may be necessary in some cases to restart the Ranger Usersync Services after the blueprint installation finished if the UNIX users cannot be seen on the Ranger Admin UI.", 
            "title": "Sample recipe for Ranger"
        }, 
        {
            "location": "/blueprints/", 
            "text": "Blueprints\n\n\nCloudbreak provides a list of default Hadoop cluster Blueprints for your convenience. However you can always build and use your own Blueprint.\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nServices\n\n\nSource\n\n\n\n\n\n\n\n\n\n\nhdp-small-default\n\n\nLaunch a multi-node HDP 2.4 cluster.\n\n\nHDFS, YARN, MAPREDUCE2, KNOX, HBASE, HIVE, HCATALOG, WEBHCAT, SLIDER, OOZIE, PIG, SQOOP, METRICS, TEZ, FALCON, ZOOKEEPER\n\n\nhdp-small-default.bp\n\n\n\n\n\n\nhdp-streaming-cluster\n\n\nLaunch a multi-node HDP 2.4 cluster optimized for streaming.\n\n\nHDFS, YARN, MAPREDUCE2, STORM, KNOX, HBASE, HIVE, HCATALOG, WEBHCAT, SLIDER, OOZIE, PIG, SQOOP, METRICS, TEZ, FALCON, ZOOKEEPER\n\n\nhdp-streaming-cluster.bp\n\n\n\n\n\n\nhdp-spark-cluster\n\n\nLaunch a multi-node HDP 2.4 cluster optimized for Spark analytic jobs.\n\n\nHDFS, YARN, MAPREDUCE2, SPARK, ZEPPELIN, KNOX, HBASE, HIVE, HCATALOG, WEBHCAT, SLIDER, OOZIE, PIG, SQOOP, METRICS, TEZ, FALCON, ZOOKEEPER\n\n\nhdp-spark-cluster.bp\n\n\n\n\n\n\n\n\nComponents\n\n\nAmbari supports the concept of stacks and associated services in a stack definition. By leveraging the stack definition, Ambari has a consistent and defined interface to install, manage and monitor a set of services and provides extensibility model for new stacks and services to be introduced.\n\n\nAt high level the supported list of components can be grouped into main categories: Master and Slave - and bundling them together form a Hadoop Service.\n\n\n\n\n\n\n\n\nServices\n\n\nComponents\n\n\n\n\n\n\n\n\n\n\nHDFS\n\n\nDATANODE, HDFS_CLIENT, JOURNALNODE, NAMENODE, SECONDARY_NAMENODE, ZKFC\n\n\n\n\n\n\nYARN\n\n\nAPP_TIMELINE_SERVER, NODEMANAGER, RESOURCEMANAGER, YARN_CLIENT\n\n\n\n\n\n\nMAPREDUCE2\n\n\nHISTORYSERVER, MAPREDUCE2_CLIENT\n\n\n\n\n\n\nHBASE\n\n\nHBASE_CLIENT, HBASE_MASTER, HBASE_REGIONSERVER\n\n\n\n\n\n\nHIVE\n\n\nHIVE_CLIENT, HIVE_METASTORE, HIVE_SERVER, MYSQL_SERVER\n\n\n\n\n\n\nHCATALOG\n\n\nHCAT\n\n\n\n\n\n\nWEBHCAT\n\n\nWEBHCAT_SERVER\n\n\n\n\n\n\nOOZIE\n\n\nOOZIE_CLIENT, OOZIE_SERVER\n\n\n\n\n\n\nPIG\n\n\nPIG\n\n\n\n\n\n\nSQOOP\n\n\nSQOOP\n\n\n\n\n\n\nSTORM\n\n\nDRPC_SERVER, NIMBUS, STORM_REST_API, STORM_UI_SERVER, SUPERVISOR\n\n\n\n\n\n\nTEZ\n\n\nTEZ_CLIENT\n\n\n\n\n\n\nFALCON\n\n\nFALCON_CLIENT, FALCON_SERVER\n\n\n\n\n\n\nZOOKEEPER\n\n\nZOOKEEPER_CLIENT, ZOOKEEPER_SERVER\n\n\n\n\n\n\nSPARK\n\n\nSPARK_JOBHISTORYSERVER, SPARK_CLIENT\n\n\n\n\n\n\nRANGER\n\n\nRANGER_USERSYNC, RANGER_ADMIN\n\n\n\n\n\n\nAMBARI_METRICS\n\n\nAMBARI_METRICS, METRICS_COLLECTOR, METRICS_MONITOR\n\n\n\n\n\n\nKERBEROS\n\n\nKERBEROS_CLIENT\n\n\n\n\n\n\nFLUME\n\n\nFLUME_HANDLER\n\n\n\n\n\n\nKAFKA\n\n\nKAFKA_BROKER\n\n\n\n\n\n\nKNOX\n\n\nKNOX_GATEWAY\n\n\n\n\n\n\nATLAS\n\n\nATLAS\n\n\n\n\n\n\nCLOUDBREAK\n\n\nCLOUDBREAK", 
            "title": "Blueprints"
        }, 
        {
            "location": "/blueprints/#blueprints", 
            "text": "Cloudbreak provides a list of default Hadoop cluster Blueprints for your convenience. However you can always build and use your own Blueprint.     Name  Description  Services  Source      hdp-small-default  Launch a multi-node HDP 2.4 cluster.  HDFS, YARN, MAPREDUCE2, KNOX, HBASE, HIVE, HCATALOG, WEBHCAT, SLIDER, OOZIE, PIG, SQOOP, METRICS, TEZ, FALCON, ZOOKEEPER  hdp-small-default.bp    hdp-streaming-cluster  Launch a multi-node HDP 2.4 cluster optimized for streaming.  HDFS, YARN, MAPREDUCE2, STORM, KNOX, HBASE, HIVE, HCATALOG, WEBHCAT, SLIDER, OOZIE, PIG, SQOOP, METRICS, TEZ, FALCON, ZOOKEEPER  hdp-streaming-cluster.bp    hdp-spark-cluster  Launch a multi-node HDP 2.4 cluster optimized for Spark analytic jobs.  HDFS, YARN, MAPREDUCE2, SPARK, ZEPPELIN, KNOX, HBASE, HIVE, HCATALOG, WEBHCAT, SLIDER, OOZIE, PIG, SQOOP, METRICS, TEZ, FALCON, ZOOKEEPER  hdp-spark-cluster.bp", 
            "title": "Blueprints"
        }, 
        {
            "location": "/blueprints/#components", 
            "text": "Ambari supports the concept of stacks and associated services in a stack definition. By leveraging the stack definition, Ambari has a consistent and defined interface to install, manage and monitor a set of services and provides extensibility model for new stacks and services to be introduced.  At high level the supported list of components can be grouped into main categories: Master and Slave - and bundling them together form a Hadoop Service.     Services  Components      HDFS  DATANODE, HDFS_CLIENT, JOURNALNODE, NAMENODE, SECONDARY_NAMENODE, ZKFC    YARN  APP_TIMELINE_SERVER, NODEMANAGER, RESOURCEMANAGER, YARN_CLIENT    MAPREDUCE2  HISTORYSERVER, MAPREDUCE2_CLIENT    HBASE  HBASE_CLIENT, HBASE_MASTER, HBASE_REGIONSERVER    HIVE  HIVE_CLIENT, HIVE_METASTORE, HIVE_SERVER, MYSQL_SERVER    HCATALOG  HCAT    WEBHCAT  WEBHCAT_SERVER    OOZIE  OOZIE_CLIENT, OOZIE_SERVER    PIG  PIG    SQOOP  SQOOP    STORM  DRPC_SERVER, NIMBUS, STORM_REST_API, STORM_UI_SERVER, SUPERVISOR    TEZ  TEZ_CLIENT    FALCON  FALCON_CLIENT, FALCON_SERVER    ZOOKEEPER  ZOOKEEPER_CLIENT, ZOOKEEPER_SERVER    SPARK  SPARK_JOBHISTORYSERVER, SPARK_CLIENT    RANGER  RANGER_USERSYNC, RANGER_ADMIN    AMBARI_METRICS  AMBARI_METRICS, METRICS_COLLECTOR, METRICS_MONITOR    KERBEROS  KERBEROS_CLIENT    FLUME  FLUME_HANDLER    KAFKA  KAFKA_BROKER    KNOX  KNOX_GATEWAY    ATLAS  ATLAS    CLOUDBREAK  CLOUDBREAK", 
            "title": "Components"
        }, 
        {
            "location": "/topologies/", 
            "text": "Platforms\n\n\n\n\nThis feature is currently \nTECHNICAL PREVIEW\n.\n\n\n\n\nWith this feature you can define platform tag that can be attached to Credentials, Networks and Templates. This way you \ncan bundle together different configurations.\n\n\nData locality and topologies\n\n\nThe \nOpenStack documentation\n about data locality:\n\n\n\n\nIt is extremely important for data processing to do locally (on the same rack, OpenStack compute node or even VM) \nas much work as possible. Hadoop supports data-locality feature and can schedule jobs to tasktracker nodes that are local for input stream. In this case tasktracker could communicate directly with local data node.\n\n\n\n\nOpenStack Topology Mapping\n\n\nTopology mapping can be attached to the Platform definition, that associates the hypervisors with racks. \nYou can set the mapping on the Cloudbreak Web UI by defining this line by line or uploading in a file.\n\n\nThe \nmapping file\n should have the following format:\n\n\nhypervisor1 /rack1\nhypervisor2 /rack2\nhypervisor3 /rack2\n\n\n\nBased on this mapping the Cloudbreak application could ensure that the rack information of the started VMs will be passed to Hadoop services via Ambari.\n\n\nManaging Platforms\n\n\nCloudbreak UI\n\n\n\n\nYou can log into the Cloudbreak application at http://PUBLIC_IP:3000. You can find the provider specific \ndocumentations here:\n\n\n\n\nAWS\n\n\nAzure\n\n\nGCP\n\n\nOpenStack\n\n\n\n\n\n\nYou can find the \nmanage platforms\n expandable section on the Cloudbreak Dashboard. With the help of \ncreate \nplatform\n button you can open the platform creation form. \nPlatform name is the only one \nrequired parameter.\n\n\n\n\nOpenStack Topology Mapping\n is optional on the \nmanage platforms\n.\n\n\n\nCloudbreak Shell\n\n\n\n\nStart the shell with \ncbd util cloudbreak-shell\n on a console. This will launch the Cloudbreak shell inside a Docker\n container and you are ready to start using it. You can find more details at the \nCloudbreak Shell\n page.\n\n\n\n\n# Creating a Platform\nplatform create --AWS --name platform-name --description 'description of the platform'\n\n# Creating OpenStack Platform with topology mapping\nplatform create --OPENSTACK --name platform-name --description 'openstack platform' --file file_path\nplatform create --OPENSTACK --name platform-name --description 'openstack platform' --url url_to_file\n\n\n\n\nCredential, Network and Template Creation\n\n\nCredential, Network and Template creation forms and the related shell commands have option to set a platform for \nthe resource.\n\n\nCloudbreak UI\n\n\nExample network resource creation an the UI with the option of selecting a Platform:\n\n\n\nCloudbreak Shell\n\n\n# Example shell command to create Network resource with a connected Platform\nnetwork create --AWS --name aws-network --subnet 10.10.10.0/24 --description 'example network' --platformId 26\n\n\n\n\nCluster Creation on UI\n\n\nIf platform has assigned to the selected credential, then only the associated networks and templates can be selected \nduring cluster creation.", 
            "title": "Platforms"
        }, 
        {
            "location": "/topologies/#platforms", 
            "text": "This feature is currently  TECHNICAL PREVIEW .   With this feature you can define platform tag that can be attached to Credentials, Networks and Templates. This way you \ncan bundle together different configurations.", 
            "title": "Platforms"
        }, 
        {
            "location": "/topologies/#data-locality-and-topologies", 
            "text": "The  OpenStack documentation  about data locality:   It is extremely important for data processing to do locally (on the same rack, OpenStack compute node or even VM) \nas much work as possible. Hadoop supports data-locality feature and can schedule jobs to tasktracker nodes that are local for input stream. In this case tasktracker could communicate directly with local data node.", 
            "title": "Data locality and topologies"
        }, 
        {
            "location": "/topologies/#openstack-topology-mapping", 
            "text": "Topology mapping can be attached to the Platform definition, that associates the hypervisors with racks. \nYou can set the mapping on the Cloudbreak Web UI by defining this line by line or uploading in a file.  The  mapping file  should have the following format:  hypervisor1 /rack1\nhypervisor2 /rack2\nhypervisor3 /rack2  Based on this mapping the Cloudbreak application could ensure that the rack information of the started VMs will be passed to Hadoop services via Ambari.", 
            "title": "OpenStack Topology Mapping"
        }, 
        {
            "location": "/topologies/#managing-platforms", 
            "text": "", 
            "title": "Managing Platforms"
        }, 
        {
            "location": "/topologies/#cloudbreak-ui", 
            "text": "You can log into the Cloudbreak application at http://PUBLIC_IP:3000. You can find the provider specific \ndocumentations here:   AWS  Azure  GCP  OpenStack    You can find the  manage platforms  expandable section on the Cloudbreak Dashboard. With the help of  create \nplatform  button you can open the platform creation form.  Platform name is the only one \nrequired parameter.   OpenStack Topology Mapping  is optional on the  manage platforms .", 
            "title": "Cloudbreak UI"
        }, 
        {
            "location": "/topologies/#cloudbreak-shell", 
            "text": "Start the shell with  cbd util cloudbreak-shell  on a console. This will launch the Cloudbreak shell inside a Docker\n container and you are ready to start using it. You can find more details at the  Cloudbreak Shell  page.   # Creating a Platform\nplatform create --AWS --name platform-name --description 'description of the platform'\n\n# Creating OpenStack Platform with topology mapping\nplatform create --OPENSTACK --name platform-name --description 'openstack platform' --file file_path\nplatform create --OPENSTACK --name platform-name --description 'openstack platform' --url url_to_file", 
            "title": "Cloudbreak Shell"
        }, 
        {
            "location": "/topologies/#credential-network-and-template-creation", 
            "text": "Credential, Network and Template creation forms and the related shell commands have option to set a platform for \nthe resource.", 
            "title": "Credential, Network and Template Creation"
        }, 
        {
            "location": "/topologies/#cloudbreak-ui_1", 
            "text": "Example network resource creation an the UI with the option of selecting a Platform:", 
            "title": "Cloudbreak UI"
        }, 
        {
            "location": "/topologies/#cloudbreak-shell_1", 
            "text": "# Example shell command to create Network resource with a connected Platform\nnetwork create --AWS --name aws-network --subnet 10.10.10.0/24 --description 'example network' --platformId 26", 
            "title": "Cloudbreak Shell"
        }, 
        {
            "location": "/topologies/#cluster-creation-on-ui", 
            "text": "If platform has assigned to the selected credential, then only the associated networks and templates can be selected \nduring cluster creation.", 
            "title": "Cluster Creation on UI"
        }, 
        {
            "location": "/kerberos/", 
            "text": "Kerberos Security\n\n\n\n\nThis feature is currently \nTECHNICAL PREVIEW\n.\n\n\n\n\nCloudbreak can enable Kerberos security on the cluster. When enabled, Cloudbreak will install an MIT KDC into the cluster\nand enable Kerberos on the cluster.\n\n\nEnable Kerberos\n\n\nTo enable Kerberos in a cluster, when creating your cluster via the UI, do the following:\n\n\n\n\nWhen in the \nCreate cluster\n wizard, on the \nSetup Network and Security\n tab, check the \nEnable security\n option.\n\n\nFill in the following fields:\n\n\n\n\n\n\n\n\n\n\nField\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nKerberos master key\n\n\nThe master key to use for the KDC.\n\n\n\n\n\n\nKerberos admin\n\n\nThe KDC admin username to use for the KDC.\n\n\n\n\n\n\nKerberos password\n\n\nThe KDC admin password to use for the KDC.\n\n\n\n\n\n\n\n\n\n\nThe Cloudbreak Kerberos setup does not contain Active Directory support or any other third party user authentication method. If you\nwant to use custom Hadoop user, you have to create users manually with the same name on all Ambari containers on each node.\n\n\n\n\nTesting Kerberos\n\n\nTo run a job on the cluster, you can use one of the default Hadoop users, like \nambari-qa\n, as usual.\n\n\nOnce kerberos is enabled you need a \nticket\n to execute any job on the cluster. Here's an example to get a ticket:\n\n\nkinit -V -kt /etc/security/keytabs/smokeuser.headless.keytab ambari-qa-sparktest-rec@NODE.DC1.CONSUL\n\n\n\n\nExample job:\n\n\nexport HADOOP_LIBS=/usr/hdp/current/hadoop-mapreduce-client\nexport JAR_EXAMPLES=$HADOOP_LIBS/hadoop-mapreduce-examples.jar\nexport JAR_JOBCLIENT=$HADOOP_LIBS/hadoop-mapreduce-client-jobclient.jar\n\nhadoop jar $JAR_EXAMPLES teragen 10000000 /user/ambari-qa/terasort-input\n\nhadoop jar $JAR_JOBCLIENT mrbench -baseDir /user/ambari-qa/smallJobsBenchmark -numRuns 5 -maps 10 -reduces 5 -inputLines 10 -inputType ascending\n\n\n\n\nCreate Hadoop Users\n\n\nTo create Hadoop users please follow the steps below.\n\n\n\n\nLog in via SSH to the Cloudbreak gateway node (IP address is the same as the Ambari UI)\n\n\n\n\nsudo docker exec -i kerberos bash\nkadmin -p [admin_user]/[admin_user]@NODE.DC1.CONSUL (type admin password)\naddprinc custom-user (type user password twice)\n\n\n\n\n\n\nLog in via SSH to all other nodes\n\n\n\n\nsudo docker exec -i $(docker ps | grep ambari-warmup | cut -d\n \n -f 1) bash\nuseradd custom-user\n\n\n\n\n\n\nLog in via SSH to one of the nodes\n\n\n\n\nsudo docker exec -i $(docker ps | grep ambari-warmup | cut -d\n \n -f 1) bash\nsu custom-user\nkinit -p custom-user (type user password)\nhdfs dfs -mkdir input\nhdfs dfs -put /tmp/wait-for-host-number.sh input\nyarn jar $(find /usr/hdp -name hadoop-mapreduce-examples.jar) wordcount input output\nhdfs dfs -cat output/*", 
            "title": "Kerberos"
        }, 
        {
            "location": "/kerberos/#kerberos-security", 
            "text": "This feature is currently  TECHNICAL PREVIEW .   Cloudbreak can enable Kerberos security on the cluster. When enabled, Cloudbreak will install an MIT KDC into the cluster\nand enable Kerberos on the cluster.", 
            "title": "Kerberos Security"
        }, 
        {
            "location": "/kerberos/#enable-kerberos", 
            "text": "To enable Kerberos in a cluster, when creating your cluster via the UI, do the following:   When in the  Create cluster  wizard, on the  Setup Network and Security  tab, check the  Enable security  option.  Fill in the following fields:      Field  Description      Kerberos master key  The master key to use for the KDC.    Kerberos admin  The KDC admin username to use for the KDC.    Kerberos password  The KDC admin password to use for the KDC.      The Cloudbreak Kerberos setup does not contain Active Directory support or any other third party user authentication method. If you\nwant to use custom Hadoop user, you have to create users manually with the same name on all Ambari containers on each node.", 
            "title": "Enable Kerberos"
        }, 
        {
            "location": "/kerberos/#testing-kerberos", 
            "text": "To run a job on the cluster, you can use one of the default Hadoop users, like  ambari-qa , as usual.  Once kerberos is enabled you need a  ticket  to execute any job on the cluster. Here's an example to get a ticket:  kinit -V -kt /etc/security/keytabs/smokeuser.headless.keytab ambari-qa-sparktest-rec@NODE.DC1.CONSUL  Example job:  export HADOOP_LIBS=/usr/hdp/current/hadoop-mapreduce-client\nexport JAR_EXAMPLES=$HADOOP_LIBS/hadoop-mapreduce-examples.jar\nexport JAR_JOBCLIENT=$HADOOP_LIBS/hadoop-mapreduce-client-jobclient.jar\n\nhadoop jar $JAR_EXAMPLES teragen 10000000 /user/ambari-qa/terasort-input\n\nhadoop jar $JAR_JOBCLIENT mrbench -baseDir /user/ambari-qa/smallJobsBenchmark -numRuns 5 -maps 10 -reduces 5 -inputLines 10 -inputType ascending", 
            "title": "Testing Kerberos"
        }, 
        {
            "location": "/kerberos/#create-hadoop-users", 
            "text": "To create Hadoop users please follow the steps below.   Log in via SSH to the Cloudbreak gateway node (IP address is the same as the Ambari UI)   sudo docker exec -i kerberos bash\nkadmin -p [admin_user]/[admin_user]@NODE.DC1.CONSUL (type admin password)\naddprinc custom-user (type user password twice)   Log in via SSH to all other nodes   sudo docker exec -i $(docker ps | grep ambari-warmup | cut -d    -f 1) bash\nuseradd custom-user   Log in via SSH to one of the nodes   sudo docker exec -i $(docker ps | grep ambari-warmup | cut -d    -f 1) bash\nsu custom-user\nkinit -p custom-user (type user password)\nhdfs dfs -mkdir input\nhdfs dfs -put /tmp/wait-for-host-number.sh input\nyarn jar $(find /usr/hdp -name hadoop-mapreduce-examples.jar) wordcount input output\nhdfs dfs -cat output/*", 
            "title": "Create Hadoop Users"
        }, 
        {
            "location": "/mesos/", 
            "text": "Mesos introduction\n\n\n\n\nMesos support is currently \nTECHNICAL PREVIEW\n. It may not be suitable for production use.\n\n\n\n\nThe basic concepts of Cloudbreak's Mesos support are the same as the other other cloud provider implementations: HDP clusters will be provisioned through Ambari with the help of blueprints and Ambari server and agents run in Docker containers. But it has a lot of major differences, and to start working with Cloudbreak on Mesos these differences must be understood first.\n\n\nDifferences with the cloud provider implementations\n\n\n1. The Mesos integration doesn't start new instances and doesn't build a new infrastructure on a cloud provider.\n\n\nCloudbreak's normal behavior is to build the infrastructure first where Hadoop components will be deployed later through Ambari. It involves creating or reusing the networking layer like virtual networks and subnets, provisioning new virtual machines in these networks from pre-existing cloud images and starting the Ambari docker containers on these nodes. Mesos integration was designed \nnot\n to include these steps because in most cases users already have their own Mesos infrastructure and would like to deploy their cluster there, near their other components. That's why Cloudbreak expects to \"bring your own Mesos infrastructure\" and configure access to this Mesos deployment in Cloudbreak first.\n\n\n2. A Mesos credential on the Cloudbreak UI means configuring access to the Marathon API.\n\n\nCloudbreak needs a control system in Mesos through which it can communicate and start Ambari containers. The standard application scheduling framework for services in Mesos is Marathon so we've chosen it as the solution for Cloudbreak. It means that to be able to communicate with Mesos, Cloudbreak needs a Marathon deployment on the Mesos cluster. When setting up access in Cloudbreak a Marathon API endpoint must be specified. Basic authentication and TLS on the Marathon API is not yet supported in the tech preview.\n\n\n3. A Mesos template on the Cloudbreak UI means constraints instead of new resources.\n\n\nCloudbreak templates describe the virtual machines in a cluster's hostgroup that will be provisioned through the cloud provider API. Templates can be created on the UI for Mesos and they can be linked to a hostgroup as well but these templates mean resource constraints that will be demanded through the Marathon API instead of resources that will created. \nExample:\n\n\n\n\nAn AWS template with an instance type of \nm4.large\n and 4 pieces of 50 GB attached magnetic volumes will create a VM with these specs when Cloudbreak is building the cluster infrastructure.\n\n\nA Mesos template with 2 CPU cores and 4 GB memory means that Cloudbreak will request the Marathon API to schedule the Ambari container on a node where these resources can be satisfied\n\n\n\n\n4. Cloudbreak doesn't start a gateway instance.\n\n\nOn the cloud providers there is a gateway VM that's deployed for every new cluster by Cloudbreak. It runs a few containers like Ambari server but most importantly runs an Nginx server. Every communication between a Cloudbreak deployment and a cluster deployed by Cloudbreak goes through this Nginx instance. This is done on a 2-way TLS channel where the Nginx server is responsible for the TLS termination. Communication inside the cluster, like between Ambari server and agents is not encrypted but every communication from outside is secure. It enables Cloudbreak to be deployed outside of the private network of the cluster. The Mesos integration doesn't have a solution like this, so every communication between Cloudbreak and the cluster goes through an unencrypted channel. This is one of the reasons that in this case Cloudbreak should be deployed inside the same private network (or in the same Mesos cluster) where the clusters will be deployed.\n\n\nTechnical Preview Restrictions\n\n\n1. No out-of-the-box dns solution like Consul.\n\n\nIn case of Mesos Cloudbreak does not provide a custom DNS solution like on other cloud providers, where Consul is used to provide addresses for every node and some services like Ambari server. In the Mesos tech preview containers are deployed with \nnet=host\n, and Mesos nodes must be set up manually in a way to be able to resolve each other's hostnames to IP addresses and vice versa with reserve DNS. This is a requirement of Hadoop and it is usually accomplished by setting up the \n/etc/hosts\n file on each node in the cluster, but it can also be provided by some DNS servers like Amazon's default DNS server in a virtual network.\nExample:\n\n\n\n\nthere are 5 nodes in the Mesos cluster: \nnode1, node2, node3, node4 and node5\n with IP addresses of \n10.0.0.1 to 10.0.0.5\n respectively.\n\n\nthe \n/etc/hosts\n file on \nnode1\n should contain these entries:\n\n\n\n\n    10.0.0.2 node2\n    10.0.0.3 node3\n    10.0.0.4 node4\n    10.0.0.5 node5\n\n\n\n\n2. Cloudbreak must be able to resolve the addresses of the Mesos slaves.\n\n\nCloudbreak must be able to communicate with the Ambari server that's deployed in the Mesos cluster to make the API requests needed for example to create a cluster. After Cloudbreak instructs Marathon to deploy the Ambari server container somewhere in the Mesos cluster it asks the address of the node where it was deployed and will try to communicate with the Ambari server through the address that was returned by Marathon. Take for example a Mesos cluster with 5 registered nodes: \nnode1, node2, node3, node4, node5\n:\n\n\n\n\nCloudbreak makes a \nPOST\n request to the Marathon API to deploy the Ambari server container somewhere with enough resources.\n\n\nThe Ambari server container is started on \nnode4\n. The node address is returned to Cloudbreak.\n\n\nCloudbreak tries to access \nnode4:8080\n to check if Ambari is running.\n\n\n\n\nBecause of the lack of a gateway node, communication is unencrypted between Cloudbreak and the clusters, so it is suggested that Cloudbreak should be deployed in the same private network. In that case the above scenario is probably not a problem.\nIf Cloudbreak is not in the same network this can be solved by adding the addresses with a reachable IP in the \n/etc/hosts\n file of the machine where Cloudbreak is deployed.\n\n\n3. Storage management needs to be improved\n\n\nThis is one of the two biggest limitations of the current Mesos integration. There is no specific volume management in the current integration, which means that data is stored inside the Docker containers. This solution has a lot of problems that will be solved only in later releases:\n\n\n\n\nData will only be stored on those volumes where Docker is installed (probably the root volume), attached volumes are not used\n\n\nAfter the container is destroyed the data is destroyed as well\n\n\nNo data locality\n\n\n\n\n4. IP-per-task is not supported yet\n\n\nThe other big limitation of the current integration is the lack of IP-per-task support. Currently containers are deployed with \nnet=host\n which means that only one container can be deployed per Mesos host because of possible port collisions, and that is the case even with multiple clusters.\nIP-per-task means that every task of an app (all the containers) deployed through Marathon will get their own network interface and an IP address.\n\nThis feature\n is already available in Mesos/Marathon but does not work in combination with Docker containers.\n\n\n5. Recipes are not supported\n\n\nRecipes\n are script extensions to an HDP cluster installation supported by Cloudbreak, but it is not supported with the Mesos integration because of the lack of a Consul deployment as this feature is heavily dependent on Consul's HTTP API.\n\n\nCloudbreak deployer\n\n\nCloudbreak Deployer Highlights\n\n\n\n\nAll \ncbd\n actions must be executed from the \ncbd\n root folder.\n\n\nMost of the \ncbd\n commands require \nroot\n permissions. So it would be worth if you apply \nsudo su\n.\n\n\n\n\nSetup Cloudbreak Deployer\n\n\nFirst you should \ninstall the Cloudbreak Deployer\n manually on a VM inside your Mesos cluster's private network.\n\n\nIf you have your own installed VM, you should check the \nInitialize your Profile\n \nsection here before starting the provisioning.\n\n\nOpen the \ncloudbreak-deployment\n directory:\n\n\ncd cloudbreak-deployment\n\n\n\n\nThis is the directory of the configuration files and the supporting binaries for Cloudbreak Deployer.\n\n\nInitialize your Profile\n\n\nFirst initialize \ncbd\n by creating a \nProfile\n file:\n\n\ncbd init\n\n\n\n\nIt will create a \nProfile\n file in the current directory. Please open the \nProfile\n file then check the \nPUBLIC_IP\n. \nThis is mandatory, because it is used to access the Cloudbreak UI. In some cases the \ncbd\n tool tries to \nguess it. If \ncbd\n cannot get the IP address during the initialization, please set the appropriate value.\n\n\nStart Cloudbreak Deployer\n\n\nTo start the Cloudbreak application use the following command.\nThis will start all the Docker containers and initialize the application.\n\n\ncbd start\n\n\n\n\n\n\nAt the very first time it will take for a while, because it will need to download all the necessary docker images.\n\n\n\n\nThe \ncbd start\n command includes the \ncbd generate\n command which applies the following steps:\n\n\n\n\ncreates the \ndocker-compose.yml\n file that describes the configuration of all the Docker containers needed for the Cloudbreak deployment.\n\n\ncreates the \nuaa.yml\n file that holds the configuration of the identity server used to authenticate users to Cloudbreak.\n\n\n\n\nValidate the started Cloudbreak Deployer\n\n\nAfter the \ncbd start\n command finishes followings are worth to check:\n\n\n\n\nPre-installed Cloudbreak Deployer version and health.\n\n\n\n\n   cbd doctor\n\n\n\n\n\n\nIn case of \ncbd update\n is needed, please check the related documentation for \nCloudbreak Deployer Update\n. Most of the \ncbd\n commands require \nroot\n permissions.\n\n\n\n\n\n\nLogs of the Cloudbreak Application:\n\n\n\n\n   cbd logs cloudbreak\n\n\n\n\n\n\nCloudbreak should start within a minute - you should see a line like this: `Started CloudbreakApplication in 36.823 seconds\n\n\n\n\nProvisioning Prerequisites\n\n\nA working Mesos cluster with Marathon\n\n\nIt is not the scope of Cloudbreak to provision a new Mesos cluster so it needs an already working Mesos cluster where it will be able to start HDP clusters. It is also required to have Marathon installed because Cloudbreak uses its API to schedule Docker containers.\n\n\nHostnames must be resolvable inside the Mesos cluster and also by Cloudbreak\n\n\nCloudbreak does not deploy a custom DNS solution like on other cloud providers, where Consul is used to provide addresses for every node. Containers are deployed with \nnet=host\n and Mesos nodes must be set up manually in a way to be able to resolve each other's hostnames to IP addresses and vice versa with reserve DNS. This is a requirement of Hadoop and it is usually accomplished by setting up the \n/etc/hosts\n file on each node in the cluster, but it can also be provided by some DNS servers like Amazon's default DNS server in a virtual network.\n\n\nExample:\n\n\n\n\nIf you have 5 nodes in the Mesos cluster: \nnode1, node2, node3, node4 and node5\n with private IP addresses of \n10.0.0.1 to 10.0.0.5\n respectively, the \n/etc/hosts\n file on \nnode1\n should contain these entries:\n\n\n\n\n    10.0.0.2 node2\n    10.0.0.3 node3\n    10.0.0.4 node4\n    10.0.0.5 node5\n\n\n\n\nDocker must be installed on Mesos slave nodes and Docker containerizer must be enabled\n\n\nTo be able to use the Docker containerizer, Docker must be installed on all the Mesos slave nodes. To install Docker, follow the instructions in their documentation \nhere\n.\n\n\nAfter Docker is installed, it can be configured for the Mesos slave, by adding the \nDocker containerizer\n to each Mesos slave configuration. To configure it, add \ndocker,mesos\n to the file \n/etc/mesos-slave/containerizers\n on each of the slave nodes (or start mesos-slave with the \n--containerizers=mesos,docker\n flag, or set the environment variable MESOS_CONTAINERIZERS=\"mesos,docker\"). You may also want to increase the executor timeout to 10 mins by adding \n10mins\n to \n/etc/mesos-slave/executor_registration_timeout\n because it will allow time for pulling large Docker images.\n\n\nProvisioning via Browser\n\n\nYou can log into the Cloudbreak application at \nhttp://\nPUBLIC_IP\n:3000\n.\n\n\nThe main goal of the Cloudbreak UI is to easily create clusters on your own cloud provider, or on your existing Mesos cluster.\nThis description details the Mesos setup - if you'd like to use a different cloud provider check out its manual.\n\n\nThis document explains the four steps that need to be followed to create Cloudbreak clusters from the UI:\n\n\n\n\nconnect your Marathon API with Cloudbreak\n\n\ncreate some resource constraints on the UI that describe the resources needed by your cluster\n\n\ncreate a blueprint that describes the HDP services in your clusters\n\n\nlaunch the cluster itself based on the resource constraints and the HDP blueprint\n\n\n\n\n\n\nIMPORTANT\n Make sure that you have sufficient quota (CPU, memory) in your Mesos cluster for the requested cluster size.\n\n\n\n\nSetting up Marathon credentials\n\n\nCloudbreak works by connecting your Marathon API through so called \nCredentials\n, and then uses the API to schedule containers on your Mesos cluster. The credentials can be configured on the \nmanage credentials\n panel on the Cloudbreak Dashboard.\n\n\nTo create a new Marathon credential follow these steps:\n\n\n\n\nFill out the new credential \nName\n\n\nOnly alphanumeric and lowercase characters (min 5, max 100 characters) can be applied\n\n\n\n\n\n\nAdd an optional description\n\n\nSpecify the endpoint of your Marathon API in this format: \nhttp://\nmarathon-address\n:\nport\n. Example: \nhttp://172.16.252.31:8080\n.\n\n\n\n\n\n\nPublic in account\n means that all the users belonging to your account will be able to use this credential to create \nclusters, but cannot delete it.\n\n\n\n\nAuthentication and HTTPS to a Marathon API is not yet supported by Cloudbreak\n\n\nResource constraints\n\n\nAfter your Marathon API is linked to Cloudbreak you can start creating resource constraint templates that describe the resources requested through the Marathon API when starting an Ambari container.\n\n\nWhen you create a resource constraint template, \nCloudbreak does not make any requests to Marathon. Resources are only requested after the \ncreate cluster\n button was pushed and Cloudbreak starts to orchestrate containers.\n These templates are saved to Cloudbreak's database and can be reused with multiple clusters to describe the same resource constraints.\n\n\nA typical setup is to combine multiple templates in a cluster for the different types of nodes. For example you may want to request more memory for Spark nodes.\n\n\nThe resource contraint templates can be configured on the \nmanage templates\n panel on the Cloudbreak Dashboard under the Mesos tab. You can specify the memory, CPU and disk needed by the nodes in a hostgroup. If \nPublic in account\n is checked all the users belonging to your account will be able to use this resource to create clusters, but cannot delete it.\n\n\nDefining cluster services\n\n\nBlueprints\n\n\nBlueprints are your declarative definition of a Hadoop cluster. These are the same blueprints that are \nused by Ambari\n.\n\n\nYou can use the 3 default blueprints pre-defined in Cloudbreak or you can create your own ones.\nBlueprints can be added from file, URL (an \nexample blueprint\n) or the \nwhole JSON can be written in the \nJSON text\n box.\n\n\nThe host groups in the JSON will be mapped to a set of instances when starting the cluster. Besides this the services and\n components will also be installed on the corresponding nodes. Blueprints can be modified later from the Ambari UI.\n\n\n\n\nNOTE\n Not necessary to define all the configuration in the blueprint. If a configuration is missing, Ambari will \nfill that with a default value.\n\n\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to use this blueprint to \ncreate clusters, but cannot delete or modify it.\n\n\n\n\nFull size \nhere\n.\n\n\nA blueprint can be exported from a running Ambari cluster that can be reused in Cloudbreak with slight \nmodifications.\n\nThere is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the \nmodifications have to be done manually.\nWhen the blueprint is exported some configurations are hardcoded for example domain names, memory configurations...etc. that won't be applicable to the Cloudbreak cluster\n\n\nCluster deployment\n\n\nAfter all the cluster resources are configured you can deploy a new HDP cluster.\n\n\nHere is a \nbasic flow for cluster creation on Cloudbreak's Web UI\n:\n\n\n\n\n\n\nStart by selecting a previously created Mesos credential in the header.\n\n\n\n\n\n\nClick on \ncreate cluster\n\n\n\n\n\n\nConfigure Cluster\n tab\n\n\n\n\nFill out the new cluster \nname\n\n\nCluster name must start with a lowercase alphabetic character then you can apply lowercase alphanumeric and \n   hyphens only (min 5, max 40 characters)\n\n\n\n\n\n\nClick on the \nChoose Blueprint\n button\n\n\nIf \nPublic in account\n is checked all the users belonging to your account will be able to see the created cluster on\n the UI, but cannot delete or modify it.\n\n\n\n\n\n\n\n\nChoose Blueprint\n tab\n\n\n\n\nSelect one of the blueprints\n\n\nAfter you've selected a \nBlueprint\n, you should be able to configure:\n\n\nthe resource constraints\n\n\nthe number of nodes for all of the host groups in the blueprint\n\n\n\n\n\n\nClick on the \nReview and Launch\n button\n\n\n\n\nReview and Launch\n tab\n\n\n\n\nAfter the \ncreate and start cluster\n button was clicked Cloudbreak will start to orchestrate the Ambari containers through your Marathon API.\n\n\n\n\nYou can check the progress on the Cloudbreak Web UI if you open the new cluster's \nEvent History\n. It is available if you click on the cluster's name.\n\n\nAdvanced options\n\n\nThere are some advanced features when deploying a new cluster, these are the following:\n\n\nValidate blueprint\n This is selected by default. Cloudbreak validates the Ambari blueprint in this case.\n\n\nConfig recommendation strategy\n Strategy for configuration recommendations how will be applied. Recommended \nconfigurations gathered by the response of the stack advisor. \n\n\n\n\nNEVER_APPLY\n               Configuration recommendations are ignored with this option.\n\n\nONLY_STACK_DEFAULTS_APPLY\n Applies only on the default configurations for all included services.\n\n\nALWAYS_APPLY\n              Applies on all configuration properties\n\n\n\n\nCluster termination\n\n\nYou can terminate running or stopped clusters with the \nterminate\n button in the cluster details.\n\n\n\n\nIMPORTANT\n Always use Cloudbreak to terminate the cluster instead of deleting the containers through the Marathon API. Deleting them first would cause inconsistencies between Cloudbreak's database and the real state and that could lead to errors", 
            "title": "Mesos"
        }, 
        {
            "location": "/mesos/#mesos-introduction", 
            "text": "Mesos support is currently  TECHNICAL PREVIEW . It may not be suitable for production use.   The basic concepts of Cloudbreak's Mesos support are the same as the other other cloud provider implementations: HDP clusters will be provisioned through Ambari with the help of blueprints and Ambari server and agents run in Docker containers. But it has a lot of major differences, and to start working with Cloudbreak on Mesos these differences must be understood first.", 
            "title": "Mesos introduction"
        }, 
        {
            "location": "/mesos/#differences-with-the-cloud-provider-implementations", 
            "text": "", 
            "title": "Differences with the cloud provider implementations"
        }, 
        {
            "location": "/mesos/#1-the-mesos-integration-doesnt-start-new-instances-and-doesnt-build-a-new-infrastructure-on-a-cloud-provider", 
            "text": "Cloudbreak's normal behavior is to build the infrastructure first where Hadoop components will be deployed later through Ambari. It involves creating or reusing the networking layer like virtual networks and subnets, provisioning new virtual machines in these networks from pre-existing cloud images and starting the Ambari docker containers on these nodes. Mesos integration was designed  not  to include these steps because in most cases users already have their own Mesos infrastructure and would like to deploy their cluster there, near their other components. That's why Cloudbreak expects to \"bring your own Mesos infrastructure\" and configure access to this Mesos deployment in Cloudbreak first.", 
            "title": "1. The Mesos integration doesn't start new instances and doesn't build a new infrastructure on a cloud provider."
        }, 
        {
            "location": "/mesos/#2-a-mesos-credential-on-the-cloudbreak-ui-means-configuring-access-to-the-marathon-api", 
            "text": "Cloudbreak needs a control system in Mesos through which it can communicate and start Ambari containers. The standard application scheduling framework for services in Mesos is Marathon so we've chosen it as the solution for Cloudbreak. It means that to be able to communicate with Mesos, Cloudbreak needs a Marathon deployment on the Mesos cluster. When setting up access in Cloudbreak a Marathon API endpoint must be specified. Basic authentication and TLS on the Marathon API is not yet supported in the tech preview.", 
            "title": "2. A Mesos credential on the Cloudbreak UI means configuring access to the Marathon API."
        }, 
        {
            "location": "/mesos/#3-a-mesos-template-on-the-cloudbreak-ui-means-constraints-instead-of-new-resources", 
            "text": "Cloudbreak templates describe the virtual machines in a cluster's hostgroup that will be provisioned through the cloud provider API. Templates can be created on the UI for Mesos and they can be linked to a hostgroup as well but these templates mean resource constraints that will be demanded through the Marathon API instead of resources that will created. \nExample:   An AWS template with an instance type of  m4.large  and 4 pieces of 50 GB attached magnetic volumes will create a VM with these specs when Cloudbreak is building the cluster infrastructure.  A Mesos template with 2 CPU cores and 4 GB memory means that Cloudbreak will request the Marathon API to schedule the Ambari container on a node where these resources can be satisfied", 
            "title": "3. A Mesos template on the Cloudbreak UI means constraints instead of new resources."
        }, 
        {
            "location": "/mesos/#4-cloudbreak-doesnt-start-a-gateway-instance", 
            "text": "On the cloud providers there is a gateway VM that's deployed for every new cluster by Cloudbreak. It runs a few containers like Ambari server but most importantly runs an Nginx server. Every communication between a Cloudbreak deployment and a cluster deployed by Cloudbreak goes through this Nginx instance. This is done on a 2-way TLS channel where the Nginx server is responsible for the TLS termination. Communication inside the cluster, like between Ambari server and agents is not encrypted but every communication from outside is secure. It enables Cloudbreak to be deployed outside of the private network of the cluster. The Mesos integration doesn't have a solution like this, so every communication between Cloudbreak and the cluster goes through an unencrypted channel. This is one of the reasons that in this case Cloudbreak should be deployed inside the same private network (or in the same Mesos cluster) where the clusters will be deployed.", 
            "title": "4. Cloudbreak doesn't start a gateway instance."
        }, 
        {
            "location": "/mesos/#technical-preview-restrictions", 
            "text": "", 
            "title": "Technical Preview Restrictions"
        }, 
        {
            "location": "/mesos/#1-no-out-of-the-box-dns-solution-like-consul", 
            "text": "In case of Mesos Cloudbreak does not provide a custom DNS solution like on other cloud providers, where Consul is used to provide addresses for every node and some services like Ambari server. In the Mesos tech preview containers are deployed with  net=host , and Mesos nodes must be set up manually in a way to be able to resolve each other's hostnames to IP addresses and vice versa with reserve DNS. This is a requirement of Hadoop and it is usually accomplished by setting up the  /etc/hosts  file on each node in the cluster, but it can also be provided by some DNS servers like Amazon's default DNS server in a virtual network.\nExample:   there are 5 nodes in the Mesos cluster:  node1, node2, node3, node4 and node5  with IP addresses of  10.0.0.1 to 10.0.0.5  respectively.  the  /etc/hosts  file on  node1  should contain these entries:       10.0.0.2 node2\n    10.0.0.3 node3\n    10.0.0.4 node4\n    10.0.0.5 node5", 
            "title": "1. No out-of-the-box dns solution like Consul."
        }, 
        {
            "location": "/mesos/#2-cloudbreak-must-be-able-to-resolve-the-addresses-of-the-mesos-slaves", 
            "text": "Cloudbreak must be able to communicate with the Ambari server that's deployed in the Mesos cluster to make the API requests needed for example to create a cluster. After Cloudbreak instructs Marathon to deploy the Ambari server container somewhere in the Mesos cluster it asks the address of the node where it was deployed and will try to communicate with the Ambari server through the address that was returned by Marathon. Take for example a Mesos cluster with 5 registered nodes:  node1, node2, node3, node4, node5 :   Cloudbreak makes a  POST  request to the Marathon API to deploy the Ambari server container somewhere with enough resources.  The Ambari server container is started on  node4 . The node address is returned to Cloudbreak.  Cloudbreak tries to access  node4:8080  to check if Ambari is running.   Because of the lack of a gateway node, communication is unencrypted between Cloudbreak and the clusters, so it is suggested that Cloudbreak should be deployed in the same private network. In that case the above scenario is probably not a problem.\nIf Cloudbreak is not in the same network this can be solved by adding the addresses with a reachable IP in the  /etc/hosts  file of the machine where Cloudbreak is deployed.", 
            "title": "2. Cloudbreak must be able to resolve the addresses of the Mesos slaves."
        }, 
        {
            "location": "/mesos/#3-storage-management-needs-to-be-improved", 
            "text": "This is one of the two biggest limitations of the current Mesos integration. There is no specific volume management in the current integration, which means that data is stored inside the Docker containers. This solution has a lot of problems that will be solved only in later releases:   Data will only be stored on those volumes where Docker is installed (probably the root volume), attached volumes are not used  After the container is destroyed the data is destroyed as well  No data locality", 
            "title": "3. Storage management needs to be improved"
        }, 
        {
            "location": "/mesos/#4-ip-per-task-is-not-supported-yet", 
            "text": "The other big limitation of the current integration is the lack of IP-per-task support. Currently containers are deployed with  net=host  which means that only one container can be deployed per Mesos host because of possible port collisions, and that is the case even with multiple clusters.\nIP-per-task means that every task of an app (all the containers) deployed through Marathon will get their own network interface and an IP address. This feature  is already available in Mesos/Marathon but does not work in combination with Docker containers.", 
            "title": "4. IP-per-task is not supported yet"
        }, 
        {
            "location": "/mesos/#5-recipes-are-not-supported", 
            "text": "Recipes  are script extensions to an HDP cluster installation supported by Cloudbreak, but it is not supported with the Mesos integration because of the lack of a Consul deployment as this feature is heavily dependent on Consul's HTTP API.", 
            "title": "5. Recipes are not supported"
        }, 
        {
            "location": "/mesos/#cloudbreak-deployer", 
            "text": "Cloudbreak Deployer Highlights   All  cbd  actions must be executed from the  cbd  root folder.  Most of the  cbd  commands require  root  permissions. So it would be worth if you apply  sudo su .", 
            "title": "Cloudbreak deployer"
        }, 
        {
            "location": "/mesos/#setup-cloudbreak-deployer", 
            "text": "First you should  install the Cloudbreak Deployer  manually on a VM inside your Mesos cluster's private network.  If you have your own installed VM, you should check the  Initialize your Profile  \nsection here before starting the provisioning.  Open the  cloudbreak-deployment  directory:  cd cloudbreak-deployment  This is the directory of the configuration files and the supporting binaries for Cloudbreak Deployer.", 
            "title": "Setup Cloudbreak Deployer"
        }, 
        {
            "location": "/mesos/#initialize-your-profile", 
            "text": "First initialize  cbd  by creating a  Profile  file:  cbd init  It will create a  Profile  file in the current directory. Please open the  Profile  file then check the  PUBLIC_IP . \nThis is mandatory, because it is used to access the Cloudbreak UI. In some cases the  cbd  tool tries to \nguess it. If  cbd  cannot get the IP address during the initialization, please set the appropriate value.", 
            "title": "Initialize your Profile"
        }, 
        {
            "location": "/mesos/#start-cloudbreak-deployer", 
            "text": "To start the Cloudbreak application use the following command.\nThis will start all the Docker containers and initialize the application.  cbd start   At the very first time it will take for a while, because it will need to download all the necessary docker images.   The  cbd start  command includes the  cbd generate  command which applies the following steps:   creates the  docker-compose.yml  file that describes the configuration of all the Docker containers needed for the Cloudbreak deployment.  creates the  uaa.yml  file that holds the configuration of the identity server used to authenticate users to Cloudbreak.", 
            "title": "Start Cloudbreak Deployer"
        }, 
        {
            "location": "/mesos/#validate-the-started-cloudbreak-deployer", 
            "text": "After the  cbd start  command finishes followings are worth to check:   Pre-installed Cloudbreak Deployer version and health.      cbd doctor   In case of  cbd update  is needed, please check the related documentation for  Cloudbreak Deployer Update . Most of the  cbd  commands require  root  permissions.    Logs of the Cloudbreak Application:      cbd logs cloudbreak   Cloudbreak should start within a minute - you should see a line like this: `Started CloudbreakApplication in 36.823 seconds", 
            "title": "Validate the started Cloudbreak Deployer"
        }, 
        {
            "location": "/mesos/#provisioning-prerequisites", 
            "text": "", 
            "title": "Provisioning Prerequisites"
        }, 
        {
            "location": "/mesos/#a-working-mesos-cluster-with-marathon", 
            "text": "It is not the scope of Cloudbreak to provision a new Mesos cluster so it needs an already working Mesos cluster where it will be able to start HDP clusters. It is also required to have Marathon installed because Cloudbreak uses its API to schedule Docker containers.", 
            "title": "A working Mesos cluster with Marathon"
        }, 
        {
            "location": "/mesos/#hostnames-must-be-resolvable-inside-the-mesos-cluster-and-also-by-cloudbreak", 
            "text": "Cloudbreak does not deploy a custom DNS solution like on other cloud providers, where Consul is used to provide addresses for every node. Containers are deployed with  net=host  and Mesos nodes must be set up manually in a way to be able to resolve each other's hostnames to IP addresses and vice versa with reserve DNS. This is a requirement of Hadoop and it is usually accomplished by setting up the  /etc/hosts  file on each node in the cluster, but it can also be provided by some DNS servers like Amazon's default DNS server in a virtual network.  Example:   If you have 5 nodes in the Mesos cluster:  node1, node2, node3, node4 and node5  with private IP addresses of  10.0.0.1 to 10.0.0.5  respectively, the  /etc/hosts  file on  node1  should contain these entries:       10.0.0.2 node2\n    10.0.0.3 node3\n    10.0.0.4 node4\n    10.0.0.5 node5", 
            "title": "Hostnames must be resolvable inside the Mesos cluster and also by Cloudbreak"
        }, 
        {
            "location": "/mesos/#docker-must-be-installed-on-mesos-slave-nodes-and-docker-containerizer-must-be-enabled", 
            "text": "To be able to use the Docker containerizer, Docker must be installed on all the Mesos slave nodes. To install Docker, follow the instructions in their documentation  here .  After Docker is installed, it can be configured for the Mesos slave, by adding the  Docker containerizer  to each Mesos slave configuration. To configure it, add  docker,mesos  to the file  /etc/mesos-slave/containerizers  on each of the slave nodes (or start mesos-slave with the  --containerizers=mesos,docker  flag, or set the environment variable MESOS_CONTAINERIZERS=\"mesos,docker\"). You may also want to increase the executor timeout to 10 mins by adding  10mins  to  /etc/mesos-slave/executor_registration_timeout  because it will allow time for pulling large Docker images.", 
            "title": "Docker must be installed on Mesos slave nodes and Docker containerizer must be enabled"
        }, 
        {
            "location": "/mesos/#provisioning-via-browser", 
            "text": "You can log into the Cloudbreak application at  http:// PUBLIC_IP :3000 .  The main goal of the Cloudbreak UI is to easily create clusters on your own cloud provider, or on your existing Mesos cluster.\nThis description details the Mesos setup - if you'd like to use a different cloud provider check out its manual.  This document explains the four steps that need to be followed to create Cloudbreak clusters from the UI:   connect your Marathon API with Cloudbreak  create some resource constraints on the UI that describe the resources needed by your cluster  create a blueprint that describes the HDP services in your clusters  launch the cluster itself based on the resource constraints and the HDP blueprint    IMPORTANT  Make sure that you have sufficient quota (CPU, memory) in your Mesos cluster for the requested cluster size.", 
            "title": "Provisioning via Browser"
        }, 
        {
            "location": "/mesos/#setting-up-marathon-credentials", 
            "text": "Cloudbreak works by connecting your Marathon API through so called  Credentials , and then uses the API to schedule containers on your Mesos cluster. The credentials can be configured on the  manage credentials  panel on the Cloudbreak Dashboard.  To create a new Marathon credential follow these steps:   Fill out the new credential  Name  Only alphanumeric and lowercase characters (min 5, max 100 characters) can be applied    Add an optional description  Specify the endpoint of your Marathon API in this format:  http:// marathon-address : port . Example:  http://172.16.252.31:8080 .    Public in account  means that all the users belonging to your account will be able to use this credential to create \nclusters, but cannot delete it.   Authentication and HTTPS to a Marathon API is not yet supported by Cloudbreak", 
            "title": "Setting up Marathon credentials"
        }, 
        {
            "location": "/mesos/#resource-constraints", 
            "text": "After your Marathon API is linked to Cloudbreak you can start creating resource constraint templates that describe the resources requested through the Marathon API when starting an Ambari container.  When you create a resource constraint template,  Cloudbreak does not make any requests to Marathon. Resources are only requested after the  create cluster  button was pushed and Cloudbreak starts to orchestrate containers.  These templates are saved to Cloudbreak's database and can be reused with multiple clusters to describe the same resource constraints.  A typical setup is to combine multiple templates in a cluster for the different types of nodes. For example you may want to request more memory for Spark nodes.  The resource contraint templates can be configured on the  manage templates  panel on the Cloudbreak Dashboard under the Mesos tab. You can specify the memory, CPU and disk needed by the nodes in a hostgroup. If  Public in account  is checked all the users belonging to your account will be able to use this resource to create clusters, but cannot delete it.", 
            "title": "Resource constraints"
        }, 
        {
            "location": "/mesos/#defining-cluster-services", 
            "text": "Blueprints  Blueprints are your declarative definition of a Hadoop cluster. These are the same blueprints that are  used by Ambari .  You can use the 3 default blueprints pre-defined in Cloudbreak or you can create your own ones.\nBlueprints can be added from file, URL (an  example blueprint ) or the \nwhole JSON can be written in the  JSON text  box.  The host groups in the JSON will be mapped to a set of instances when starting the cluster. Besides this the services and\n components will also be installed on the corresponding nodes. Blueprints can be modified later from the Ambari UI.   NOTE  Not necessary to define all the configuration in the blueprint. If a configuration is missing, Ambari will \nfill that with a default value.   If  Public in account  is checked all the users belonging to your account will be able to use this blueprint to \ncreate clusters, but cannot delete or modify it.   Full size  here .  A blueprint can be exported from a running Ambari cluster that can be reused in Cloudbreak with slight \nmodifications. \nThere is no automatic way to modify an exported blueprint and make it instantly usable in Cloudbreak, the \nmodifications have to be done manually.\nWhen the blueprint is exported some configurations are hardcoded for example domain names, memory configurations...etc. that won't be applicable to the Cloudbreak cluster", 
            "title": "Defining cluster services"
        }, 
        {
            "location": "/mesos/#cluster-deployment", 
            "text": "After all the cluster resources are configured you can deploy a new HDP cluster.  Here is a  basic flow for cluster creation on Cloudbreak's Web UI :    Start by selecting a previously created Mesos credential in the header.    Click on  create cluster    Configure Cluster  tab   Fill out the new cluster  name  Cluster name must start with a lowercase alphabetic character then you can apply lowercase alphanumeric and \n   hyphens only (min 5, max 40 characters)    Click on the  Choose Blueprint  button  If  Public in account  is checked all the users belonging to your account will be able to see the created cluster on\n the UI, but cannot delete or modify it.     Choose Blueprint  tab   Select one of the blueprints  After you've selected a  Blueprint , you should be able to configure:  the resource constraints  the number of nodes for all of the host groups in the blueprint    Click on the  Review and Launch  button   Review and Launch  tab   After the  create and start cluster  button was clicked Cloudbreak will start to orchestrate the Ambari containers through your Marathon API.   You can check the progress on the Cloudbreak Web UI if you open the new cluster's  Event History . It is available if you click on the cluster's name.  Advanced options  There are some advanced features when deploying a new cluster, these are the following:  Validate blueprint  This is selected by default. Cloudbreak validates the Ambari blueprint in this case.  Config recommendation strategy  Strategy for configuration recommendations how will be applied. Recommended \nconfigurations gathered by the response of the stack advisor.    NEVER_APPLY                Configuration recommendations are ignored with this option.  ONLY_STACK_DEFAULTS_APPLY  Applies only on the default configurations for all included services.  ALWAYS_APPLY               Applies on all configuration properties", 
            "title": "Cluster deployment"
        }, 
        {
            "location": "/mesos/#cluster-termination", 
            "text": "You can terminate running or stopped clusters with the  terminate  button in the cluster details.   IMPORTANT  Always use Cloudbreak to terminate the cluster instead of deleting the containers through the Marathon API. Deleting them first would cause inconsistencies between Cloudbreak's database and the real state and that could lead to errors", 
            "title": "Cluster termination"
        }, 
        {
            "location": "/spi/", 
            "text": "Service Provider Interface (SPI)\n\n\nCloudbreak already supports multiple cloud platforms and provides an easy way to integrate a new provider trough \nCloudbreak's Service Provider Interface (SPI)\n which is a plugin mechanism to enable a seamless integration of any cloud provider.\n\n\nThe SPI plugin mechanism has been used to integrate all existing providers to Cloudbreak, therefore if a new provider is integrated it immediately becomes a first class citizen in Cloudbreak.\n\n\n\n\ncloud-aws\n module integrates Amazon Web Services\n\n\ncloud-gcp\n module integrates Google Cloud Platform\n\n\ncloud-arm\n module integrates Microsoft Azure\n\n\ncloud-openstack\n module integrates OpenStack\n\n\n\n\nThe SPI interface is event based, scales well and decoupled from Cloudbreak. The core of Cloudbreak is communicating trough \nEventBus\n with providers, but the complexity of Event handling is hidden from the provider implementation.\n\n\nResource management\n\n\nThere are two kind of deployment/resource management method is supported by cloud providers:\n\n\n\n\ntemplate based deployments\n\n\nindividual resource based deployments\n\n\n\n\nCloudbreak's SPI supports both way of resource management. It provides a well defined interfaces, abstract classes and helper classes like scheduling and polling of resources to aid the integration and to avoid any boilerplate code in the module of cloud provider.\n\n\nTemplate based deployments\n\n\nProviders with template based deployments like \nAWS CloudFormation\n, \nAzure ARM\n or \nOpenStack Heat\n have the ability to create and manage a collection of related cloud resources, provisioning and updating them in an orderly and predictable fashion. This means that Cloudbreak needs a reference to the template itself and every change in the infrastructure (e.g creating new instance or deleting one) is managed through this templating mechanism.\n\n\nIf the provider has templating support then the provider's \ngradle\n module shall depend on the \ncloud-api\n module.\n\n\napply plugin: 'java'\n\nsourceCompatibility = 1.7\n\nrepositories {\n    mavenCentral()\n}\n\njar {\n    baseName = 'cloud-new-provider'\n}\n\ndependencies {\n\n    compile project(':cloud-api')\n\n}\n\n\n\n\nThe entry point of the provider is the  \nCloudConnector\n interface and every interface that needs to be implemented is reachable trough this interface.\n\n\nIndividual resource based deployments\n\n\nProviders like GCP that does not support suitable templating mechanism or for customisable providers like OpenStack where the Heat Orchestration (templating) component optional the individual resources needs to be handlet separately. This means that resources like networks, discs and compute instances needs to be created and managed with an ordered sequence of API calls and Cloudbreak shall provide a solution to manage the collection of related cloud resources together.\n\n\nIf the provider has no templating support then the provider's \ngradle\n module shall depend on the \ncloud-template\n module, that includes Cloudbreak defined abstract template. This template is a set of abstract and utility classes to support provisioning and updating related resources in an orderly and predictable manner trough ordered sequences of cloud API calls.\n\n\napply plugin: 'java'\n\nsourceCompatibility = 1.7\n\nrepositories {\n    mavenCentral()\n}\n\njar {\n    baseName = 'cloud-new-provider'\n}\n\ndependencies {\n\n    compile project(':cloud-template')\n\n}\n\n\n\n\nVariants\n\n\nOpenStack is very modular and allows to install different components for e.g. volume storage or different components for networking (e.g. Nova networking or Neutron) or even you have a chance that some components like Heat are not installed at all.\n\n\nCloudbreak's SPI interface reflects this flexibility using so called variants. This means that if some part of cloud provider (typically OpenStack) is using different component you don't need re-implement the complete stack but just use a different variant and re-implement the part what is different.\n\n\nThe reference implementation for this feature can be found in  \ncloud-openstack\n module which support a HEAT and NATIVE variants. The HEAT variant utilizes the Heat templating to launch a stack, but the NATIVE variant starts the cluster by using a sequence of API calls without Heat to achieve the same result, although both of them are using the same authentication and credential management.\n\n\nDevelopment\n\n\nIn order to set up a development environment please take a look at \nLocal Development Setup\n documentation.", 
            "title": "SPI"
        }, 
        {
            "location": "/spi/#service-provider-interface-spi", 
            "text": "Cloudbreak already supports multiple cloud platforms and provides an easy way to integrate a new provider trough  Cloudbreak's Service Provider Interface (SPI)  which is a plugin mechanism to enable a seamless integration of any cloud provider.  The SPI plugin mechanism has been used to integrate all existing providers to Cloudbreak, therefore if a new provider is integrated it immediately becomes a first class citizen in Cloudbreak.   cloud-aws  module integrates Amazon Web Services  cloud-gcp  module integrates Google Cloud Platform  cloud-arm  module integrates Microsoft Azure  cloud-openstack  module integrates OpenStack   The SPI interface is event based, scales well and decoupled from Cloudbreak. The core of Cloudbreak is communicating trough  EventBus  with providers, but the complexity of Event handling is hidden from the provider implementation.", 
            "title": "Service Provider Interface (SPI)"
        }, 
        {
            "location": "/spi/#resource-management", 
            "text": "There are two kind of deployment/resource management method is supported by cloud providers:   template based deployments  individual resource based deployments   Cloudbreak's SPI supports both way of resource management. It provides a well defined interfaces, abstract classes and helper classes like scheduling and polling of resources to aid the integration and to avoid any boilerplate code in the module of cloud provider.", 
            "title": "Resource management"
        }, 
        {
            "location": "/spi/#template-based-deployments", 
            "text": "Providers with template based deployments like  AWS CloudFormation ,  Azure ARM  or  OpenStack Heat  have the ability to create and manage a collection of related cloud resources, provisioning and updating them in an orderly and predictable fashion. This means that Cloudbreak needs a reference to the template itself and every change in the infrastructure (e.g creating new instance or deleting one) is managed through this templating mechanism.  If the provider has templating support then the provider's  gradle  module shall depend on the  cloud-api  module.  apply plugin: 'java'\n\nsourceCompatibility = 1.7\n\nrepositories {\n    mavenCentral()\n}\n\njar {\n    baseName = 'cloud-new-provider'\n}\n\ndependencies {\n\n    compile project(':cloud-api')\n\n}  The entry point of the provider is the   CloudConnector  interface and every interface that needs to be implemented is reachable trough this interface.", 
            "title": "Template based deployments"
        }, 
        {
            "location": "/spi/#individual-resource-based-deployments", 
            "text": "Providers like GCP that does not support suitable templating mechanism or for customisable providers like OpenStack where the Heat Orchestration (templating) component optional the individual resources needs to be handlet separately. This means that resources like networks, discs and compute instances needs to be created and managed with an ordered sequence of API calls and Cloudbreak shall provide a solution to manage the collection of related cloud resources together.  If the provider has no templating support then the provider's  gradle  module shall depend on the  cloud-template  module, that includes Cloudbreak defined abstract template. This template is a set of abstract and utility classes to support provisioning and updating related resources in an orderly and predictable manner trough ordered sequences of cloud API calls.  apply plugin: 'java'\n\nsourceCompatibility = 1.7\n\nrepositories {\n    mavenCentral()\n}\n\njar {\n    baseName = 'cloud-new-provider'\n}\n\ndependencies {\n\n    compile project(':cloud-template')\n\n}", 
            "title": "Individual resource based deployments"
        }, 
        {
            "location": "/spi/#variants", 
            "text": "OpenStack is very modular and allows to install different components for e.g. volume storage or different components for networking (e.g. Nova networking or Neutron) or even you have a chance that some components like Heat are not installed at all.  Cloudbreak's SPI interface reflects this flexibility using so called variants. This means that if some part of cloud provider (typically OpenStack) is using different component you don't need re-implement the complete stack but just use a different variant and re-implement the part what is different.  The reference implementation for this feature can be found in   cloud-openstack  module which support a HEAT and NATIVE variants. The HEAT variant utilizes the Heat templating to launch a stack, but the NATIVE variant starts the cluster by using a sequence of API calls without Heat to achieve the same result, although both of them are using the same authentication and credential management.", 
            "title": "Variants"
        }, 
        {
            "location": "/spi/#development", 
            "text": "In order to set up a development environment please take a look at  Local Development Setup  documentation.", 
            "title": "Development"
        }, 
        {
            "location": "/operations/", 
            "text": "Operations\n\n\nDebug\n\n\nIf you want to have more detailed output set the \nDEBUG\n env variable to non-zero:\n\n\nDEBUG=1 cbd some_command\n\n\n\n\nTroubleshoot\n\n\nYou can use the \ndoctor\n command to diagnose your environment.\nIt can reveal some common problems with your docker or boot2docker configuration and it also checks the cbd versions.\n\n\ncbd doctor\n\n\n\n\nLogs\n\n\nThe aggregated logs of all the Cloudbreak components can be checked with:\n\n\ncbd logs\n\n\n\n\nIt can also be used to check the logs of an individual docker container. To see only the logs of the Cloudbreak backend:\n\n\ncbd logs cloudbreak\n\n\n\n\nYou can also check the individual logs of \nuluwatu\n, \nperiscope\n, and \nidentity\n.\n\n\nUpdate Cloudbreak Deployer\n\n\nThe cloudbreak-deployer tool is capable of upgrading itself to a newer version.\n\n\nPlease apply the following steps on the console:\n\n\n\n\nUpdate Cloudbreak Deployer \n\n\n\n\n   cbd update\n\n\n\n\n\n\nUpdate the \ndocker-compose.yml\n file with new Docker containers that are needed for the \ncbd\n\n\n\n\n   cbd regenerate\n\n\n\n\n\n\nStart the new version of the \ncbd\n\n\n\n\n   cbd start\n\n\n\n\n\n\nIt will take for a while, because of need to download all the updated docker images for the new version.\n\n\n\n\n\n\nCheck the health and version of the updated \ncbd\n \n\n\n\n\n   cbd doctor\n\n\n\n\n\n\nCheck the started Cloudbreak Application logs\n\n\n\n\n   cbd logs cloudbreak\n\n\n\n\n\n\nCloudbreak should start within a minute - you should see a line like this: \nStarted CloudbreakApplication in 36.823 seconds\n\n\n\n\nSSH to the hosts\n\n\nIn the current version of Cloudbreak all the nodes have a public IP address and all the nodes are accessible via SSH.\nThe public IP addresses of a running cluster can be checked on the Cloudbreak UI under the \nNodes\n tab.\nOnly key-based authentication is supported - the public key can be specified when creating a cloud credential.\n\n\nEach cloud provider has different SSH user. In order to figure out the username you can try it wit the \nroot\n user - that will tell you the correct username.\n\n\nAs an example:\n\n\nssh root@172.16.252.31\nWarning: Permanently added '172.16.252.31' (ECDSA) to the list of known hosts.\nPlease login with one of the following username rather than the user \nroot\n:\n\n  centos                  : Reach the host running the ambari container\n  ambari-enter            : Enters the Ambari container\n  ambari-log              : Watches the Ambari logs\n\n\n\n\nAfter you figured out the username you can SSH into the host.\n\n\nssh -i ~/.ssh/private-key.pem USER_NAME@\npublic-ip\n\n\n\n\n\nAccessing HDP client services\n\n\nThe main difference between general HDP clusters and Cloudbreak-installed HDP clusters is that each host runs an Ambari server or agent Docker container and the HDP services will be installed in this container as well.\nIt means that after \nssh\n the client services won't be available instantly, first you'll have to \nenter\n the ambari-agent container.\nInside the container everything works the same way as expected. In order to do so there are a few options.\n\n\nHelper functions\n\n\nWe have created a few helper functions in order to help operations. Once yoy SSH into the host you should a message as such:\n\n\nYou are now logged in to the docker host ...\nGetting started with docker:\n\n  docker ps                : Listing running containers\n  docker logs -f \nID\n      : Star tail -f on container stdout/stderr\n  docker exec -it \nID\n sh  : Entering the container (instead of ssh)\n\nHelper commands:\n\n  h                        : prints this message\n  ambari-enter             : Enters the ambari container\n  ambari-log               : Watches the ambari logs\n\n\n\n\nYou can enter into the container where the HDP services are running using the \nambari-enter\n functions.\n\n\nDocker exec\n\n\nTo check the containers running on the host enter:\n\n\n[cloudbreak@vmhostgroupclient11 ~]$ sudo docker ps\nCONTAINER ID   IMAGE                                    COMMAND                CREATED      STATUS      PORTS     NAMES\n1098ca778176   sequenceiq/baywatch-client:v1.0.0        \n/etc/bootstrap.sh -d\n 4 hours ago  Up 4 hours            baywatch-client-14454170059514\nf4097c52fda5   sequenceiq/logrotate:v0.5.1              \n/start.sh\n            4 hours ago  Up 4 hours            logrotate-14454169954830\n7b94aedaab30   sequenceiq/docker-consul-watch-plugn:1.0 \n/start.sh consul://1\n 4 hours ago  Up 4 hours            consul-watch-14454169884044\nd8128b001427   sequenceiq/ambari:2.1.2-v2               \n/start-agent\n         4 hours ago  Up 4 hours            ambari-agent-14454169805924\na8ec90037aaf   swarm:0.4.0                              \n/swarm join --addr=1\n 4 hours ago  Up 4 hours  2375/tcp  vmhostgroupmaster12-swarm-agent\nef02b43eacee   sequenceiq/consul:v0.5.0-v5              \n/bin/start\n           4 hours ago  Up 4 hours            vmhostgroupmaster12-consul\n\n\n\n\nYou should see the \nambari-agent\n container running. Copy its id or name and \nexec\n into the container:\n\n\n[cloudbreak@vmhostgroupclient11 ~]$ sudo docker exec -it ambari-agent-14454169805924 bash\n[root@docker-ambari tmp]#\n\n\n\n\nOr you can use this one-step command as well:\n\n\n[cloudbreak@vmhostgroupclient11 ~]$ sudo docker exec -it $(sudo docker ps -f=name=ambari-agent -q) bash\n[root@docker-ambari tmp]#\n\n\n\n\nData volumes\n\n\nThe disks that are attached to the instances are automatically mounted to \n/hadoopfs/fs1\n, \n/hadoopfs/fs2\n, ... \n/hadoopfs/fsN\n respectively.\nThese directories are mounted from the host into the ambari-agent container under the same name so these can be accessed from inside.\nIt means that if you'd like to move some data to the instances you can use these volumes and the data will be available from the container instantly to work on it.\n\n\nAn \nscp\n Example:\n\n\n$ scp -qr -i ~/.ssh/private-key.pem ~/tmp/data cloudbreak@\nclient-node\n:/hadoopfs/fs1\n$ ssh -i ~/.ssh/private-key.pem cloudbreak@\nclient-node\n\n[cloudbreak@vmhostgroupclient11 ~]$ sudo docker exec -it $(sudo docker ps -f=name=ambari-agent -q) bash\n[root@docker-ambari tmp]# su hdfs\n[hdfs@docker-ambari tmp]# hadoop fs -put /hadoopfs/fs1/data /tmp\n[hdfs@docker-ambari tmp]# hadoop fs -ls /tmp\nFound 2 items\ndrwxr-xr-x   - hdfs supergroup          0 2015-10-21 13:46 /tmp/data\ndrwx-wx-wx   - hive supergroup          0 2015-10-21 08:51 /tmp/hive\n\n\n\n\nInternal hostnames\n\n\nAfter a cluster is created with Cloudbreak, the nodes will have internal hostnames like this:\n\n\nvmhostgroupclient11.node.dc1.consul\n\n\nThis is because Cloudbreak uses \nConsul\n to provide DNS services.\nIt means that you won't see entries to the other nodes inside the \n/etc/hosts\n file, because nodes are registered inside Consul and the hostnames are resolved by Consul as well.\n\n\nIn the current version the \nnode.dc1.consul\n domain is hardcoded and cannot be changed.\n\n\nAccessing Ambari server from the other nodes\n\n\nAmbari server is registered as a service in Consul, so it can always be accessed through its domain name \nambari-8080.service.consul\n from the other ambari containers.\nIt can be tried by pinging it from one of the \nambari-agent\n containers:\n\n\nping ambari-8080.service.consul\n\n\n\n\nCloudbreak gateway node\n\n\nWith every Cloudbreak cluster installation there is a special node called \ncbgateway\n started that won't run an ambari-agent container so it won't run HDP services either.\nIt can be seen on the Cloudbreak UI among the hostgroups when creating a cluster, but its node count cannot be changed from 1 and it shouldn't be there in the Ambari blueprint.\nIt is by design because this instance has some special tasks:\n\n\n\n\nit runs the Ambari server and its database inside Docker containers\n\n\nit runs an nginx proxy that is used by the Cloudbreak API to communicate with the cluster securely\n\n\nit runs the Swarm manager that orchestrates the Docker containers on the whole cluster\n\n\nit runs the Baywatch server that is responsible for collecting the operational logs from the cluster\n\n\nit runs a Kerberos KDC container if Kerberos is configured\n\n\n\n\nLogs\n\n\nHadoop logs\n\n\nHadoop logs are available from the host and from the container as well in the \n/hadoopfs/fs1/logs\n directory.\n\n\nAmbari logs\n\n\nFor Ambari logs you can use our helper function, \nambari-log\n. For further information please check the \nHelper functions\n section. Alternatively you watch the Ambari logs on the host instance as well under the \n`/hadoopfs/fs1/logs\n folder as well.\n\n\nAmbari database\n\n\nAmbari's database runs on the \ncbgateway\n node inside a PostgreSQL docker container. To access it SSH to the \ngateway\n node and run the following command:\n\n\n[cloudbreak@vmcbgateway0 ~]$ sudo docker exec -it ambari_db psql -U postgres", 
            "title": "Operations"
        }, 
        {
            "location": "/operations/#operations", 
            "text": "", 
            "title": "Operations"
        }, 
        {
            "location": "/operations/#debug", 
            "text": "If you want to have more detailed output set the  DEBUG  env variable to non-zero:  DEBUG=1 cbd some_command", 
            "title": "Debug"
        }, 
        {
            "location": "/operations/#troubleshoot", 
            "text": "You can use the  doctor  command to diagnose your environment.\nIt can reveal some common problems with your docker or boot2docker configuration and it also checks the cbd versions.  cbd doctor", 
            "title": "Troubleshoot"
        }, 
        {
            "location": "/operations/#logs", 
            "text": "The aggregated logs of all the Cloudbreak components can be checked with:  cbd logs  It can also be used to check the logs of an individual docker container. To see only the logs of the Cloudbreak backend:  cbd logs cloudbreak  You can also check the individual logs of  uluwatu ,  periscope , and  identity .", 
            "title": "Logs"
        }, 
        {
            "location": "/operations/#update-cloudbreak-deployer", 
            "text": "The cloudbreak-deployer tool is capable of upgrading itself to a newer version.  Please apply the following steps on the console:   Update Cloudbreak Deployer       cbd update   Update the  docker-compose.yml  file with new Docker containers that are needed for the  cbd      cbd regenerate   Start the new version of the  cbd      cbd start   It will take for a while, because of need to download all the updated docker images for the new version.    Check the health and version of the updated  cbd        cbd doctor   Check the started Cloudbreak Application logs      cbd logs cloudbreak   Cloudbreak should start within a minute - you should see a line like this:  Started CloudbreakApplication in 36.823 seconds", 
            "title": "Update Cloudbreak Deployer"
        }, 
        {
            "location": "/operations/#ssh-to-the-hosts", 
            "text": "In the current version of Cloudbreak all the nodes have a public IP address and all the nodes are accessible via SSH.\nThe public IP addresses of a running cluster can be checked on the Cloudbreak UI under the  Nodes  tab.\nOnly key-based authentication is supported - the public key can be specified when creating a cloud credential.  Each cloud provider has different SSH user. In order to figure out the username you can try it wit the  root  user - that will tell you the correct username.  As an example:  ssh root@172.16.252.31\nWarning: Permanently added '172.16.252.31' (ECDSA) to the list of known hosts.\nPlease login with one of the following username rather than the user  root :\n\n  centos                  : Reach the host running the ambari container\n  ambari-enter            : Enters the Ambari container\n  ambari-log              : Watches the Ambari logs  After you figured out the username you can SSH into the host.  ssh -i ~/.ssh/private-key.pem USER_NAME@ public-ip", 
            "title": "SSH to the hosts"
        }, 
        {
            "location": "/operations/#accessing-hdp-client-services", 
            "text": "The main difference between general HDP clusters and Cloudbreak-installed HDP clusters is that each host runs an Ambari server or agent Docker container and the HDP services will be installed in this container as well.\nIt means that after  ssh  the client services won't be available instantly, first you'll have to  enter  the ambari-agent container.\nInside the container everything works the same way as expected. In order to do so there are a few options.  Helper functions  We have created a few helper functions in order to help operations. Once yoy SSH into the host you should a message as such:  You are now logged in to the docker host ...\nGetting started with docker:\n\n  docker ps                : Listing running containers\n  docker logs -f  ID       : Star tail -f on container stdout/stderr\n  docker exec -it  ID  sh  : Entering the container (instead of ssh)\n\nHelper commands:\n\n  h                        : prints this message\n  ambari-enter             : Enters the ambari container\n  ambari-log               : Watches the ambari logs  You can enter into the container where the HDP services are running using the  ambari-enter  functions.  Docker exec  To check the containers running on the host enter:  [cloudbreak@vmhostgroupclient11 ~]$ sudo docker ps\nCONTAINER ID   IMAGE                                    COMMAND                CREATED      STATUS      PORTS     NAMES\n1098ca778176   sequenceiq/baywatch-client:v1.0.0         /etc/bootstrap.sh -d  4 hours ago  Up 4 hours            baywatch-client-14454170059514\nf4097c52fda5   sequenceiq/logrotate:v0.5.1               /start.sh             4 hours ago  Up 4 hours            logrotate-14454169954830\n7b94aedaab30   sequenceiq/docker-consul-watch-plugn:1.0  /start.sh consul://1  4 hours ago  Up 4 hours            consul-watch-14454169884044\nd8128b001427   sequenceiq/ambari:2.1.2-v2                /start-agent          4 hours ago  Up 4 hours            ambari-agent-14454169805924\na8ec90037aaf   swarm:0.4.0                               /swarm join --addr=1  4 hours ago  Up 4 hours  2375/tcp  vmhostgroupmaster12-swarm-agent\nef02b43eacee   sequenceiq/consul:v0.5.0-v5               /bin/start            4 hours ago  Up 4 hours            vmhostgroupmaster12-consul  You should see the  ambari-agent  container running. Copy its id or name and  exec  into the container:  [cloudbreak@vmhostgroupclient11 ~]$ sudo docker exec -it ambari-agent-14454169805924 bash\n[root@docker-ambari tmp]#  Or you can use this one-step command as well:  [cloudbreak@vmhostgroupclient11 ~]$ sudo docker exec -it $(sudo docker ps -f=name=ambari-agent -q) bash\n[root@docker-ambari tmp]#", 
            "title": "Accessing HDP client services"
        }, 
        {
            "location": "/operations/#data-volumes", 
            "text": "The disks that are attached to the instances are automatically mounted to  /hadoopfs/fs1 ,  /hadoopfs/fs2 , ...  /hadoopfs/fsN  respectively.\nThese directories are mounted from the host into the ambari-agent container under the same name so these can be accessed from inside.\nIt means that if you'd like to move some data to the instances you can use these volumes and the data will be available from the container instantly to work on it.  An  scp  Example:  $ scp -qr -i ~/.ssh/private-key.pem ~/tmp/data cloudbreak@ client-node :/hadoopfs/fs1\n$ ssh -i ~/.ssh/private-key.pem cloudbreak@ client-node \n[cloudbreak@vmhostgroupclient11 ~]$ sudo docker exec -it $(sudo docker ps -f=name=ambari-agent -q) bash\n[root@docker-ambari tmp]# su hdfs\n[hdfs@docker-ambari tmp]# hadoop fs -put /hadoopfs/fs1/data /tmp\n[hdfs@docker-ambari tmp]# hadoop fs -ls /tmp\nFound 2 items\ndrwxr-xr-x   - hdfs supergroup          0 2015-10-21 13:46 /tmp/data\ndrwx-wx-wx   - hive supergroup          0 2015-10-21 08:51 /tmp/hive", 
            "title": "Data volumes"
        }, 
        {
            "location": "/operations/#internal-hostnames", 
            "text": "After a cluster is created with Cloudbreak, the nodes will have internal hostnames like this:  vmhostgroupclient11.node.dc1.consul  This is because Cloudbreak uses  Consul  to provide DNS services.\nIt means that you won't see entries to the other nodes inside the  /etc/hosts  file, because nodes are registered inside Consul and the hostnames are resolved by Consul as well.  In the current version the  node.dc1.consul  domain is hardcoded and cannot be changed.", 
            "title": "Internal hostnames"
        }, 
        {
            "location": "/operations/#accessing-ambari-server-from-the-other-nodes", 
            "text": "Ambari server is registered as a service in Consul, so it can always be accessed through its domain name  ambari-8080.service.consul  from the other ambari containers.\nIt can be tried by pinging it from one of the  ambari-agent  containers:  ping ambari-8080.service.consul", 
            "title": "Accessing Ambari server from the other nodes"
        }, 
        {
            "location": "/operations/#cloudbreak-gateway-node", 
            "text": "With every Cloudbreak cluster installation there is a special node called  cbgateway  started that won't run an ambari-agent container so it won't run HDP services either.\nIt can be seen on the Cloudbreak UI among the hostgroups when creating a cluster, but its node count cannot be changed from 1 and it shouldn't be there in the Ambari blueprint.\nIt is by design because this instance has some special tasks:   it runs the Ambari server and its database inside Docker containers  it runs an nginx proxy that is used by the Cloudbreak API to communicate with the cluster securely  it runs the Swarm manager that orchestrates the Docker containers on the whole cluster  it runs the Baywatch server that is responsible for collecting the operational logs from the cluster  it runs a Kerberos KDC container if Kerberos is configured   Logs  Hadoop logs  Hadoop logs are available from the host and from the container as well in the  /hadoopfs/fs1/logs  directory.  Ambari logs  For Ambari logs you can use our helper function,  ambari-log . For further information please check the  Helper functions  section. Alternatively you watch the Ambari logs on the host instance as well under the  `/hadoopfs/fs1/logs  folder as well.  Ambari database  Ambari's database runs on the  cbgateway  node inside a PostgreSQL docker container. To access it SSH to the  gateway  node and run the following command:  [cloudbreak@vmcbgateway0 ~]$ sudo docker exec -it ambari_db psql -U postgres", 
            "title": "Cloudbreak gateway node"
        }, 
        {
            "location": "/database/", 
            "text": "Migrate the databases\n\n\nCreate the database schema or migrate it to the latest version:\n\n\ncbd startdb\ncbd migrate cbdb up\n\n\n\n\nVerify that all scripts have been applied:\n\n\ncbd migrate cbdb status\n\n\n\n\ncbd generate\ncbd migrate cbdb up", 
            "title": "Migration"
        }, 
        {
            "location": "/database/#migrate-the-databases", 
            "text": "Create the database schema or migrate it to the latest version:  cbd startdb\ncbd migrate cbdb up  Verify that all scripts have been applied:  cbd migrate cbdb status  cbd generate\ncbd migrate cbdb up", 
            "title": "Migrate the databases"
        }, 
        {
            "location": "/configuration/", 
            "text": "Configuration\n\n\nConfiguration is based on environment variables. Cloudbreak Deployer always forks a new bash subprocess \nwithout\ninheriting environment variables\n. The only way to set ENV vars relevant for Cloudbreak Deployer is to set them\nin a file called \nProfile\n.\n\n\nTo see all available config variables with their default value:\n\n\ncbd env show\n\n\n\n\nThe \nProfile\n will be simple \nsourced\n in bash terms, so you can use the usual syntaxes to set config values:\n\n\nexport MY_VAR=some_value\nexport OTHER_VAR=dunno\n\n\n\n\nEnv specific Profile\n\n\nLet\u2019s say you want to use a different version of Cloudbreak for \nprod\n and \nqa\n profile.\nYou can specify the Docker image tag via: \nDOCKER_TAG_CLOUDBREAK\n.\n\nProfile\n is always sourced, so you will have two env specific configurations:\n- \nProfile.dev\n\n- \nProfile.qa\n\n\nFor prod you need:\n\n\n\n\ncreate a file called \nProfile.prod\n\n\nwrite the env specific \nexport DOCKER_TAG_CLOUDBREAK=0.3.99\n into \nProfile.prod\n\n\nset the env variable: \nCBD_DEFAULT_PROFILE=prod\n\n\n\n\nTo use the \nprod\n specific profile once:\n\n\nCBD_DEFAULT_PROFILE=prod cbd some_commands\n\n\n\n\nFor permanent setting you can \nexport CBD_DEFAULT_PROFILE=prod\n in your \n.bash_profile\n.\n\n\nAvailable Configurations\n\n\nSMTP\n\n\nIf you want to change SMTP parameters, put the corresponding lines into your \nProfile\n. You can also see the default values of the parameters in the following box.\n\n\nexport CLOUDBREAK_SMTP_SENDER_USERNAME=\nexport CLOUDBREAK_SMTP_SENDER_PASSWORD=\nexport CLOUDBREAK_SMTP_SENDER_HOST=\nexport CLOUDBREAK_SMTP_SENDER_PORT=25\nexport CLOUDBREAK_SMTP_SENDER_FROM=\nexport CLOUDBREAK_SMTP_AUTH=true\nexport CLOUDBREAK_SMTP_STARTTLS_ENABLE=true\nexport CLOUDBREAK_SMTP_TYPE=smtp\n\n\n\n\nUsing SMTPS\n\n\nIf your SMTP server uses SMTPS you should change the protocol in your Profile:\n\n\nexport CLOUDBREAK_SMTP_TYPE=smtps\n\n\n\n\nIf the certificate used by the SMTP server is self-signed, or Java's default trust store doesn't contain it than you can add it to the trust store by copying it to \ncerts/trusted\n inside the Cloudbreak deployer directory and start (or restart) the Cloudbreak container (with \ncbd start\n).The Cloudbreak container will automatically import the certificates in that directory to its trust store on startup.\n\n\nAccess from custom domains\n\n\nCloudbreak deployer uses UAA as an identity provider and supports multi tenancy. In UAA terminology this is referred as identity zones. An identity zone is accessed through a unique subdomain. If the standard UAA responds to \nhttps://uaa.10.244.0.34.xip.io\n a zone on this UAA would be accessed through \nhttps://testzone1.uaa.10.244.0.34.xip.io\n.\n\n\nAs an example in our hosted deployment the \nidentity.sequenceiq.com\n domain refers to our identity server and the \nUAA_ZONE_DOMAIN\n variable has to be set to that domain. This variable is necessary for UAA to identify which zone provider should handle the requests that arrives to the given domain.\n\n\nIf you want to use a custom domain for your identity or deployment, put the \nUAA_ZONE_DOMAIN\n line into your\n\nProfile\n. You can see an example in the following box:\n\n\nexport UAA_ZONE_DOMAIN=my-subdomain.example.com\n\n\n\n\nConsul\n\n\nConsul\n is used for DNS resolution. All Cloudbreak related services are registered as\n\nsomeservice.service.consul\n. Consul\u2019s built in DNS server is able to \u201cfall-back\u201d on an other DNS server.\nThis option is called \n-recursor\n. Clodbreak Deployer first tries to discover the DNS settings of the host,\nby looking for \nnameserver\n entry in \n/etc/resolv.conf\n. If it finds one consul will use it as a recursor,\notherwise \n8.8.8.8\n will be used.\n\n\nFor a full list of available consul config options, see the \ndocs\n.\n\n\nYou can pass any additional consul configuration by defining a \nDOCKER_CONSUL_OPTIONS\n in \nProfile\n.\n\n\nAzure Resource manager command\n\n\n\n\ncbd azure configure-arm\n\n\ncbd azure deploy-dash\n\nSee the documentation \nhere\n.\n\n\n\n\nCaveats\n\n\nThe \nCloudbreak Deployer\n tool opens a clean bash subshell, without inheriting environment variables.\n\n\nOnly the following environment variables \nare\n inherited:\n\n\n\n\nHOME\n\n\nDEBUG\n\n\nTRACE\n\n\nCBD_DEFAULT_PROFILE\n\n\nall \nDOCKER_XXX\n\n\n\n\nSSH fingerprint verification\n\n\nCloudbreak is able to verify the SSH fingerprints of the provisioned virtual machines. We disable this feature by default for AWS and GCP because we have experienced issues, since Cloud providers do not always print the SSH fingerprint into the provisioned machines console output. The fingerprint validation feature could be turned on by configuring the 'CB_AWS_HOSTKEY_VERIFY' and/or the CB_GCP_HOSTKEY_VERIFY variables in your cbd profile like in the following example:\n\n\nexport CB_AWS_HOSTKEY_VERIFY=true\nexport CB_GCP_HOSTKEY_VERIFY=true", 
            "title": "Advanced Configuration"
        }, 
        {
            "location": "/configuration/#configuration", 
            "text": "Configuration is based on environment variables. Cloudbreak Deployer always forks a new bash subprocess  without\ninheriting environment variables . The only way to set ENV vars relevant for Cloudbreak Deployer is to set them\nin a file called  Profile .  To see all available config variables with their default value:  cbd env show  The  Profile  will be simple  sourced  in bash terms, so you can use the usual syntaxes to set config values:  export MY_VAR=some_value\nexport OTHER_VAR=dunno", 
            "title": "Configuration"
        }, 
        {
            "location": "/configuration/#env-specific-profile", 
            "text": "Let\u2019s say you want to use a different version of Cloudbreak for  prod  and  qa  profile.\nYou can specify the Docker image tag via:  DOCKER_TAG_CLOUDBREAK . Profile  is always sourced, so you will have two env specific configurations:\n-  Profile.dev \n-  Profile.qa  For prod you need:   create a file called  Profile.prod  write the env specific  export DOCKER_TAG_CLOUDBREAK=0.3.99  into  Profile.prod  set the env variable:  CBD_DEFAULT_PROFILE=prod   To use the  prod  specific profile once:  CBD_DEFAULT_PROFILE=prod cbd some_commands  For permanent setting you can  export CBD_DEFAULT_PROFILE=prod  in your  .bash_profile .", 
            "title": "Env specific Profile"
        }, 
        {
            "location": "/configuration/#available-configurations", 
            "text": "", 
            "title": "Available Configurations"
        }, 
        {
            "location": "/configuration/#smtp", 
            "text": "If you want to change SMTP parameters, put the corresponding lines into your  Profile . You can also see the default values of the parameters in the following box.  export CLOUDBREAK_SMTP_SENDER_USERNAME=\nexport CLOUDBREAK_SMTP_SENDER_PASSWORD=\nexport CLOUDBREAK_SMTP_SENDER_HOST=\nexport CLOUDBREAK_SMTP_SENDER_PORT=25\nexport CLOUDBREAK_SMTP_SENDER_FROM=\nexport CLOUDBREAK_SMTP_AUTH=true\nexport CLOUDBREAK_SMTP_STARTTLS_ENABLE=true\nexport CLOUDBREAK_SMTP_TYPE=smtp  Using SMTPS  If your SMTP server uses SMTPS you should change the protocol in your Profile:  export CLOUDBREAK_SMTP_TYPE=smtps  If the certificate used by the SMTP server is self-signed, or Java's default trust store doesn't contain it than you can add it to the trust store by copying it to  certs/trusted  inside the Cloudbreak deployer directory and start (or restart) the Cloudbreak container (with  cbd start ).The Cloudbreak container will automatically import the certificates in that directory to its trust store on startup.", 
            "title": "SMTP"
        }, 
        {
            "location": "/configuration/#access-from-custom-domains", 
            "text": "Cloudbreak deployer uses UAA as an identity provider and supports multi tenancy. In UAA terminology this is referred as identity zones. An identity zone is accessed through a unique subdomain. If the standard UAA responds to  https://uaa.10.244.0.34.xip.io  a zone on this UAA would be accessed through  https://testzone1.uaa.10.244.0.34.xip.io .  As an example in our hosted deployment the  identity.sequenceiq.com  domain refers to our identity server and the  UAA_ZONE_DOMAIN  variable has to be set to that domain. This variable is necessary for UAA to identify which zone provider should handle the requests that arrives to the given domain.  If you want to use a custom domain for your identity or deployment, put the  UAA_ZONE_DOMAIN  line into your Profile . You can see an example in the following box:  export UAA_ZONE_DOMAIN=my-subdomain.example.com", 
            "title": "Access from custom domains"
        }, 
        {
            "location": "/configuration/#consul", 
            "text": "Consul  is used for DNS resolution. All Cloudbreak related services are registered as someservice.service.consul . Consul\u2019s built in DNS server is able to \u201cfall-back\u201d on an other DNS server.\nThis option is called  -recursor . Clodbreak Deployer first tries to discover the DNS settings of the host,\nby looking for  nameserver  entry in  /etc/resolv.conf . If it finds one consul will use it as a recursor,\notherwise  8.8.8.8  will be used.  For a full list of available consul config options, see the  docs .  You can pass any additional consul configuration by defining a  DOCKER_CONSUL_OPTIONS  in  Profile .", 
            "title": "Consul"
        }, 
        {
            "location": "/configuration/#azure-resource-manager-command", 
            "text": "cbd azure configure-arm  cbd azure deploy-dash \nSee the documentation  here .", 
            "title": "Azure Resource manager command"
        }, 
        {
            "location": "/configuration/#caveats", 
            "text": "The  Cloudbreak Deployer  tool opens a clean bash subshell, without inheriting environment variables.  Only the following environment variables  are  inherited:   HOME  DEBUG  TRACE  CBD_DEFAULT_PROFILE  all  DOCKER_XXX", 
            "title": "Caveats"
        }, 
        {
            "location": "/configuration/#ssh-fingerprint-verification", 
            "text": "Cloudbreak is able to verify the SSH fingerprints of the provisioned virtual machines. We disable this feature by default for AWS and GCP because we have experienced issues, since Cloud providers do not always print the SSH fingerprint into the provisioned machines console output. The fingerprint validation feature could be turned on by configuring the 'CB_AWS_HOSTKEY_VERIFY' and/or the CB_GCP_HOSTKEY_VERIFY variables in your cbd profile like in the following example:  export CB_AWS_HOSTKEY_VERIFY=true\nexport CB_GCP_HOSTKEY_VERIFY=true", 
            "title": "SSH fingerprint verification"
        }, 
        {
            "location": "/development/", 
            "text": "Development\n\n\nCloudbreak application\n\n\nLocal Development Setup\n\n\nTo use this development environment on OSX, you need to have Docker and Boot2docker installed.\n\n\nSimplest way to prepare the working environment is to start the Cloudbreak on your local machine is to use the \nCloudbreak Deployer\n.\n\n\nFirst you need to create a \nsandbox\n directory which will store the necessary  configuration files and dependencies of \nCloudbreak Deployer\n. This directory must be created outside of the cloned Cloudbreak git repository:\n\n\nmkdir cbd-local\n\n\n\n\nTo start the complete Cloudbreak ecosystem on your machine just execute the following sequence of commands:\n\n\ncd cbd-local\ncurl https://raw.githubusercontent.com/sequenceiq/cloudbreak-deployer/master/install | sh \n cbd --version\ncbd update master\ncbd init\ncbd start\n\n\n\n\nIf everything went well then the Cloudbreak will be available on http://192.168.59.103:3000. For more details and config parameters please check the documentation of \nCloudbreak Deployer\n.\n\n\nThe deployer has generated a \ncerts\n directory under \ncbd-local\n directory which will be needed later on to set up the IDEA properly.\n\n\nThe next step is to edit the \ncbd-local/Profile\n file with any editor and add the CB_SCHEMA_SCRIPTS_LOCATION environment variable which configures the location of SQL scripts that are in the 'core/src/main/resources/schema' directory in the cloned Cloudbreak git repository. Please note that the full path needs to be configured and env variables like $USER cannot be used.\n\n\nexport CB_SCHEMA_SCRIPTS_LOCATION=/Users/myusername/prj/cloudbreak/core/src/main/resources/schema\n\n\n\n\nIn order to kill Cloudbreak container running inside the boot2docker and redirect the Cloudbreak related traffic to the Cloudbreak running in IDEA use the followig command:\n\n\ncbd util local-dev\n\n\n\n\nIDEA\n\n\nCloudbreak can be imported into IDEA as gradle project. Once it is done, you need to import the proper code formatter by using the \nFile -\n Import Settings...\n menu and selecting the \nidea_settings.jar\n located in the \nconfig\n directory in Cloudbreak git repository.\n\n\nTo launch the Cloudbreak application execute the \ncom.sequenceiq.cloudbreak.CloudbreakApplication\n class with VM options:\n\n\n-XX:MaxPermSize=1024m\n-Dcb.cert.dir=FULL_PATH_OF_THE_CERTS_DIR_GENERATED_BY_CBD\n-Dcb.client.id=cloudbreak\n-Dcb.client.secret=cbsecret2015\n-Dcb.db.port.5432.tcp.addr=192.168.59.103\n-Dcb.db.port.5432.tcp.port=5432\n-Dspring.cloud.consul.host=192.168.59.103\n-Dcb.identity.server.url=http://192.168.59.103:8089\n-Dserver.port=9091\n\n\n\n\nThe \n-Dcb.cert.dir=FULL_PATH_OF_THE_CERTS_DIR_GENERATED_BY_CBD\n value above needs to be replaced with the full path of \ncerts\n directory generated by Cloudbreak Deployer e.g. \n-Dcb.cert.dir=/Users/myusername/prj/cbd-local/certs\n\n\nCommand line\n\n\nTo run Cloudbreak from command line you have to create a property file, for example \napplication.properties\n, with the content below, and execute \njava -jar -XX:MaxPermSize=1024m -Dspring.config.location=file:///path/of/property/application.properties core/build/libs/cloudbreak.jar\n command at project root. Important that the path of \napplication.properties\n must be an absolute path.\n\n\ncb.cert.dir=FULL_PATH_OF_THE_CERTS_DIR_GENERATED_BY_CBD\ncb.client.id=cloudbreak\ncb.client.secret=cbsecret2015\ncb.db.port.5432.tcp.addr=192.168.59.103\ncb.db.port.5432.tcp.port=5432\nspring.cloud.consul.host=192.168.59.103\ncb.identity.server.url=http://192.168.59.103:8089\nserver.port=9091\n\n\n\n\nDatabase development\n\n\nIf schema change is required in Cloudbreak database (cbdb), then the developer needs to write SQL scripts to migrate the database accordingly. In Cloudbreak the schema migration is managed by \nMYBATIS Migrations\n and the cbd tool provides an easy-to-use wrapper for it. The syntax for using the migration commands is \ncbd migrate \ndatabase name\n \ncommand\n [parameters]\n e.g. \ncbd migrate migrate status\n.\n\n\nCreate a SQL template for schema changes:\n\n\ncbd migrate cbdb new \nCLOUD-123 schema change for new feature\n\n\n\n\n\nAs as result of the above command an SQL file template is generated under the path specified in \nCB_SCHEMA_SCRIPTS_LOCATION\n environment variable, which is defined in Profile. The structure of the generated SQL template looks like the following:\n\n\n-- // CLOUD-123 schema change for new feature\n-- Migration SQL that makes the change goes here.\n\n\n\n-- //@UNDO\n-- SQL to undo the change goes here.\n\n\n\n\nOnce you have implemented your SQLs then you can execute them with:\n\n\ncbd migrate cbdb up\n\n\n\n\nIf you would like to rollback the last SQL file, then just use the down command:\n\n\ncbd migrate cbdb down\n\n\n\n\nOn order to check the status of database\n\n\ncbd migrate cbdb status\n\n#Every script that has not been executed will be marked as ...pending... in the output of status command:\n\n------------------------------------------------------------------------\n-- MyBatis Migrations - status\n------------------------------------------------------------------------\nID             Applied At          Description\n================================================================================\n20150421140021 2015-07-08 10:04:28 create changelog\n20150421150000 2015-07-08 10:04:28 CLOUD-607 create baseline schema\n20150507121756 2015-07-08 10:04:28 CLOUD-576 change instancegrouptype hostgroup to core\n20151008090632    ...pending...    CLOUD-123 schema change for new feature\n\n------------------------------------------------------------------------\n\n\n\n\nBuilding\n\n\nGradle is used for build and dependency management. Gradle wrapper is added to Cloudbreak git repository, therefore building can be done with:\n\n\n./gradlew clean build\n\n\n\n\nCloudbreak deployer\n\n\nContribution\n\n\nDevelopment process should happen on separate branches. Then a pull-request should be opened as usual.\n\n\nTo build the project\n\n\n# make deps needed only once\nmake deps\n\nmake install\n\n\n\n\nTo run the unit tests:\n\n\nmake tests\n\n\n\n\nIf you want to test the binary CircleCI build from your branch named \nfix-something\n, to validate the PR binary \ncbd\n tool will be tested. It is built by CircleCI for each branch.\n\n\ncbd update fix-something\n\n\n\n\nSnapshots\n\n\nWe recommend to always use the latest release, but you might want to check new features or bugfixes.\nAll successful builds from the \nmaster\n branch are uploaded to the public S3 bucket. You can download it:\n\n\ncurl -L s3.amazonaws.com/public-repo-1.hortonworks.com/HDP/cloudbreak/cloudbreak-deployer_snapshot_$(uname)_x86_64.tgz | tar -xz\n\n\n\n\nInstead of overwriting the released version, download it to a \nlocal directory\n and useit by refering as \n./cbd\n\n\n./cbd --version\n\n\n\n\nTesting\n\n\nShell scripts shouldn\u2019t raise exceptions when it comes to unit testing. \nbasht\n is\n used for testing. See the reasoning: \nwhy not bats or shunit2\n.\n\n\nPlease cover your bash functions with unit tests and run test with:\n\n\nmake tests\n\n\n\n\nRelease Process of the Clodbreak Deployer tool\n\n\nThe master branch is always built on \nCircleCI\n.\nWhen you wan\u2019t a new release, all you have to do:\n\n\nmake release-next-ver\n\n\n\n\nmake release-next-ver\n performs the following steps:\n\n\n\n\nOn the \nmaster\n branch:\n\n\nUpdates the \nVERSION\n file by increasing the \npatch\n version number (for example from 0.5.2 to 0.5.3)\n\n\nUpdates \nCHANGELOG.md\n with the release date\n\n\nCreates a new \nUnreleased\n section in top of \nCHANGELOG.md\n\n\n\n\n\n\nCreates a PullRequest for the release branch:\n\n\ncreate a new branch with a name like \nrelease-0.5.x\n\n\nthis branch should be the same as \norigin/master\n\n\ncreate a pull request into \nrelease\n branch\n\n\n\n\n\n\n\n\nAcceptance\n\n\nNow you should test this release. You can get it by \ncbd update release-x.y.z\n. Comment with LGTM (Looking Good To Me).\n\n\nOnce the PR is merged, CircleCI will:\n\n\n\n\ncreate a new release on \nGitHub releases tab\n, with the\n help of \ngh-release\n.\n\n\nit will create the git tag with \nv\n prefix like: \nv0.0.3", 
            "title": "Development"
        }, 
        {
            "location": "/development/#development", 
            "text": "", 
            "title": "Development"
        }, 
        {
            "location": "/development/#cloudbreak-application", 
            "text": "", 
            "title": "Cloudbreak application"
        }, 
        {
            "location": "/development/#local-development-setup", 
            "text": "To use this development environment on OSX, you need to have Docker and Boot2docker installed.  Simplest way to prepare the working environment is to start the Cloudbreak on your local machine is to use the  Cloudbreak Deployer .  First you need to create a  sandbox  directory which will store the necessary  configuration files and dependencies of  Cloudbreak Deployer . This directory must be created outside of the cloned Cloudbreak git repository:  mkdir cbd-local  To start the complete Cloudbreak ecosystem on your machine just execute the following sequence of commands:  cd cbd-local\ncurl https://raw.githubusercontent.com/sequenceiq/cloudbreak-deployer/master/install | sh   cbd --version\ncbd update master\ncbd init\ncbd start  If everything went well then the Cloudbreak will be available on http://192.168.59.103:3000. For more details and config parameters please check the documentation of  Cloudbreak Deployer .  The deployer has generated a  certs  directory under  cbd-local  directory which will be needed later on to set up the IDEA properly.  The next step is to edit the  cbd-local/Profile  file with any editor and add the CB_SCHEMA_SCRIPTS_LOCATION environment variable which configures the location of SQL scripts that are in the 'core/src/main/resources/schema' directory in the cloned Cloudbreak git repository. Please note that the full path needs to be configured and env variables like $USER cannot be used.  export CB_SCHEMA_SCRIPTS_LOCATION=/Users/myusername/prj/cloudbreak/core/src/main/resources/schema  In order to kill Cloudbreak container running inside the boot2docker and redirect the Cloudbreak related traffic to the Cloudbreak running in IDEA use the followig command:  cbd util local-dev", 
            "title": "Local Development Setup"
        }, 
        {
            "location": "/development/#idea", 
            "text": "Cloudbreak can be imported into IDEA as gradle project. Once it is done, you need to import the proper code formatter by using the  File -  Import Settings...  menu and selecting the  idea_settings.jar  located in the  config  directory in Cloudbreak git repository.  To launch the Cloudbreak application execute the  com.sequenceiq.cloudbreak.CloudbreakApplication  class with VM options:  -XX:MaxPermSize=1024m\n-Dcb.cert.dir=FULL_PATH_OF_THE_CERTS_DIR_GENERATED_BY_CBD\n-Dcb.client.id=cloudbreak\n-Dcb.client.secret=cbsecret2015\n-Dcb.db.port.5432.tcp.addr=192.168.59.103\n-Dcb.db.port.5432.tcp.port=5432\n-Dspring.cloud.consul.host=192.168.59.103\n-Dcb.identity.server.url=http://192.168.59.103:8089\n-Dserver.port=9091  The  -Dcb.cert.dir=FULL_PATH_OF_THE_CERTS_DIR_GENERATED_BY_CBD  value above needs to be replaced with the full path of  certs  directory generated by Cloudbreak Deployer e.g.  -Dcb.cert.dir=/Users/myusername/prj/cbd-local/certs", 
            "title": "IDEA"
        }, 
        {
            "location": "/development/#command-line", 
            "text": "To run Cloudbreak from command line you have to create a property file, for example  application.properties , with the content below, and execute  java -jar -XX:MaxPermSize=1024m -Dspring.config.location=file:///path/of/property/application.properties core/build/libs/cloudbreak.jar  command at project root. Important that the path of  application.properties  must be an absolute path.  cb.cert.dir=FULL_PATH_OF_THE_CERTS_DIR_GENERATED_BY_CBD\ncb.client.id=cloudbreak\ncb.client.secret=cbsecret2015\ncb.db.port.5432.tcp.addr=192.168.59.103\ncb.db.port.5432.tcp.port=5432\nspring.cloud.consul.host=192.168.59.103\ncb.identity.server.url=http://192.168.59.103:8089\nserver.port=9091", 
            "title": "Command line"
        }, 
        {
            "location": "/development/#database-development", 
            "text": "If schema change is required in Cloudbreak database (cbdb), then the developer needs to write SQL scripts to migrate the database accordingly. In Cloudbreak the schema migration is managed by  MYBATIS Migrations  and the cbd tool provides an easy-to-use wrapper for it. The syntax for using the migration commands is  cbd migrate  database name   command  [parameters]  e.g.  cbd migrate migrate status .  Create a SQL template for schema changes:  cbd migrate cbdb new  CLOUD-123 schema change for new feature   As as result of the above command an SQL file template is generated under the path specified in  CB_SCHEMA_SCRIPTS_LOCATION  environment variable, which is defined in Profile. The structure of the generated SQL template looks like the following:  -- // CLOUD-123 schema change for new feature\n-- Migration SQL that makes the change goes here.\n\n\n\n-- //@UNDO\n-- SQL to undo the change goes here.  Once you have implemented your SQLs then you can execute them with:  cbd migrate cbdb up  If you would like to rollback the last SQL file, then just use the down command:  cbd migrate cbdb down  On order to check the status of database  cbd migrate cbdb status\n\n#Every script that has not been executed will be marked as ...pending... in the output of status command:\n\n------------------------------------------------------------------------\n-- MyBatis Migrations - status\n------------------------------------------------------------------------\nID             Applied At          Description\n================================================================================\n20150421140021 2015-07-08 10:04:28 create changelog\n20150421150000 2015-07-08 10:04:28 CLOUD-607 create baseline schema\n20150507121756 2015-07-08 10:04:28 CLOUD-576 change instancegrouptype hostgroup to core\n20151008090632    ...pending...    CLOUD-123 schema change for new feature\n\n------------------------------------------------------------------------", 
            "title": "Database development"
        }, 
        {
            "location": "/development/#building", 
            "text": "Gradle is used for build and dependency management. Gradle wrapper is added to Cloudbreak git repository, therefore building can be done with:  ./gradlew clean build", 
            "title": "Building"
        }, 
        {
            "location": "/development/#cloudbreak-deployer", 
            "text": "", 
            "title": "Cloudbreak deployer"
        }, 
        {
            "location": "/development/#contribution", 
            "text": "Development process should happen on separate branches. Then a pull-request should be opened as usual.  To build the project  # make deps needed only once\nmake deps\n\nmake install  To run the unit tests:  make tests  If you want to test the binary CircleCI build from your branch named  fix-something , to validate the PR binary  cbd  tool will be tested. It is built by CircleCI for each branch.  cbd update fix-something", 
            "title": "Contribution"
        }, 
        {
            "location": "/development/#snapshots", 
            "text": "We recommend to always use the latest release, but you might want to check new features or bugfixes.\nAll successful builds from the  master  branch are uploaded to the public S3 bucket. You can download it:  curl -L s3.amazonaws.com/public-repo-1.hortonworks.com/HDP/cloudbreak/cloudbreak-deployer_snapshot_$(uname)_x86_64.tgz | tar -xz  Instead of overwriting the released version, download it to a  local directory  and useit by refering as  ./cbd  ./cbd --version", 
            "title": "Snapshots"
        }, 
        {
            "location": "/development/#testing", 
            "text": "Shell scripts shouldn\u2019t raise exceptions when it comes to unit testing.  basht  is\n used for testing. See the reasoning:  why not bats or shunit2 .  Please cover your bash functions with unit tests and run test with:  make tests", 
            "title": "Testing"
        }, 
        {
            "location": "/development/#release-process-of-the-clodbreak-deployer-tool", 
            "text": "The master branch is always built on  CircleCI .\nWhen you wan\u2019t a new release, all you have to do:  make release-next-ver  make release-next-ver  performs the following steps:   On the  master  branch:  Updates the  VERSION  file by increasing the  patch  version number (for example from 0.5.2 to 0.5.3)  Updates  CHANGELOG.md  with the release date  Creates a new  Unreleased  section in top of  CHANGELOG.md    Creates a PullRequest for the release branch:  create a new branch with a name like  release-0.5.x  this branch should be the same as  origin/master  create a pull request into  release  branch", 
            "title": "Release Process of the Clodbreak Deployer tool"
        }, 
        {
            "location": "/development/#acceptance", 
            "text": "Now you should test this release. You can get it by  cbd update release-x.y.z . Comment with LGTM (Looking Good To Me).  Once the PR is merged, CircleCI will:   create a new release on  GitHub releases tab , with the\n help of  gh-release .  it will create the git tag with  v  prefix like:  v0.0.3", 
            "title": "Acceptance"
        }, 
        {
            "location": "/issues/", 
            "text": "Known issues\n\n\nDecommission\n\n\n\n\nAMBARI-15294\n In case of downscaling if the selected host group contains an HBase RegionServer it's not guaranteed that all regions will be safely moved to another RegionServer which will remain as part of the cluster. If you choose to scale down such a host group Cloudbreak won't track the region movement process. It is recommended to put the RegionServers in a different host group in your blueprint than the ones you'll be scaling.", 
            "title": "Known issues"
        }, 
        {
            "location": "/issues/#known-issues", 
            "text": "", 
            "title": "Known issues"
        }, 
        {
            "location": "/issues/#decommission", 
            "text": "AMBARI-15294  In case of downscaling if the selected host group contains an HBase RegionServer it's not guaranteed that all regions will be safely moved to another RegionServer which will remain as part of the cluster. If you choose to scale down such a host group Cloudbreak won't track the region movement process. It is recommended to put the RegionServers in a different host group in your blueprint than the ones you'll be scaling.", 
            "title": "Decommission"
        }
    ]
}